
<!-- README.md is generated from README.Rmd. Please edit that file -->
<!-- badges: start -->

![forthebadge](https://img.shields.io/badge/GEMM-Building-orange)
![forthebadge](https://forthebadge.com/images/badges/built-with-science.svg)

<!-- badges: end -->

# An√°lise do Metagenoma total - Shotgun <img src="imgs/1.png" align="right" width = "120px"/>

**Autor: MsC. Kelly Hidalgo**

üáßüá∑ Pipeline para a montagem e anota√ß√£o funcional de metagenomas totais.
Este pipeline contempla todas as etapas do processamento, desde a
avalia√ß√£o da qualidade das sequ√™ncias, trimagem, montagem, c√°lculo da
cobertura, predi√ß√£o e anota√ß√£o funcional e taxon√¥mica dos genes.

> üá™üá∏Pipeline para montaje y anotaci√≥n funcional de metagenomas totales.
> Este pipeline contempla todas las etapas del procesamiento, desde la
> evaluaci√≥n de la calidad de las secuencias, *trimming*, montaje,
> c√°lculo de la cobertura, predicci√≥n y anotaci√≥n funcional y taxon√≥mica
> de genes.

## Ferramientas bioinform√°ticas

### Instala√ß√£o Anaconda

üáßüá∑ √â recomend√°vel instalar Anaconda, pois √© a forma mais f√°cil para
instalar as ferramentas bioinform√°ticas necess√°rias pro desenvolvimento
deste pipeline. Anaconda √© uma distribui√ß√£o livre e aberta das
linguagens *Python* e *R*, utilizada na ci√™ncia de dados e
bioinform√°tica. As diferente vers√µes dos programas se administram
mediante um sinstema de gest√£o chamado *conda*, o qual faz bastante
simples instalar, rodar e atualizar programas.
[Aqui](https://conda.io/projects/conda/en/latest/user-guide/install/index.html)
se encontram as instru√ß√µes para a instala√ß√£o de Anaconda.

Depois de instalado, *Anaconda* e o gestor *Conda*, podram ser criados
*ambientes virtuais* par a instala√ß√£o das diferentes ferramentas
bioinform√°tica que ser√£o usadas.

> üá™üá∏ Es recomendable instalar Anaconda, pues es la forma m√°s f√°cil para
> instalar las herramientas bioinform√°ticas necesarias para el
> desarrollo de este pipeline. Anaconda es una distribuci√≥n libre y
> abierta de los lenguajes *Python* y *R*, utilizada en ciencia de datos
> y bioinform√°tica. Las diferentes versiones de los programas se
> administran mediante un sistema de gesti√≥n llamado *conda*, el cual
> hace bastante sencillo instalar, correr y actualizar programas.
> [Aqui](https://conda.io/projects/conda/en/latest/user-guide/install/index.html)
> se encuentran las instrucciones para la instalaci√≥n de Anaconda.
>
> Despu√©s de instalado *Anaconda* y su gestor *Conda*, podran ser
> creados *ambientes virtuales* para la instalaci√≥n de las diferentes
> herramientas bioinform√°ticas que ser√°n usadas.

------------------------------------------------------------------------

# I. Metagen√¥mica

## 0. Organizando os dados

### 0.1. Sequ√™ncias

üáßüá∑ Em este tutorial ser√£o usadas seis metagenomas exemplo para rodar
todo o *pipeline*. Descarregue os datasets usando o comando `wget`.

> üá™üá∏ En este tutorial ser√°n usados seis metagenomas ejemplo para correr
> todo el *pipeline*. Descargue los datasets usando el comando `wget`.

    # Crie um diret√≥rio para este tutorail
    mkdir metagenomica 
    cd metagenomica/

Agora dentro de metagenomica crie outro diret√≥rio chamado `00.RawData`,
onde vai descarregar o dataset de exemplo para este tutorial

    mkdir 00.RawData

Para descarregar o dataset‚Ä¶

    curl -L https://figshare.com/ndownloader/articles/19015058/versions/1 -o 00.RawData/dataset.zip
    unzip 00.RawData/dataset.zip
    rm 00.RawData/dataset.zip

Com `ls`voc√™ pode ver o conte√∫do descarregado.

    ls 00.RawData

Por √∫ltimo ‚Äúlistou‚Äù (`ls`) o conte√∫do da pasta `00.RawData`, vai
observar que t√™m 4 amostras paired-end (R1 e R2)

    Sample1_1.fq.gz Sample1_2.fq.gz Sample2_1.fq.gz Sample2_2.fq.gz Sample3_1.fq.gz Sample3_2.fq.gz Sample4_1.fq.gz Sample4_2.fq.gz Sample5_1.fq.gz Sample5_2.fq.gz Sample6_1.fq.gz Sample6_2.fq.gz

√â fortemente recomendado rodar os comandos desde o diret√≥rio base, que
neste caso √©: `metagenomica/`

> ## **Nota importante: A maioria dos comandos que encontrar√° a continua√ß√£o, ter√£o um par√¢metro para definir o n√∫mero de n√∫cleos/threads/cpus (`-t/--threads/`) que ser√£o usados para o processamento de cada comando. Coloque o n√∫mero de n√∫cleos baseado na sua m√°quina o servidor que esteja usando para rodar as an√°lises. Procure n√£o usar todos os n√∫cleos dispon√≠veis.**

## 1. Controle da Qualidade

## 1.1. Avalia√ß√£o da qualidade

üáßüá∑ Para a avalia√ß√£o da qualidade ser√° usado o programa
[FastQC](http://www.bioinformatics.babraham.ac.uk/projects/fastqc/) que
√© uma ferramenta que permite observar graficamente a qualidade das
sequencias de Illumina.

> üá™üá∏ Para la evaluaci√≥n de la calidad ser√° usado el programa
> [FastQC](http://www.bioinformatics.babraham.ac.uk/projects/fastqc/)
> que es una herramienta que permite observar graficamente la calidad de
> las secuencias de Illumina.

### 1.1.1. Instala√ß√£o

Las instru√ß√µes para a instala√ß√£o usando conda se encontram
[aqui](https://anaconda.org/bioconda/fastqc). No entanto neste tutorial
tamb√©m ser√£o apresentados.

Como j√° foi explicado anteriormente, com conda √© poss√≠vel criar
ambientes virtuais para instalar as ferramentas bioinform√°ticas. O
primeiro ambiente que ser√° criado se chamar√° **QualityControl**, onde se
instalaram os programas relacionados com esse processo.

> üá™üá∏ [FastQC](http://www.bioinformatics.babraham.ac.uk/projects/fastqc/)
> es una herramienta para evaluar graficamente la calidad de las
> secuencias de Illumina.
>
> Las instrucciones para instalaci√≥n usando conda se encuentran
> [aqui](https://anaconda.org/bioconda/fastqc). Sin embargo aqui en este
> tutorial tambi√©n ser√°n presentadas
>
> Como ya fue explicado anteriorimente, con conda es posible crear
> ambientes virutuales para instalar las herramientas bioinform√°ticas.
> El primer ambiente que ser√° creado se llamar√° **QualityControl**,
> donde se instalaran los programas relacionados con este proceso.

    conda create -n QualityControl

üáßüá∑ Durante o processo, o sistema perguntar√° se deseja proceder com a
crea√ß√£o do ambiente, com as op√ß√µes y/n (sim ou n√£o). Escreva `y` e
depois disso o ambiente virutal estar√° criado.

Para instalar as ferramentas dentro do ambiente anteriormente criado, √©
necess√°rio ativ√°-lo.

> üá™üá∏ Durante el proceso, el sistema preguntar√° s√≠ desea proceder con la
> creaci√≥n del ambiente, con las opciones y/n (si o no). Escriba `y` y
> despu√©s de eso el ambiente virtual estar√° creado.
>
> Para instalar las herramientas dentro del ambiente anteriormente
> creado, es necesario activarlo

    conda activate QualityControl

üáßüá∑ O ambiente estar√° ativo quando o nome se encontre ao come√ßo da linha
do comando, asssim: `(QualityControl) user@server:~/$`. Posteriormente
se procede √† instala√ß√£o do programa:

> üá™üá∏ El ambiente estar√° activo cuando el nombre de √©ste se encuentra en
> el comienzo de la linea de comando, as√≠:
> `(QualityControl) user@server:~/$`.
>
> Posteriormente se procede a la instalaci√≥n del programa:

    conda install -c bioconda fastqc

### 1.1.2. Uso

üáßüá∑ A primeira etapa do processo √© a avalia√ß√£o da qualidade das
sequ√™ncias cortas (Illumina paired end) usando *FastQC*, com o objetivo
de determianr se √© necess√°rio trimar ou filtrar as sequ√™ncias da baixa
qualidade para nos pr√≥ximos pasos.

Esta etapa √© para identificar principalmente as sequ√™ncias *outlier* com
baixa qualidade (*Q*‚ÄÑ&lt;‚ÄÑ20)

Ative o ambiente `QualityControl`:

> üá™üá∏ La primera etapa del proceso es la evaluaci√≥n de la calidad de las
> secuencias cortas (Illumina paired end) usando *FastQC*, con el
> objetivo de determinar s√≠ es necesario trimar o filtrar las secuencias
> de baja calidad en los pr√≥ximos pasos.
>
> √âsta etapa es para identificar principalmente las secuencias *outlier*
> con baja calidad (*Q*‚ÄÑ&lt;‚ÄÑ20).
>
> Active el ambiente `QualityControl`:

    conda activate QualityControl

    ## Onde vc est√°?
    pwd

üáßüá∑ Deve estar em `~/metagenomica/`. Se esse n√£o √© o resultado del
comando `pwd`, use o comando `cd` para chegar no diret√≥rio desejado.

> üá™üá∏ Debe estar em `~/metagenomica/`. Si ese no es el resultado del
> comando `pwd`, use el comando `cd` para llegar en el directorio base.

Execute **FastQC**:

    ## Crie um direct√≥rio para salvar o output do FastQC
    mkdir 01.FastqcReports
    ## Run usando 10 threads
    fastqc -t 10 00.RawData/* -o 01.FastqcReports/

**Sintaxe** `fastqc [op√ß√µes] input -o output`

üáßüá∑ O comando `fastqc` tem v√°rias op√ß√µes ou par√¢metros, entre eles,
escolher o n√∫mero de n√∫cleos da m√°quina para rodar a an√°lise, para este
exemplo `-t 10`. O input √© o diret√≥rio que contem as sequ√™ncias
`00.RawData/*`, o `*` indica ao sistema que pode analisar todos os
arquivos que est√£o dentro desse diret√≥rio. O output, indicado pelo
par√¢mtero `-o`, √© o diret√≥rio onde se deseja que sejam guardados os
resultados da an√°lise. A continua√ß√£o se encontram uma explica√ß√£o
detalhada de cada output gerado.

> üá™üá∏ El comando `fastqc` tiene varias opciones o parametros, entre
> ellas, escoger el n√∫mero de n√∫cleos de la m√°quina para correr el
> an√°lisis, para este caso `-t 10`. El input es el directorio que
> contiene las secuencias `00.RawData/*`, el `*` indica al sistema que
> puede analizar todos los archivos que est√°n dentro de ese directorio.
> El output, indicado por el parametro `-o`, es el directorio donde se
> desea que sean guardados los resultados del an√°lisis. A continuaci√≥n
> se encuentra una explicaci√≥n detallada de cada output generado.

**Outputs**

üáßüá∑

-   Reportes html `.html`: Aqui √© poss√≠vel ver toda informa√ß√£o de
    qualidade graficamente.

-   Zip files `.zip`: Aqui se encontram cada um dos gr√°ficos de maneira
    separada. **IGNORE**

Descarregue os arquivos `html` e explore no seu *web browser*.

Observe as estat√≠sticas b√°sicas que se encontram na primeira tabela.
Al√≠, voc√™ pode saber quantas sequ√™ncias tem, o tamanho e o %GC. O
gr√°fico mais importante para saber a quealidade das leituras, √© o
primeiro, *Per base sequence quality*. Este gr√°fico √© um boxplot com a
distribui√ß√£o dos valores de qualidade *Phred Score* (eje y) em cada um
dos nucleot√≠deos das leituras (eje x). Se consideram sequ√™ncias de
excelente qualidade quando o *Phred Score &gt; 30*. √â norla que o pair 2
apresente uma qualidade um pouco inferior ao pair 1.

As amostras deste tutorial, apresentam qualidade um pouco baixa,
principalmente no pair2. Por tanto, ser√° necess√°rio fazer a fase da
trimagem.

> üá™üá∏ Observe las estad√≠sticas b√°sicas que se encuentran en la primera
> tabla. All√≠, ud puede saber cuantas secuencias tiene, el tama√±o y el
> %GC. El gr√°fico m√°s importante para saber la calidad de las lecturas
> es el primero, *Per base sequence quality*. Este gr√°fico es un boxblot
> con la distribuci√≥n de los valores de calidad *Phred Score* (eje y) en
> cada uno de los nucle√≥tidos de las lecturas (eje x). Se consideran
> secuencias de excelente calidad cuando el *Phred Score &gt; 30*. Es
> normal que el pair 2 presente una calidad un poco inferior al pair 1.
>
> Las muestras de este tutorial, presentan calidad un poco baja,
> principalmente en el pair2. Por lo tanto, ser√° necesario hacer la fase
> de depuraci√≥n.

### 1.2. Trimagem

> üá™üá∏ 1.2 Depuraci√≥n

üáßüá∑ [Trimmomatic](http://www.usadellab.org/cms/?page=trimmomatic) √© um
programa pra filtrar (remover) leituras ou *reads* curtas de baixa
qualidade.

Trimmomatic tem v√°rios par√¢metros que podem ser considerados para
filtrar leituras com baixa qualidade. No presente tutorial usaremos
alguns deles. Se quiser saber que otros par√¢metros e como funciona cada
um deles, consulte o
[manual](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf).

> üá™üá∏ [Trimmomatic](http://www.usadellab.org/cms/?page=trimmomatic) es un
> programa para filtrar (remover) lecturas o *reads* cortas de baja
> calidad.
>
> Trimmomatic tiene v√°rios parametros que pueden ser considerados para
> filtrar lecturas con baja calidad. Aqui usaremos algunos. Si quiere
> saber que otros parametros y como funciona cada uno de ellos, consulte
> el
> [manual](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf).

### 1.2.1. Instala√ß√£o

üáßüá∑ Como se trata de uma ferramenta que participa dentro do processo de
control de qualidade, ser√° instalada dentro do ambiente virtual
**QualityControl**.

> Como se trata de una herramienta que participa dentro del proceso de
> control de calidad, ser√° instalada dentro del ambiente virtual
> **QualityControl**

    # Si no est√° activado el ambiente
    conda activate QualityControl

    # Instale Trimmomatic
    conda install -c bioconda trimmomatic

### 1.2.2. Uso

üáßüá∑ Segundo foi avaliado no controle de qualidade, pode ser necess√°rio
filtrar algumas leituras com qualidade baixa.

O programa Trimmomatic tem v√°rios par√¢metros que podem ser considerados
para filtrar reads com baixa qualidade. Aqui usaremos alguns. Se quer
saber que outros par√¢metros e como funciona cada um deles, consulte o
[manual](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf).

Para os dados aqui analizados se usara a seguinte linha de comando:

> üá™üá∏ Seg√∫n fue evaluado en el control de calidad, puede ser necesario
> filtrar algunas lecturas con calidad baja.
>
> El programa Trimmomatic tiene v√°rios parametros que pueden ser
> considerados para filtrar lecturas con baja calidad. Aqui usaremos
> algunos. Si quiere saber que otros parametros y como funciona cada uno
> de ellos, consulte el
> [manual](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf).
>
> Para los datos aqui analizados se usar√° la siguiente linea de comando:

    # Activa o ambiente QualityControl
    conda activate QualityControl

    # Crie uma pasta para salvar as reads limpas
    mkdir 02.CleanData

    # Crie uma pasta para salvar as reads n√£o pareadas
    mkdir unpaired

    # Corra Trimmomatic
    trimmomatic PE -threads 10 00.RawData/Sample1_1.fastq.gz 00.RawData/Sample1_2.fastq.gz 02.CleanData/Sample1_1_paired.fastq.gz unpaired/Sample1_1_unpaired.fastq.gz 02.CleanData/Sample1_2_paired.fastq.gz unpaired/Sample1_2_unpaired.fastq.gz LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:150

üáßüá∑ Com o comando anterior voc√™ tem que rodar a linha de comando para
cada amostra. Se quiser rodar todas as amostras de maneira autom√¢tica √©
poss√≠vel usar um *loop* `for` para executar esta tarefa.

> üá™üá∏ Con el comnado anterior ud tiene que correr esa l√≠nea de comando
> para cada muestra. Si quiere correr todas las muestras de manera
> autom√°tica es posible usar un *loop* `for` para ejecutrar esta tarea.

    # loop
    for i in 00.RawData/*1.fastq.gz 
    do
    BASE=$(basename $i 1.fastq.gz)
    trimmomatic PE -threads 10 $i  00.RawData/${BASE}2.fastq.gz 02.CleanData/${BASE}1_paired.fq.gz unpaired/${BASE}1_unpaired.fq.gz 02.CleanData/${BASE}2_paired.fq.gz unpaired/${BASE}2_unpaired.fq.gz LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:100
    done

**SINTAXE**
`trimmomatic PE -threads input_forward input_reverse output_forward_paired output_forward_unpaired output_reverse_paired output_reverse_unpaired [op√ß√µes]`

üáßüá∑ O comando anterior tem muitas partes. Primeiro, o nome do comando √©
`trimmomatic`, a continua√ß√£o a op√ß√£o `PE` indica para o programa que as
sequ√™ncias que ir√£o ser analisadas s√£o de tipo *paired end*. Depois se
encontram os inputs, forward (pair1) e reverse (pair2). Depois est√£o os
outputs, sendo o primeiro, as sequ√™ncias forward pareadas (limpas) e n√£o
pareadas (‚Äúdescartadas‚Äù) e depois igual para as sequ√™ncias reverse. Por
√∫ltimo se encontram os par√¢metros de filtragem. Para este caso usamos os
par√¢metros `SLIDINGWINDOW`, `LEADING` e `TRAILING`. O primeiro de eles,
gera uma janela deslizante, que em este caso vai de 4 em 4 bases,
c√°lcula a m√©dia do *Phred Score* e se estiver por baixo de 15 essas
bases ser√£o cortadas. `LEADING` corta bases do come√ßo da leitura que
estejam por debaixo do *threshold* de qualidade, igualmente faz o
`TRAILING` mas no final das leituras. `MINLEN` elimina todas as reads
com tamanho menor ao informado. Trimmomatic tem muitos mais par√¢metros
para customizar, veja no
[manual](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf).

Depois de rodar Trimmomatic √© necess√°rio avaliar a qualidade das
sequ√™ncias limpas usando novamente FastQC.

> üá™üá∏ El comando anterior tiene muchas partes. Primero, el nombre del
> comando es `trimmomatic`, a continuaci√≥n la opci√≥n `PE` indica para el
> programa que las secuencias que ir√°n a ser analizadas son de tipo
> *paired end*. Despu√©s se encuentran los inputs, forward (pair1) y
> reverse (pair2). Despu√©s son los outputs, siendo primero las
> secuencias forward pareadas (limpias) y no pareadas (‚Äúdescartadas‚Äù) y
> despu√©s las secuencias reverse. Por √∫ltimo se encuentran los
> parametros de filtrado. Para este caso usamos los parametros
> `SLIDINGWINDOW`, `LEADING` y `TRAILING`. El primero de ellos, genera
> una ventana deslizante, que en este caso va de 4 en 4 bases, c√°lcula
> el promedio del *Phred Score* y si est√° por debajo de 15 esas bases
> son cortadas. `LEADING` corta bases del comienzo de la lectura si
> est√°n por debajo de *threshold* de calidad, lo mismo hace `TRAILING`
> pero al final de las lecturas. `MINLEN` elimina todas las lecturas con
> tama√±o menor al informado. Trimmomatic tiene muchos m√°s par√°metros
> customizables, revise en el
> [manual](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf).
>
> Despu√©s de correr Trimmomatic es necesario evaluar la calidad de las
> secuencias generadas (‚Äúlimpias‚Äù) usando nuevamente FastQC.

    fastqc -t 10 02.CleanData/* -o 01.FastqcReports/

Descargue los reportes `.html` de las secuencias pareadas
(i.e.¬†`01.FastqcReports/Sample1_1_paired_fastqc.html` y
`01.FastqcReports/Sample1_2_paired_fastqc.html`).

Fa√ßa uma tabela com o n√∫mero de sequ√™ncias antes e depois da trimagem
para calcular a porcentagem de *reads* que sobreveviveram ao processo.

> üá™üá∏ Haga una tabla con el n√∫mero de secuencias antes y despu√©s de la
> depuraci√≥n para calcular el porcentaje de *reads* que sobrevivieron al
> proceso.

### 1.3 Cobertura dos Metagenoma

üáßüá∑ Al√©m de limpar e trimar as sequ√™ncias com baixa qualidade, √©
necess√°rio calcular a cobertura dos metagenomas.Este programa usa a
redund√¢ncia de reads nos metagenomas para estimar a cobertura m√©dia e
prediz a quantidade de sequ√™ncias que s√£o requeridas para atingir o
*‚Äúnearly complete coverage‚Äù*, definida como ‚ÄÑ‚â•‚ÄÑ95% ou ‚ÄÑ‚â•‚ÄÑ99% de
cobertura m√©dia. A ferramenta [**NonPareil
v3.3.3**](https://nonpareil.readthedocs.io/en/latest/) ser√° usada nesta
etapa.

> üá™üá∏ Adem√°s de limpiar y *trimar* las secuencias con baja calidad, es
> necesario calcular la cobertura de los metagenomas. Este programa usa
> la redundancia de las *reads* en los metagenomas para estimar la
> cobertura promedio y predice la cantidade de secuencias que son
> requeridas para conseguir el *‚Äúnearly complete coverage‚Äù*, definida
> como ‚ÄÑ‚â•‚ÄÑ95% o ‚ÄÑ‚â•‚ÄÑ99% de la cobertura promedio. La herramienta
> [**NonPareil v3.3.3**](https://nonpareil.readthedocs.io/en/latest/)
> ser√° usada en esta etapa.

### 1.3.1. Instala√ß√£o

üáßüá∑ [NonPareil v3.3.3](https://nonpareil.readthedocs.io/en/latest/) √© uma
ferramenta que ser√° usada para o c√°lculo da cobertura dos metagenomas.
Devido a incompatibilidades com a vers√£o do Python usado para escrever
esta ferramenta, ela ser√° instalada em um ambiente diferente ao de
controle de qualidade, chamado **NonPareil**.

> üá™üá∏ [NonPareil](https://nonpareil.readthedocs.io/en/latest/) es una
> herramienta que ser√° usada para el c√°lculo de la cobertura de los
> metagenomas. Debido a incompatibilidades con la versi√≥n de Python
> usado para escribir esta herramienta, ser√° instalada en un ambiente
> diferente al de control de calidad, llamado **NonPareil**.

    # Crie o ambiente
    conda create -n NonPareil

    # Instale NonPareil
    conda install -c bioconda nonpareil

### 1.3.2. Uso

Como *input* para esta an√°lise s√≥ √© necess√°rio um pair de cada amostra,
e deve estar sem compress√£o.

    # Crie o diret√≥rio pra o output
    mkdir 03.NonPareil

    # entre no directorio
    cd 03.NonPareil

    # Copie os pair 1 da pasta 02.CleanData

    cp ../02.CleanData/*_1* ./

    # Descomprimir 
    gunzip -d *

üáßüá∑ Agora est√° tudo pronto para rodar a an√°lise, mas antes disso tome-se
o tempo para entender o comando que vai usar. Para conhecer que √© cada
um dos argumentos, explore o men√∫ de ajuda da ferramenta.

> üá™üá∏ Ahora est√° todo listo para correr el an√°lisis, pero antes de eso
> t√≥mese el tiempo para entender el comando que va a usar. Para conocer
> que es cada uno de los argumentos, explore el men√∫ de ayuda de la
> herramienta.

    # Ative o ambiente NonPareil
    conda activate NonPareil

    # Explore o men√∫ da ferramenta
    nonpareil --help

    # Comando do NonPareil para cada amostra
    nohup nonpareil -s Sample1.fq -T kmer -f fastq -b Sample1 -t 6 &

No caso, se tiver v√°rias amostras pode usar o seguinte loop para
facilitar o processo.

    for i in ./*.fq
    do
    BASE=$(basename $i .fq)
    nonpareil -s $i -T kmer -f fastq -b $i -t 6
    done

**Sintaxe**

-   `-s`: caminho para o *input*
-   `-T`: algor√≠tmo a ser usado. `kmer` √© recomendado para arquivos
    `.fastq` e `alignment` √© recomendado para arquivos `.fasta`.
-   `-f`: indique aqui o formato do input (p.e. `fastq` ou `fasta`)
-   `-b`: prefixo para os *outputs*
-   `-t`: n√∫mero de threads

üáßüá∑ Ao terminar esse processo, o programa ter√° criado varios
[*outputs*](https://nonpareil.readthedocs.io/en/latest/redundancy.html#output)
por cada amostra. Descarregue os arquivos `.npo`. Os quais s√£o tabelas
delimitadas por tabula√ß√µes com seis colunas. A primeira coluna indica o
esfor√ßo de sequenciamento (em n√∫mero de reads), as demais colunas t√™m
informa√ß√£o sobre a distribui√ß√£o da redund√¢ncia a determinado esfor√ßo de
sequenciamento. Usando os arquivos `.npo` e o R, pode gr√°ficar as curvas
de satura√ß√£o. A continua√ß√£o se encontra o script. Al√©m dos arquivos
`.npo` √© necess√°rio criar um arquivo chamado `Samples.txt`, o qual deve
ter tr√™s colunas (separadas por tabula√ß√µes), a primeira ter√° o nome de
cada arquivo `.npo`, a segunda o nome da amostra, e a terceira a cor em
formato JSON que vai ser usada para a curva. A continua√ß√£o se encontram
uma s√©rie de comandos no bash para gerar o arquivo, no entanto este
arquivo pode ser construido em um bloco de notas, ou incluso no excel.

> üá™üá∏ Al terminar este proceso, el programa habr√° creado varios
> [*outputs*](https://nonpareil.readthedocs.io/en/latest/redundancy.html#output)
> por cada muestra. Descargue los archivos `.npo`. Los cuales son tablas
> delimitadas por tabulaciones con seis columnas. La primera columna
> indica el esfuerzo de secuenciaci√≥n (en n√∫mero de *reads*), las dem√°s
> columnas tienen informaci√≥n sobre la distribuci√≥n de la redundancia a
> determinao esfuerzo de secuenciaci√≥n. Usando los archivos `.npo` e R,
> puede gr√°ficar las curvas de saturaci√≥n. A continuaci√≥n se encuentra
> el script.
>
> Adem√°s de los archivos `.npo` es necesario crear un archivo llamado
> `Samples.txt`, el cual debe tener tres columnas (separadas por
> tabulaciones), la primera tendr√° el nombre de cada archivo `.npo`, la
> segunda el nombre de la muestra, y la tercera el color en formato JSON
> que va a ser usado para la curva. A continuaci√≥n se encontran una
> serie de comandos en bash para generar el archivo, sin embargo, este
> archivo puede ser construido en un bloc de notas, o incluso en excel.

    # Cria um arquivo com os nomes dos arquivos
    ls *.npo > files.txt

    # Cria um arquivo com os nomes das amostras

    ls *.npo | sed 's/_1_paired.fq.npo//g' > prefix.txt

Agora precisa criar uma lista de cores para diferenciar suas amostras no
gr√°fico. Use o site [IWantHue](http://medialab.github.io/iwanthue/) para
criar uma paleta com o n√∫mero de cores igual ao n√∫merop de amostras.
Copie os c√≥digos **HEX json** das cores e coloque dentro de um arquivo
(elimine as v√≠rgulas):

> üá™üá∏ Ahora necesita crear una lista de colores para diferencias sus
> muestras en el gr√°fico. Use el sitio de internet
> [IWantHue](http://medialab.github.io/iwanthue/) para crear una paleta
> con el n√∫mero de colores igual al n√∫mero de muestras. Copie los
> c√≥digos **HEX json** de los colores e coloque dentro de un archivo
> (elimine las comas):

    # Crie o arquivo
    nano colors.txt

    # Copie e cole os c√≥digos
    "#c151b6"
    "#5eb04d"
    "#7d65ce"
    "#b5b246"
    "#688ccd"
    "#4bb092"

Cree o arquivo final com os t√≠tulos de las columnas e una los tr√™s
arquivos gerados anteriormente:

    echo -e 'File\tName\tCol' > Samples.txt

    # Unindo os arquivos dentro de Samples.txt
    paste -d'\t' files.txt prefix.txt colors.txt >> Samples.txt

Use `less` para explorar o arquivo, ele deve se ver assim:

    File    Name    Col
    Sample1.npo   Sample1   "#c151b6"
    Sample2.npo   Sample2   "#5eb04d"
    Sample3.npo   Sample3   "#7d65ce"
    Sample4.pno   Sample4   "#b5b246"
    Sample5.npo   Sample5   "#688ccd"
    Sample6.npo   Sample6   "#4bb092"

Descarregue os arquivos `.npo` e o arquivo `Samples.txt`. Usando o
seguinte script do R, grafique as curvas de satura√ß√£o. **Nota:** todos
os arquivos descarregados devem estar dentro de uma pasta s√≥, p.e.
`03.NonPareil`.

Descarregue o script
[Nonpareil](https://github.com/khidalgo85/Metagenoma_Total_Shotgun/blob/master/nonpareil.R)

``` r
install.packages("Nonpareil") #para instalar o pacote
library(Nonpareil) # ativa o pacote
setwd("~/03.NonPareil") # determina seu diret√≥rio de trabalho (coloque o seu, onde colocou os arquivos .npo e o arquivo Samples.txt)

Samples <- read.table('Samples.txt', sep='\t', header=TRUE, as.is=TRUE); #l√™ o arquivo Samples.txt com a informa√ß√£o das amostras

attach(Samples);
nps <- Nonpareil.set(File, col=Col, labels=Name, 
                     plot.opts=list(plot.observed=FALSE, 
                                    ylim = c(0, 1.05),
                                    legend.opts = FALSE)) #grafica as curvas

Nonpareil.legengd(nps, x.intersp=0.5, y.intersp=0.7, pt.cex=0.5, cex=0.5) #coloca e personaliza a legenda
  
detach(Samples);
summary(nps) #mostra o resumo em forma de tabela
```

Vai obter um gr√°fico com as curvas de satura√ß√£o de cada amostra, como
este:

<img src="imgs/nonpareil.png" align='center' width="80%">

üáßüá∑ As linhas tracejadas <font color='red'> vermelha </font> e
<font color='gray'> cinza </font> representam os *threshold* de 95% e
99% da cobertura m√©dia, respeitivamente. O circulo em cada curva
representa a cobertura atual das amostras, o ideal √© que esteja por cima
do primeiro *threshold*. As curvas tamb√©m apresentam a estima√ß√£o de
quanto esfor√ßo de sequenciamento √© necess√°rio (zetas no eixo x). Devido
a que se trata de um dataset exemplo que foi obtido apartir de um
subSample aleatorio de um conjunto de dados, a maioria das amostras n√£o
conseguem uma boa cobertura. As curvas reais para as amostras originais
se apresentam a continua√ß√£o:

> üá™üá∏ Las l√≠neas punteadas <font color='red'> roja </font> y
> <font color='gray'> gris </font> representam los *threshold* de 95% y
> 99% de cobertura promedio, respectivamente. El c√≠rculo en cada curva
> representa la cobertura actual de las muestras, lo ideal es que est√©n
> por encima del primer *threshold*. Las curvas tambi√©n presentan la
> estimaci√≥n de cuanto esfuerzo de secuenciaci√≥n es necesario (flechas
> en el eje x). Debido a que se trata de un dataset ejemplo que fue
> obtenido a partir de un subSample aleatorio de un conjunto de datos,
> la mayoria de las muestras no consiguen una buena cobertura. Las
> curvas reales para las muestras originais se presentan a continuaci√≥n:

<img src="imgs/realnonpareil.png" align='center' width="80%">

### 1.4. An√°lise de Dist√¢ncias MinHash

üáßüá∑ Ap√≥s obter as sequ√™ncias limpas, de boa qualidade, e determinar a
cobertura dos metagenomas, √© poss√≠vel fazer a montagem. No entanto, pode
ser inclu√≠do um passo extra antes da montagem e √© verificar a
similaridade dos datasets para determinar se pode ser usada a abordagem
de *co-assembly*, onde s√£o misturadas as *reads* de v√°rios metagenomas
para gerar os contigs. O programa [**Mash
v2.3**](https://mash.readthedocs.io/en/latest/) usa uma t√©cnica chamada
redu√ß√£o de dimensionalidad *MinHash* que avalia as dist√¢ncias um a um
entre os datasets.

> üá™üá∏ Despu√©s de obtener las secuencias limpias, de buena calidad, y
> determinar la cobertura de los metagenomas, es posible hacer el
> montaje. Sin embargo, puede ser inclu√≠do un paso extra antes del
> montaje y es verificar la similaridade de los datasets para determinar
> si puede ser usado el abordaje de *co-assembly*, donde son mezcladas
> las *reads* de varios metagenomas para generar los contigs. El
> programa [**Mash v2.3**](https://mash.readthedocs.io/en/latest/) usa
> una t√©cnica llamada reducci√≥n de dimensionalidad *MinHash* que evalua
> las distancias un a un entre los datasets.

### 1.4.1. Instala√ß√£o

üáßüá∑ [Mash v2.3](https://mash.readthedocs.io/en/latest/) √© uma ferramenta
que usa a t√©cnica de redu√ß√£o da dimensionalidade *MinHash* para calcular
as dist√¢ncias um a um entre os datasets, assim, √© poss√≠vel determinar se
os metagenomas s√£o similares ou n√£o para serem montados usando
*co-assembly*.

üáßüá∑ Por ser considerada uma ferramenta que participa no processo de
assembly, ser√° instalada dentro de um ambiente virtual chamado
**Assembly**.

> üá™üá∏ [Mash](https://mash.readthedocs.io/en/latest/) es una herramienta
> que usa la t√©cnica de reducci√≥n de dimensionalidad *MinHash* para
> calcular las distancias un a un entre los datasets, as√≠, es posible
> determinar si los metagenomas son similares o no para ser ensamblados
> usando *co-assembly*.
>
> üá™üá∏ Por ser considera una herramienta que participa en el proceso de
> ensamble, ser√° instalada dentro de un ambiente virtual llamado
> **Assebly**.

    # Crie o ambiente virtual
    conda create -n Assembly

    # Instale Mash
    conda install -c bioconda mash

### 1.4.2. Uso

    ## Crie uma pasta para o output
    mkdir 04.MinHash

üáßüá∑ O primeiro paso √© concatenar os reads 1 e 2, e armazenar eles na nova
pasta criada `04.MinHash/`.

**Nota:** Se voc√™ trimou suas sequ√™ncias, deve usar os arquivos gerados
pelo **Trimmomatic** na pasta `02.CleanData`, se pelo contr√°rio suas
sequ√™ncias estavam de boa qualidade e n√£o foi necess√°rio trimar, use os
arquivos originais, que est√£o dentro da pasta `00.RawData/`.

> üá™üá∏
>
> **Nota:** Si usted filtr√≥ sus secuencias, debe usar los archivos
> generados por **Trimmomatic** en el directorio `02.CleanData`, si por
> el contrario sus secuencias estaban de buena calidade y no fue
> necesario filtrar, use los archivos originales, que est√°n dentro de la
> carpeta `00.RawData`.

    for i in 02.CleanData/*_1_paired.fq.gz
    do
    BASE=$(basename $i _1_paired.fq.gz)
    cat $i 02.CleanData/${BASE}_2_paired.fq.gz > 04.MinHash/${BASE}.fq
    done

üáßüá∑ Depois ser√° criado um *sketch* para combinar todas as amostras.
Usando `mash info` pode verificar o conte√∫do e, em seguida, estimar as
dist√¢ncias par a par:

> üá™üá∏
>
> Despu√©s ser√° creado un *sketch* para combinar todas las muestras.
> Usando `mash info` puede verificar el contenido y, en seguida, estimar
> las distancias par a par:

    mash sketch -o 04.MinHash/reference 04.MinHash/Sample1.fq 04.MinHash/Sample2.fq 04.MinHash/Sample3.fq 04.MinHash/Sample4.fq 04.MinHash/Sample5.fq 04.MinHash/Sample6.fq

    #verifiyng
    mash info 04.MinHash/reference.msh

**Sintaxe**

`mash sketch -o reference [inputs]`

`mash info reference.msh`

-   `sketch`: Comando para criar um *sketch*, combinando todas as
    amostras, recomendado quando t√™m mais de tr√™s amostras.
-   `-o`: caminho pro *output*, criar√° um *sketch* `.msh`.
-   `inputs`: liste os inputs (sequencias concatenadas dos pair1 e
    pair2)
-   `info`: pode verificar o conte√∫do do `sketch`
-   `reference.msh`: *sketch* criado

Por √∫ltimo, calcule as dist√¢ncias entre cada par de metagenomas usando
`mash dist` e salve o resultado no arquivo `distancesOutput.tsv`.

    mash dist 04.MinHash/reference.msh 04.MinHash/reference.msh -p 6 -t > 04.MinHash/distancesOutputFinal.tsv

**Sintaxe** `mash dist [reference] [query] [options]`

-   `dist`: comando para calcular as dist√¢ncias entre cada par de
    mategenomas, baseado na dist√¢ncia *MinHash*.
-   `reference`: aqui pode colocar o *sketch* criado, ou arquivos `.fq`,
    `fasta`.
-   `query`: √≠dem
-   `-p`: n√∫mero de threads
-   `-t`: indica o tipo de formato matriz

Descarregue o output (`04.MinHash/distancesOutputFinal.tsv`) e use o
seguinte script do R para plotar um heatmap com as dist√¢ncias.

Descarregue o script para graficar o heatmap das distancias
[MinHash](https://github.com/khidalgo85/Metagenoma_Total_Shotgun/blob/master/minhash.R)

``` r
setwd("~/04.MinHash/")

# install.packages('dplyr')
library(dplyr)
# install.packages('stringr')
library(stringr)
# install.packages('tidyverse')
library(tidyverse)

data <- read.table("distancesOutputFinal.tsv", comment.char = '', 
                    header = TRUE ) %>% 
  rename(X = X.query) 
  

data$X <- str_remove_all(data$X, "04.MinHash/")
data$X <- str_remove_all(data$X, ".fq")

names <- c("X", data[,1])

colnames(data) <- names

data <- column_to_rownames(data, var="X")

library(pheatmap)


pheatmap(data)
```

Vai obter um heatmap com clusteriza√ß√£o:

<img src="imgs/minhash.png" align='center' width="80%">

Como pode ser observado, se formaram v√°rios clusters, por exemplo
Sample5, Sample4 e Sample1, Sample3 e 2, e Sample6 formou um cluster
aparte. Assim, poderiam ser feitos dois co-assemblies e um assembly
individual. No entanto para facilitar o processo no tutorial, ser√° feito
um co-assembly s√≥, com todas as amostras.

## 2. Montagem dos Metagenomas

üáßüá∑ A montagem dos metagenomas √© a etapa mais importante do processo,
porque os demais passos para adelante dependen de uma boa montagem. No
caso dos metagenomas, se trata de um proceso que n√£o √© para nada
trivial, requer um grande esfor√ßo computacional. Por este motivo, ser√£o
testados v√°rios par√¢metros, para comparar cada montagem e decidir qual √©
o melhor para √°s an√°lises *downstream*. Neste processo ser√° usado o
montador [Spades v3.15.3](https://github.com/ablab/spades).

> üá™üá∏ El montaje de los metagenomas es la etapa m√°s importante del
> proceso, porque los dem√°s pasos para adelante dependen de un buen
> ensamble. En el caso de los metagenomas, se trata de un proceso que no
> es para nada trivial, requiere un gran esfuerzo computacional. Por
> este motivo ser√°n testados varios par√°metros, para comparar cada
> ensamble y decidir cual es el mejor para los an√°lisis *downstream*. En
> este proceso ser√° usado el montado [Spades
> v3.15.3](https://github.com/ablab/spades).

### 2.1. Instala√ß√£o

üáßüá∑ [Spades v3.15.3](https://github.com/ablab/spades) √© um dos montadores
de genomas e metagenomas, mais conhecido e com melhores resultados, pode
ser usado tanto para leituras curtas como longas. Leia atentamente o
[manual](http://cab.spbu.ru/files/release3.15.2/manual.html), j√° que
este programa tem muitas op√ß√µes diferentes. Spades usa o algor√≠tmo do
*Grafo de Bruijn* para a montagem das secu√™ncias.

Siga as seguintes instru√ß√µes para a instala√ß√£o do **Spades** dentro do
ambiente virtual *Assembly*.

> üá™üá∏ [Spades v3.15.3](https://github.com/ablab/spades) es uno de los
> ensambladores de genomas y metagenomas, m√°s conocido y con mejores
> resultados, puede ser usado tanto para lecturas cortas como largas.
> Lea atentamente el
> [manual](http://cab.spbu.ru/files/release3.15.2/manual.html), ya que
> este programa tiene muchas opciones diferentes. Spades usa el
> algor√≠tmo del *Grafo de Bruijn* para el montaje de las secuencias.
>
> Siga las siguientes instrucciones para la instalaci√≥n de **Spades**
> dentro del ambiente virtual *Assembly*.

    # Active el ambiente virtual
    conda activate Assembly

    # Instale Spades
    conda install -c bioconda spades

### 2.2. Uso

üáßüá∑ Agora √© momento de fazer as montagens. Use o resultado da an√°lisis de
dist√¢ncias *MinHash* para decidir como ser√£o feitos as montagens.
Amostras muito pr√≥ximas pode fazer *co-assembly*, para amostras
distantes √© recomendado montar individualmente. Opcionalmente podem ser
usadas as sequ√™ncias no pareadas (sequ√™ncias ‚Äúdescartadas‚Äù pelo
Trimmomatic). O montador usado neste m√©todo ser√°
[Spades](https://github.com/ablab/spades).

A continua√ß√£o se encontram os comandos se sua montagem for individual:

> üá™üá∏ Ahora es el momento de hacer los ensamblajes. Use el resultado del
> an√°lisis de distancias *MinHash* para decidir como ser√°n hechos los
> montajes. Muestras muy pr√≥xima puede hacer *co-assembly*, para
> muestras distantes es recomendado montar individualmente.
> Opcionalmente pueden ser las secuencias no pareadas (secuencias
> ‚Äúdescartadas‚Äù por Trimmomatic). El montador usado en este m√©todo ser√°
> [Spades](https://github.com/ablab/spades).

1.  Criar um diret√≥rio para todas as montagens

<!-- -->

    mkdir 05.Assembly

2.  Se voc√™ quiser usar as *reads* no pareadas (sa√≠da do
    **Trimmomatic**), deve primeiro concatenarlas em um arquivo s√≥

<!-- -->

    cat unpaired/Sample1_1_unpaired.fq.gz unpaired/Sample1_2_unpaired.fq.gz > unpaired/Sample1_12_unpaired.fq.gz

3.  Montagem com MetaSpades

<!-- -->

    metaspades.py -o 05.Assembly/Sample1/ -1 02.CleanData/Sample1_1_paired.fq.gz -2 02.CleanData/Sample1_2_paired.fq.gz -s unpaired/Sample1_12_unpaired.fq.gz -t 6 -m 100 -k 21,29,39,59,79,99,119

**Sintaxe**

-   `metaspades.py`: script para montar metagenomas
-   `-o`: caminho para diret√≥rio de sa√≠da
-   `-1`: caminho para diret√≥rio do pair1
-   `-2`: caminho para diret√≥rio do pair2
-   `-s`: caminho para diret√≥rio das *reads* no pareadas
-   `-t`: n√∫mero de threads
-   `-m`: Mem√≥ria em gigas (m√°ximo)
-   `-k`: lista de *k-mers*

üáßüá∑ Se sua montagem for no modo *co-assembly* deve fazer uma etapa
anterior, onde vai concatenar todos os pair1 das amostras que ser√£o
montadas e todos os pair2 das mesmas.

> üá™üá∏ Si su ensamblaje es en el modo *co-assembly* debe hacer una etapa
> anterior, donde va a concatenar todos los pair1 de las muestras que
> ser√°n montadas y todos los pair2 de las mismas.

1.  Concatene os pair 1

<!-- -->

    cat 02.CleanData/Sample1_1.fq.gz 02.CleanData/Sample2_1.fq.gz 02.CleanData/Sample3_1.fq.gz 02.CleanData/Sample4_1.fq.gz 02.CleanData/Sample5_1.fq.gz 02.CleanData/Sample6_1.fq.gz > 02.CleanData/Sample_all_1.fq.gz

2.  Concatene os pair 2

<!-- -->

    cat 02.CleanData/Sample1_2.fq.gz 02.CleanData/Sample2_2.fq.gz 02.CleanData/Sample3_2.fq.gz 02.CleanData/Sample4_2.fq.gz 02.CleanData/Sample5_2.fq.gz 02.CleanData/Sample6_2.fq.gz > 02.CleanData/Sample_all_2.fq.gz

3.  Se voc√™ quiser usar as *reads* no pareadas (sa√≠da do
    **Trimmomatic**), deve primeiro concatenarlas em um arquivo s√≥

<!-- -->

    cat unpaired/Sample1_1_unpaired.fq.gz unpaired/Sample1_2_unpaired.fq.gz unpaired/Sample2_1_unpaired.fq.gz unpaired/Sample2_2_unpaired.fq.gz unpaired/Sample3_1_unpaired.fq.gz unpaired/Sample3_2_unpaired.fq.gz unpaired/Sample4_1_unpaired.fq.gz unpaired/Sample4_2_unpaired.fq.gz unpaired/Sample5_1_unpaired.fq.gz unpaired/Sample5_2_unpaired.fq.gz unpaired/Sample6_1_unpaired.fq.gz unpaired/Sample6_2_unpaired.fq.gz > unpaired/Sample_all_unpaired.fq.gz

4.  Montagem com MetaSpades

<!-- -->

    metaspades.py -o 05.Assembly/ -1 02.CleanData/Sample_all_1.fq.gz -2 02.CleanData/Sample_all_2.fq.gz-s unpaired/Sample_all_unpaired.fq.gz -t 10 -m 100 -k 21,29,39,59,79,99,119

**Outputs**

Para conhecer os demais par√¢metros do comando que n√£o foram modificados
(usados por *default*), consulte o
[manual](http://cab.spbu.ru/files/release3.15.2/manual.html).

-   `corrected/`: cont√©m as reads corregidas por **BayesHammer** em
    `.fastq.gz`

-   `scaffolds.fasta`: cont√©m os scaffolds obtidos

-   `contigs.fasta`: cont√©m os contigis obtidos

-   `assembly_graph_with_scaffolds.gfa`: cont√©m o grafo da montagem en
    formato GFA 1.0.

-   `assembly_graph.fastg`: cont√©m o grafo da montagem em formato FASTG

## 3. Controle de Qualidade das montagens

üáßüá∑ Para avaliar a qualidade das montagens ser√° usada a ferramenta
[**Quast v5.0.2**](http://quast.sourceforge.net/docs/manual.html)
(*QUality ASsesment Tool*), especificamente o *script* `metaquast.py`,
com o qual √© poss√≠vel determinar as principais estat√≠sticas da montagem
(i.e.¬†N50, n√∫mero de contigs, tamanho total da montagem, tamanho dos
contigs, etc). **Metaquast** gera uma s√©rie de arquivos e reportes onde
√© poss√≠vel observar essas estat√≠sticas b√°sicas da montagem. √â uma
ferramente muito √∫til para comparar montagens e escolher a melhor pro
mesmo conjunto de dados.

> üá™üá∏ Para evaluar la calidad de los montajes ser√° usada la herramienta
> [**Quast v5.0.2**](http://quast.sourceforge.net/docs/manual.html)
> (*QUality ASsesment Tool*), especificamente el *script*
> `metaquast.py`, con el cual es posible determinar las principales
> estad√≠sticas del montaje (i.e.¬†N50, n√∫mero de contigs, tama√±o total
> del montaje, tama√±o de los contigs, etc). **Metaquast** genera una
> serie de archivos y reportes donde es posible observar esas
> estad√≠sticas b√°sicas del montaje. Es una herramienta muy √∫til para
> comparar monatajes y escoger el mejor del mismo conjunto de datos.

### 3.1. Instala√ß√£o

Crie um novo ambiente virtual, chamado bioinfo, onde se instalar√°
**Quast**.

    # Crie o ambiente
    conda create -n bioinfo

    # Ative o ambiente bioinfo
    conda activate bioinfo

    # Instale Quast
    conda install -c bioconda quast

### 3.2. Uso

üáßüá∑ Se voc√™ tiver v√°rias montagens e quer comparar todas √© necess√°rio
trocar os nomes dos assemblies, j√° que eles tem todos o mesmo nome,
`contigs.fasta` ou `scaffolds.fasta`. Use o comando `mv` para trocar os
nomes. Siga o seguinte exemplo:

> üá™üá∏ Si usted tiene varios ensambles e quiere compararlos es necesario
> cambiar los nombres de los montajes, ya que todos tienen el mismo
> nombre, `contigs.fasta` ou `scaffolds.fasta`. Use el comando `mv` para
> cambiar los nombres. Siga el siguiente ejemplo:

Por exemplo:

    mv 05.Assembly/Sample1/scaffolds.fasta 05.Assembly/Sample1/Sample1.fasta

    mv 05.Assembly/Sample45/scaffolds.fasta 05.Assembly/Sample45/Sample45.fasta

Para as amostras deste tutorial n√£o √© necess√°rio trocar os nomes porque
s√≥ √© uma montagem:

    # Crie um diret√≥rio pro output
    mkdir 06.AssemblyQuality

    # Rode Quast
    metaquast.py 05.Assembly/scaffolds.fasta -o 06.AssemblyQuality/ --threads 10

**Sintaxis**
`metaquast.py path/to/assembly/contigs.fasta -o path/to/output/`

-   Pode colocar v√°rios inputs (montagens) separados por espa√ßo.

**Interpreta√ß√£o dos resultados**

üáßüá∑ A ideia de usar **Metaquast**, a parte de avaliar as estat√≠sticas
b√°sicas das montagens, √© comparar varias montagens para escolher a
melhor. Por exemplo: entre menor seja o n√∫mero de contigs √© melhor,
porque significa que a montagem est√° menos fragmentada. E isso ser√°
refletido no tamanho dos contigs que ser√£o maiores. O valor de N50, √©
melhor entre maior seja. Al√©m, tamb√©m √© ideal um menor n√∫mero de gaps e
Ns. No entanto, estas estat√≠sticas funcionam melhor para genomas que
para metagenomas, por se tratar de um conjunto de microrganismos.

> üá™üá∏ La idea de usar **Metaquast**, aparte de evaluar las estid√≠sticas
> b√°sicas de los montajes, es comparar varios montajes para escoger el
> mejor. Por ejemplo: entre menor sea el n√∫mero de contigs es mejor,
> porque significa que el montaje est√° menos fragementado. Y eso se
> reflejar√° en el tama√±o de los contigs que ser√°n m√°s grandes. El valor
> de N50, es mejor entre mayor sea. As√≠ mismo, es ideal menor n√∫mero de
> gaps y Ns. Sin embargo, √©stas estad√≠sticas funcionan mejor para
> genomas que para metagenomas, por tratarse de un grupo de
> microorganismos.

**Outputs**

Explore o diret√≥rio do output usando o comando `ls`.

-   `06.AssemblyQuality/report.html`: Este relat√≥rio pode ser aberto em
    um *web browser* e contem as informa√ß√µes mais relevantes. Como
    n√∫mero de contigs, tamanho del maior contig, tamanho total da
    montagem, N50, etc.

> üá™üá∏ `06.AssemblyQuality/report.html`: reporte puede ser abierto en un
> *web browser* y contiene las informaciones m√°s relevantes. Como n√∫mero
> de contigs, tama√±o del mayor contig, tama√±o total del montaje, N50,
> etc.

<img src="imgs/report_quast1.png" align="center" width = "100%"/>

-   `06.AssemblyQuality/report.tex`, `06.AssemblyQuality/report.txt`,
    `06.AssemblyQuality/report.tsv`, `06.AssemblyQuality/report.pdf`: √©
    o mesmo relat√≥rio por√©m em diferentes formatos.

-   `06.AssemblyQuality/transposed_report.tsv`,
    `06.AssemblyQuality/transposed_report.tex`,
    `06.AssemblyQuality/transposed_report.tex`: Tamb√©m √© o relat√≥rio
    por√©m em formato tabular.

-   `06.AssemblyQuality/icarus_viewers/contig_size_viewer.html`:
    Visualizador das contigs

-   `06.AssemblyQuality/basis_stats/`: Dentro desta pasta se encontram
    v√°rios gr√°ficos em formato `.pdf`.

## 4. Predi√ß√£o das ORFs (*Open Reading Frame*)

üáßüá∑ O objetivo desta etapa √© procurar os marcos abertos de leitura ou
ORFs (em ingl√™s) dentro dos contig/scaffols. Ou seja, predizer onde
iniciam e terminam os genes. Basicamente o programa procura por codons
de inicio, principalmente **ATG**, por√©m, tamb√©m s√£o c√≥dons de inicia√ß√£o
**GTG** e **TTG**. Depois, procura os c√≥dons de parada, como **TAA**,
**TAG** e **TGA**.

O programa a usar para a predi√ß√£o das ORFs em procariotos √© [Prodigal
v2.6.3 (*Prokaryotic Dynamic Programming Genefinding
Algorithm*)](https://github.com/hyattpd/prodigal/wiki).

> üá™üá∏ El objetivo de esta etapa es buscar los marcos abiertos de lectura
> o ORF (en ingl√©s) dentro de los contigs/scaffolds. O sea, predecir
> donde incian y terminan los genes. Basicamente el programa busca por
> c√≥dones de inicio, principalmente **ATG**, sin embargo tambi√©n son
> c√≥dones de inico **GTG** e **TTG**. Despu√©s, busca los c√≥dones de
> parada, como **TAA**, **TAG** y **TGA**.
>
> El programa a usar para la predicci√≥n de ORFs en procariotos es
> [Prodigal v2.6.3 (*Prokaryotic Dynamic Programming Genefinding
> Algorithm*)](https://github.com/hyattpd/prodigal/wiki).

### 4.1. Instala√ß√£o

Crie um novo ambiente para instala√ß√£o das ferramentas relacionadas com a
anota√ß√£o de genes, chamada `Annotation`.

    # Crie o ambiente
    conda create -n Annotation

    # Ative o ambiente
    conda activate Annotation

    # Instale Prodigal
    conda install -c bioconda prodigal

### 4.2. Uso

Se tiver v√°rias montagens, passe todas as montagens para uma pasta s√≥.
No caso deste tutorial s√≥ √© uma montagem ent√£o n√£o √© necess√°rio.

Crie uma pasta chamada `07.GenePrediction` para colocar a sa√≠da do
**Prodigal**.

`mkdir 07.GenePrediction`

A continua√ß√£o encontrar√° o comando **individual**

    prodigal -i 05.Assembly/scaffolds.fasta -f gff -o 07.GenePrediction/GenesCoordenates.gff -a 07.GenePrediction/GenesAA.faa -d 07.GenePrediction/GenesNucl.fa -p meta

Se tiver v√°rias amostras, pode usar o seguinte loop para automatizar o
processo com todas as amostras:

    for i in 05.Assembly/scaffolds/*.fasta
    do
    BASE=$(basename $i .fasta)
    prodigal -i $i -f gff -o 07.GenePrediction/${BASE}.gff -a 07.GenePrediction/${BASE}.faa -d 07.GenePrediction/${BASE}.fa -p meta
    done

**Sintaxe**

    prodigal -i assembly.fasta -f <gbk, gff, sqn, sco> -o coord -a proteins.faa -d nucleotides.fa

-   `-i`: caminho para a montagem em formato `.fasta`, `.fa` ou `.fna`
-   `-f`: formato de sa√≠da pro arquivo de coordenadas, default `.gbk`
    (*Genbank-like format*), `.gff` (*Gene Feature Format*), `.sqn`
    (*Sequin feature table format*) ou `.sco` (*Simple coordinate
    input*)
-   `-o`: arquivo output com as coordenadas das ORFs
-   `-a`: sequ√™ncias das ORFs em prote√≠na
-   `-d`: sequ√™ncias das ORFs em nucleot√≠deos

**Formato `.gff` (Gene Feature Format)**

üáßüá∑ Este formato guarda as informa√ß√µes dos genes preditos pelo Prodigal.
Explore-o (`less GenesCoordenates.gff`).

Cada sequ√™ncia comen√ßa com um *header* com as infroma√ß√µes da sequ√™ncia
analizada, seguido de uma tabela separada por tabula√ß√µes com informa√ß√µes
dos genes encontrados em dita sequ√™ncia.

O *header* cont√©m os seguentes campos:

> üá™üá∏ Este formato guarda las informaciones de los genes predichos por
> Prodigal. Explorelo (`less GenesCoordenates.gff`).
>
> Cada secuencia comienza con un *header* con las informaciones de la
> secuencia analizada, seguido de una tabla separada por tabulaciones
> con informaciones de los genes encontrados en dicha secuencia.
>
> El *header* contiene los siguientes campos:

-   **seqnum**: O n√∫mero da sequ√™ncia, come√ßando pelo n√∫mero 1.
-   **seqlen**: tamanho em bases da sequ√™ncia
-   **seqhdr**: t√≠tulo completo da sequ√™ncia extra√≠do do arquivo
    `.fasta`.
-   **version**: vers√£o do Prodigal usado
-   **run\_type**: modo de corrida, p.e. m*metagenomic*
-   **model**: informa√ß√£o sob o arquivo de treinamento usado para a
    predi√ß√£o.
-   **gc\_cont**: % de GC na sequ√™ncia
-   **transl\_table**: Tabela do c√≥digo gen√©tico usada para analizar a
    sequ√™ncia. Para bact√©rias e archaeas √© usada a [tabela
    11](https://www.ncbi.nlm.nih.gov/Taxonomy/Utils/wprintgc.cgi#SG11).
-   **uses\_sd**: 1 se o Prodigal usa o
    *[RBS](https://parts.igem.org/Ribosome_Binding_Sites) finder*, ou 0
    se usa outros *motifs*.

Despu√©s do *header* se encuentra una tabla con las informaciones de los
genes encontrados:

-   **seqname**: nome da sequ√™ncia, neste caso nome do scaffold/contig.

-   **source**: nome do programa que gerou a predi√ß√£o

-   **feature**: tipo de *feature*, p.e. CDS (*Coding DNA Sequence*)

-   **start**: primeira posi√ß√£o da *feature*

-   **end**: √∫tlima posi√ß√£o da *feature*

-   **score**: Valor numerico que geralmente indica a confian√ßa do
    programa na predi√ß√£o da ORF.

-   **strand**: fita do DNA que foi encontrado a *feature*. A fita
    *forward* √© definida como ‚Äò+‚Äô, e a *reverse* como ‚Äò-‚Äô.

-   **frame**: 0 indica que a primeira base da *feature* √© a primeira
    base do c√≥don de inicio, 1, que a segunda base da *feature* √© a
    primeira base do c√≥don de inicio.

-   **atribute**: informaci√≥n adicional sobre la *feature*, parada por
    ponto e v√¨rgula ‚Äú;‚Äù.

    -   **ID**: identificador √∫nico de cada gene, consistindo em um
        n√∫mero ordinal ID da sequ√™ncia e um n√∫mero ordinal ID do n√∫mero
        do gene separados por "\_‚Äú. Por exemplo‚Äù1\_688" siginifa que √© o
        gene n√∫mero 688 da sequ√™ncia 1.
    -   **partial**: indica se o gene est√° completo ou n√£o. ‚Äú0‚Äù indica
        que no gene foi encontrado o c√≥don de inicio ou de parada, ‚Äú01‚Äù
        indica que no gene s√≥ foi encontrado o c√≥ndon de inicio, ‚Äú11‚Äù
        indica que n√£o foram encontrados nenhum dos dois c√≥dons e ‚Äú00‚Äù
        indica que foram encontrados ambos c√≥dons.
    -   **start\_type**: sequ√™ncia do c√≥don de inicio.
    -   **stop\_type**: sequ√™ncias do c√≥don de parada
    -   **rbs\_motif**: *RBS motif* encontrado pelo Prodigal
    -   **rbs\_spacer**: n√∫mero de bases entre o c√≥don de inicio e o
        *motif* observado.
    -   **gc\_cont**: Conet√∫do de GC no gene
    -   **conf**: nota de confian√ßa pra o gene, representa a
        probabilidade que esse gene seja real.
    -   **score**: *score* total pro gene
    -   **cscore**: fra√ß√£o hexamero do *score*, o quanto este gene se
        parece com uma prote√≠na verdadeira.
    -   **sscore**: *score* para o sitio de inicio da tradu√ß√£o do gene.
        √© a soma dos tr√™s seguintes *scores*.
    -   **rscore**: *score* pro *RBS motif*
    -   **uscores**: *score* pra sequ√™ncia em torno do c√≥don de in√≠cio.
    -   **tscore**: *score* para o tipo de c√≥don de inicio
    -   **mscore**: *score* pros sinais restantes (tipo de c√≥don de
        parada e informa√ß√µes da fita principal / reversa).

## 5. Anota√ß√£o de genes

üáßüá∑ A anota√ß√£o dos genes √© feita alinhando as ORFs preditas contra bases
de dados. No caso da anota√ß√£o funcional, ser√° usado o alinhador
[**Diamond**](https://github.com/bbuchfink/diamond) e as bases de dados
ser√£o [**EggNOG**](http://eggnog5.embl.de/#/app/home) e
[**KEGG**](https://www.kegg.jp/kegg/). No caso da anota√ß√£o taxon√¥mica,
podem ser usados dois programas, o
[**Kaiju**](https://github.com/bioinformatics-centre/kaiju) ou o
[**Kraken2**](https://github.com/DerrickWood/kraken2/wiki).

> üá™üá∏La anotaci√≥n de los genes es realizada alineando las ORFs predichas
> contra bases de dados. En el caso de la anotaci√≥n funcional ser√° usado
> el programa para alineamiento
> [**Diamond**](https://github.com/bbuchfink/diamond) y las bases de
> datos [**EggNOG**](http://eggnog5.embl.de/#/app/home) y
> [**KEGG**](https://www.kegg.jp/kegg/). Ya en el caso de la anotaci√≥n
> taxon√≥mica, pueden ser usados dos programas,
> [**Kaiju**](https://github.com/bioinformatics-centre/kaiju) o
> [**Kraken2**](https://github.com/DerrickWood/kraken2/wiki).

### 5.1. Instala√ß√£o

#### 5.1.1 Obten√ß√£o das Bases de Dados

üáßüá∑Para a obten√ß√£o das bases de dados, pode ir nos sites e descarregar
diretamente. No entanto, tenha em conta que a base de dados **KEGG** √©
paga. Se voc√™ descarregar direto da fonte, dever√° formatar as DBs para o
seu uso com Diamond (anota√ß√£o funcional). Isto √© feito com o comando
`makedb --in reference.fasta -d reference`.

Para facilitar, no seguinte link, voc√™ encontrar√° as bases de dados
**KEGG**, **EggNOG**, previamente formatadas para o uso em Diamond e
**Kraken2**.

Use o programa `gdown` para descarregar as dbs que se encontram em um
GoogleDrive. Se n√£o tiver o `gdown` instalado, siga o seguintes passos:

> üá™üá∏ Para la obtenci√≥n de las bases de datos, puede ir directamente en
> las p√°ginas web de cada una. Sin embargo, tenga en cuenta que la base
> de datos **KEGG** es paga. Si ud decide descargar directamente de la
> fuente, deber√° hacer una formataci√≥n de las DBs para el uso con
> Diamond (anotaci√≥n funcional). Este processo es realizado usando el
> comando `makedb --in reference.fasta -d reference`.
>
> Para facilitar, en el siguiente link, encontrar√° las bases de
> datos**KEGG**, **EggNOG**, previamente formatadas para su uso en
> Diamond e **Kraken2**.

-   [**Dbs**](https://drive.google.com/drive/folders/1GLP6vA4Gs0cce-nnBXCmZSgmONWybOSF?usp=sharing)

<!-- -->

    ## Se n√£o tiver instalado pip
    sudo apt update
    sudo apt install python3-pip
    pip3 --verision

    ## Instale gdown
    pip install gdown

üáßüá∑ Crie uma pasta, chamada `dbs/`, e use o programa `gdown` para
descarregar as dbs.

    # Crie o diret√≥rio
    mkdir dbs/

    # Descarregue as DBs

    ## KEGG
    gdown --id 1ZxjJdwh1izP32X5CH-B8SN0DK2WAAAvr

    ##EggNOG
    gdown --id 1x2Kp4PTX8GFFhkJm6EVDQLfi-xRSQ735

Ser√£o descarregados os seguintes arquivos:

-   `eggnog.dmnd`: Base de dados EggNOG formatada para Diammond
-   `keggdb.dmnd`: Base de dados KEGG formatada para Diammond

üáßüá∑ **Nota** √â recomend√°vel procurar os links originais para descarga das
bases de dados para assim obter a vers√£o mais atualizada (p.e.
[Kraken2](https://ccb.jhu.edu/software/kraken2/index.shtml?t=downloads))

> üá™üá∏ **Nota** es recomendable buscar los links originales para descargar
> las bases de datos en sus versiones m√°s actualizadas (p.e.
> [Kraken2](https://ccb.jhu.edu/software/kraken2/index.shtml?t=downloads))

    ## Kraken2
    mkdir Kraken2
    cd Kraken2

    ## Descarregando desde o servidor dos desenvolvedores
    wget  ftp://ftp.ccb.jhu.edu/pub/data/kraken2_dbs/old/minikraken2_v1_8GB_201904.tgz

    tar zxvf minikraken2_v1_8GB_201904.tgz

    ## Troque o nome da pasta de sa√≠da
    mv minikraken2_v1_8GB/ mainDB

    ## Elimine o arquivo original
    rm minikraken2_v1_8GB_201904.tgz

#### 5.1.2 Instala√ß√£o Diammond

O [**Diamond**](https://github.com/bbuchfink/diamond) ser√° usado para a
anota√ß√£o funcional. Instale atrav√©s do conda, no ambiente `Annotation`

    # Active o ambiente
    conda activate Annotation

    # Instala√ßao
    conda install -c bioconda diamond=2.0.9

#### 5.1.3. Instala√ß√£o Kraken2

[**Kraken2**](https://github.com/DerrickWood/kraken2/wiki) ser√° usado
para a anota√ß√£o taxon√¥mica. Instale o programa no ambiente `Annotation`.

    # Se n√£o estiver ativado
    conda activate Annotation

    # Instale 
    conda install -c bioconda kraken2

Ap√≥s instalado, deve configurar a base de dados, isto √© indicar pro
programa o caminho (*PATH*) onde se encontram a base de dados. D√≠gite o
seguinte comando: (*coloque o caminho que corresponda a onde voc√™
descarregou suas bases de dados*)

> üá™üá∏ Despu√©s de instalado, debe ser configurada la base de datos, esto
> es, indicar para el programa el camino (*PATH*) donde se encuentra la
> base de datos. D√≠gite el siguiente comando: (\*coloque el camino que
> corresponda a donde ud descarg√≥ sus bases de datos)

    export KRAKEN2_DB_PATH="/home/metagenomica/dbs/Kraken2/"

### 5.2. Anota√ß√£o Funcional

üáßüá∑ Uma vez instaladas todas as ferramentas e descarregadas as bases de
dados, pode proceder √† anota√ß√£o. Neste caso ser√° feita primeiro √†
funcional, usando Diammond e as bases de dados **KEGG** e **EggNOG**. A
continua√ß√£o se encontra o comando ndividual (*uma montagem e uma base de
dados por vez*)

> üá™üá∏ Una vez instaladas todas las herramientas y descargadas las bases
> de datos, puede proceder a la anotaci√≥n. En este caso ser√° hecha
> primero la anotaci√≥n funcional, usando Diammond e las bases de datos
> **KEGG** e **EggNOG**

    ## Crie uma pasta pra sa√≠da
    mkdir 08.FunctionalAnnotation

    ## Diammond
    diamond blastx --more-sensitive --threads 6 -k 1 -f 6 qseqid qlen sseqid sallseqid slen qstart qend sstart send evalue bitscore score length pident qcovhsp --id 60 --query-cover 60 -d dbs/keggdb.dmnd --query 07.GenePrediction/GenesNucl.fa -o 08.FunctionalAnnotation/GenesNucl_kegg.txt --tmpdir /dev/shm

**SINTAXE**

    diamond blastx --more-sensitive --threads -k -f --id --query-cover -d dbs/db.dmnd --query orfs_nucleotides.fa -o annotation.txt --tmpdir /dev/shm

-   `blastx`: Alinha sequ√™ncias de DNA contra uma base de dados de
    prote√≠nas
-   `--more-sensitive`: este modo permite hits com &gt;40% de
    identidade. Existem outros modos
    `--fast --min-sensitive --very-sensitive --ultra-sensitive`. Clique
    [aqui](https://github.com/bbuchfink/diamond/wiki/3.-Command-line-options)
    para mais detalhes
-   `--threads`: n√∫mero de n√∫cleos
-   `-k/--max-target-seqs`: N√∫mero m√°ximo de sequ√™ncias *target* por
    *query* para reportar alinheamentos.
-   `-f/--outfmt`: Formato de sa√≠da. S√£o aceptos os seguintes valores:
    -   `0` Formato BLAST *pairwise*
    -   `5` fomato BLAST XML
    -   `6` Formato do BLAST tabular (default), pode customizar as
        colunas com uma lista separada por espa√ßos, das seguintes
        op√ß√µes:
        -   `qseqid` id da sequ√™ncia *query*
        -   `qlen` tamanho da sequ√™ncia *query*
        -   `sseqid` id da sequ√™ncia da base de dados
        -   `sallseqid` todas os id das sequ√™ncias das bases de dados
        -   `slen` tamanho da sequ√™ncia da base de dados
        -   `qstart` inicio do alinhamento no *query*
        -   `qend` fim do alinhamento no *query*
        -   `sstart` inicio do alinhamento na sequ√™ncia da base de dados
        -   `send` fim do alinhamento na sequ√™ncia da base de dados
        -   `evalue`
        -   `bitscore`
        -   `score`
        -   `length` tamanho do alinhamento
        -   `pident` porcentagem de matches identicos

Com o comando anterior foi feita a anota√ß√£o do co-assembly de todas as
amostras `scaffolds.fasta` com a base de dados `kegg.dmnd` e os dados
foram guardados no arquivo `kegg_annotation.txt`.

> üá™üá∏ Con el comando anterior fue realizada la anotaci√≥n del co-assembly
> de todas las muestras `scaffolds.fasta` con la base de datos
> `kegg.dmnd` y los datos fueron guardadas en el archivo
> `GenesNucl_kegg.txt`.

Se tiver mais de uma montagem e quiser rodar todas e as duas bases de
dados ao mesmo tempo, pode usar o seguinte loop `for`:

> üá™üá∏ Si tiene m√°s de un ensamble y quiere correr todos e las dos bases
> de datos al mismo tiempo, puede usar el siguiente loop `for`:

    for i in 07.GenePrediction/*.fa
    do
    BASE=$(basename $i .fa)
      for j in dbs/*.dmnd
      do
      db=$(basename $j .dmnd)
    diamond blastx --more-sensitive --threads 6 -k 1 -f 6 qseqid qlen sseqid sallseqid slen qstart qend sstart send evalue bitscore score length pident qcovhsp --id 60 --query-cover 60 -d $j --query $i -o 08.FunctionalAnnotation/${BASE}_${db}.txt --tmpdir /dev/shm
    done
    done

Com o comando anterior, √© feita a anota√ß√£o em todas as ORFs preditas na
pasta `07.GenePrediction/` com todas as bases de dados para diammond
dentro da pasta `dbs/`. Veja que no loop foram declaradas duas
variav√©is, `i` que corresponde a cada um dos arquivos das ORFs
(nucleot√≠deos) preditas com Prodigal e a vari√°vel `j` que corresponde a
cada um dos arquivos terminados em `.dmnd` dentro da pasta `dbs/`, ou
seja as bases de dados `kegg.dmnd` e `eggnog.dmnd`. Os arquivos de sa√≠da
s√£o duas tabelas por cada montagem, uma da anota√ß√£o com *eggnog* e outra
com *kegg*.

> üá™üá∏ Con el comado anterior, es realizada la anotaci√≥n de todas las ORF
> predichas en el directorio `07.GenePrediction/` con todas las bases de
> datos para Diammond dentro de la carpeta `dbs/`. Vea que en el loop
> fueron declaradas dos variables, `i` que corresponde a cada uno de los
> archivos de las ORFs (nucle√≥tidos) predichos con Prodigal e la
> variable `j` que corresponde a cada uno de los archivos terminados en
> `.dmnd` dentro de la carpeta `dbs/`, o sea las bases de datos
> `kegg.dmnd` y `eggnog.dmnd`. Los archivos de salida son dos tablas por
> cada ensamble, una con la anotaci√≥n con *eggnog* e otra con *kegg*.

### 5.3 Anota√ß√£o Taxon√¥mica

Para a anota√ß√£o taxon√¥mica ser√° usada a ferramenta Kraken2. Depois de
instalada a ferramenta, descarregada e configurada a base de dados, √©
poss√¨vel rodar o comando para anota√ß√£o. Lembrando que este procedimento
deve ser feito para cada uma das predi√ß√µes de ORFs de cada montagem.

> üá™üá∏ Para la anotaci√≥n taxon√≥mica ser√° usado la herramienta Kraken2.
> Despues de instalada la herramienta, descargada y configurada la base
> de dados, es posible correr el comando para anotaci√≥n. Recordando que
> este procedimiento debe ser hecho apra cada una de las predicciones de
> ORFs de cada ensamble.

    mkdir 09.TaxonomicAnnotation

    kraken2 --db mainDB 07.GenePrediction/GenesNucl.fa > mkdir 09.TaxonomicAnnotation/GenesNucl_tax_annoted.tsv

**SINTAXE**

`kraken2 --db db orfs_nucleotides.fa`

-   `--db`: nome da pasta onde se encontra a base de dados e que foi
    configurada no PATH.
-   `orfs_nucleotides.fa`: Arquivo de sa√≠da da predi√ß√£o de ORFs, en
    formato `.fa` (nucleot√≠deos)

Para rodar num comando s√≥ todas as montagens, pode ser usado o seguinte
loop:

> üá™üá∏ Para correr en un solo comando todas los ensambles, puede ser usado
> el siguiente loop:

    for i in 07.GenePrediction/*.fa
    do
    BASE=$(basename $i .fa)
    kraken2 --db mainDB $i > 09.TaxonomicAnnotation/${BASE}_taxa_annoted.tsv
    done

O arquivo de sa√≠da √© uma tabela `.tsv` por cada montagem. As colunas
est√£o organizadas da seguinte forma:

1.  ‚ÄúC‚Äù/‚ÄúU‚Äù: Para indicar se a sequ√™ncia foi classificada ou n√£o
    classificada (*Unclassified*).
2.  Nome do contig
3.  Identifica√ß√£o Taxon√¥mica
4.  Tamanho da sequ√™ncia em bp.
5.  Mapeamento LCA de cada *k*-mer.

Depois de obtida a tabela de anota√ß√£o taxon√¥mica, √© necess√°rio ordenar
pela primeira coluna (IDs das sequ√™ncias):

    for i in 09.TaxonomicAnnotation/*_taxa_annoted.tsv
    do
    BASE=$(basename $i _taxa_annoted.tsv)
    sort -k1,1 $i > 09.TaxonomicAnnotation/${BASE}_tax_annoted_sorted.tsv

## 6. Mapeamento

Agora precisamos mapear as ORFs anotadas nos contigs gerados no
assembly. A ferramenta principal durante este proceso se chama
[Bowtie2](https://github.com/BenLangmead/bowtie2), a qual √© uma
ferramenta que permite o mapeamento de sequ√™ncias. Tamb√©m ser√° usado o
programa [Samtools](https://github.com/samtools/samtools) para a
transforma√ß√£o e manipula√ß√£o dos arquivos do mapeamento.

> üá™üá∏ Ahora necesitamos mapear los ORFs anotados en los contigs generados
> en el ensamblaje. La herramienta principal durantes este processo se
> llama [Bowtie2](https://github.com/BenLangmead/bowtie2), la cual es
> una herramienta que permite el mapeo de secuencias. Tambi√©n ser√° usado
> el programa [Samtools](https://github.com/samtools/samtools) para la
> transformaci√≥n y manipulaci√≥n de los archivos del mapeo

### 6.1. Instala√ß√£o

Para a instala√ß√£o das ferramentas do mapeamento, crie um ambiente
virtual chamado **Mapping**.

    # Crie o ambiente
    conda create -n mapping

    # Ative o ambiente
    conda activate mapping

#### 6.1.1. Bowtie2

    conda install -c bioconda bowtie2

#### 6.1.2. SamTools

    conda install -c bioconda samtools=1.9

### 6.2 Uso

#### 6.2.1. Extra√ß√£o de Sequ√™ncias anotadas

O primeiro passo √© selecionar a coluna um das tabelas de anota√ß√µes, que
correspondem aos nomes das sequ√™ncias anotadas, para depois extra√≠-las.
Isto pode ser feito usando o comando `cut`.

> üá™üá∏ El primer paso es seleccionar la columna uno de las tablas de
> anotaciones que corresponde a los nombres de las secuencias anotadas,
> para despu√©s extraerlas. Esto puede ser realizado usando o comando
> `cut`.

    cut -f1 08.FunctionalAnnotation/GenesNucl_keggdb.txt > 08.FunctionalAnnotation/GenesNucl_keggdb_contigsIDs.txt

Com o loop, voc√™ consegue fazer ao mesmo tempo o processo para todas as
tabelas de anota√ß√µes que tiver (p.e. mais de um assembly e/ou mais de
uma base de dados).

> üá™üá∏ Con el loop, usted consigue hacer al mismo tiempo el proceso para
> todas las tablas de anotaciones que tenga (p.e. m√°s de un ensamble y/o
> m√°s de una base de datos)

**Loop**

    for i in 08.FunctionalAnnotation/*.txt; do BASE=$(basename $i .txt); cut -f1 $i > 08.FunctionalAnnotation/${BASE}_contigsIDs.txt; done

**SINTAXE** `cut [options] file`

-   `-f`: Fields (colunas) selecionadas

No comando anterior, basicamente s√£o criados novos arquivos
(`_contigsIDs.txt`) com a primeira coluna das tabelas de anota√ß√£o, que
cont√©m os IDs.

A continua√ß√£o, use os √∫ltimos arquivos gerados
(`GenesNucl_keggdb_contigsIDs.txt`) para extrair as sequ√™ncias desses
genes anotados. Ou seja, use a lista dos IDs, para que sejam procuradas
as sequ√™ncias anotadas dentro dos arquivos dos genes preditos (i.e
`GenesNucl.fa`). Este processo √© feito com um script escrito na
linguagem [*perl*](https://www.perl.org/). Rode este comando para cada
anota√ß√£o ou base de dados usada (i.e.¬†kegg e eggnog).

> üá™üá∏ Con el comando anterior, basicamente fueron creados los nuevos
> archivos (`_contigsIDs.txt`) con la primera columna de las tablas de
> anotaci√≥n, que contienen los IDs.
>
> A continuaci√≥n, use los ultimos archivos generados
> (`GenesNucl_keggdb_contigsIDs.txt`) para extraer las secuencias de
> esos genes anotados. O sea, use la lista de los IDs, para que sean
> buscadas las secuencias anotadas dentro de los archivos de predicci√≥n
> de genes (i.e.¬†`GenesNucl.fa`). Este proceso es hecho con un script
> escrito en el lenguaje [*perl*](https://www.perl.org/). Corra este
> comando para cada anotaci√≥n o base de datos usada (i.e.¬†kegg y
> eggnog).

**KEGG**

    mkdir 10.Mapping

    perl -ne 'if(/^>(\S+)/){$c=$i{$1}}$c?print:chomp;$i{$_}=1 if @ARGV' 08.FunctionalAnnotation/GenesNucl_keggdb_contigsIDs.txt 07.GenePrediction/GenesNucl.fa > 10.Mapping/GenesNucl_keggdb_seqs.fa

**SINTAXE** `perl script input1 input2`

-   `input1`: IDs das sequ√™ncias
-   `input2`: Sequ√™ncias Genes preditos (output Prodigal)

**Loop**

    # Crie uma nova pasta
    mkdir 10.Mapping

    ## KEGG
    for i in 08.FunctionalAnnotation/*_ekeggdb_contigsIDs.txt; do BASE=$(basename $i _keggdb_contigsIDs.txt);   for j in 07.GenePrediction/${BASE}.fa;   do   ID=$(basename $j ${BASE}.fa); perl -ne 'if(/^>(\S+)/){$c=$i{$1}}$c?print:chomp;$i{$_}=1 if @ARGV' $i $j > 10.Mapping/${BASE}_keggdb_seqs.fa; done; done

    ## EggNOG
    for i in 08.FunctionalAnnotation/*_eggnog_contigsIDs.txt; do BASE=$(basename $i _eggnog_contigsIDs.txt);   for j in 07.GenePrediction/${BASE}.fa;   do   ID=$(basename $j ${BASE}.fa); perl -ne 'if(/^>(\S+)/){$c=$i{$1}}$c?print:chomp;$i{$_}=1 if @ARGV' $i $j > 10.Mapping/${BASE}_eggnog_seqs.fa; done; done

Use o comando `ls` para listar o conte√∫do da pasta `10.Mapping/`.
Perceba que tem dois arquivos por cada amostras ou montagem, um com as
sequ√™ncias anotadas com KEGG, e outra com as sequ√™ncias anotadas com
EggNOG.

> üá™üá∏ Use el comando `ls` para listar el contenido de la carpeta
> `10.Mapping/`. Perciba que tiene dos archivos para cada muestra o
> ensamble, uno con las secu√™ncias anotadas con KEGG y otro con las
> secuencias anotadas con EggNOG.

#### 6.2.3. Mapeamento

**1. Cria√ß√£o do √≠ndice**

Ap√≥s a formata√ß√£o das tabelas de anota√ß√µes e a extra√ß√£o das sequ√™ncias,
ser√° iniciado o processo de mapeamento. O primeiro paso √© criar um √≠ndex
das sequ√™ncias anotadas e extra√≠das (`GenesNucl_keggdb_seqs.fa`), usando
**Bowtie2**.

> üá™üá∏ Despu√©s de la formataci√≥n de las tablas de anotaciones e la
> extracci√≥n de las secuencias, ser√° iniciado el proceso de mapeo. El
> primero paso es crear un √≠ndice de las secuencias anotadas y extraidas
> (`GenesNucl_keggdb_seqs.fa`), usando **Bowtie2**.

Entre na pasta do mapeamento:

    cd 10.Mapping/

Rode este comando para cada arquivo de sequ√™ncias extra√≠das por base de
dados que tiver:

    bowtie2-build GenesNucl_keggdb_seqs.fa GenesNucl_keggdb_seqs.fa

**SINTAXE** `bowtie2-build refer√™ncia √≠ndice_name`

-   `refer√™ncia`: arquivo `.fasta` com as sequ√™ncias
-   `√≠ndice_name`: nome do √≠ndice

**Loop**

O loop √© muito √∫til se tiver mais de uma montagem. Assim, criar√° um
√≠ndice para cada montagem.

    for i in ./*_seqs.fa; do BASE=$(basename $i _seqs.fa); bowtie2-build $i $i; done

**2. Obten√ß√£o arquivos `.sam`**

Assim que criado(s) o(s) √≠ndice(s), pode se proceder ao comando do
mapeamento, que ir√° criar arquivos `.sam` (*Sequence Alignment Map*)
para cada amostra ou assembly e cada base de dados.

> üá™üá∏ Despu√©s de creados los √≠ndices, puede se proceder al comando de
> mapeo, que ir√° crear los archivos `.sam` (*Sequence Alignment Map*)
> para cada muestra o para cada ensamble y cada base de datos.

    bowtie2 -p 6 -x GenesNucl_keggdb_seqs.fa -1  ../02.CleanData/Sample1_1_paired.fq.gz -2 ../02.CleanData/Sample1_2_paired.fq.gz -S Sample1_kegg.sam

**SINTAXE** `bowtie2 -p threads -x index -1 pair1 -2 pair2 -S file.sam`

-   `-p`: N√∫mero de threads/ n√∫cleos
-   `-x`: √≠ndice (bowtie2-build)
-   `-S`: Archivo de sa√≠da em formato `.sam`.

Se quise ter mais informa√ß√µes sobre os arquivos
[`.sam`](https://en.wikipedia.org/wiki/SAM_(file_format))

**Loop**

Com o seguinte loop, √© poss√≠vel fazer o mapeamento de v√°rias amostras
usando o mesmo √∫nico √≠ndice que proveem de uma √∫nicam montagem (caso
deste tutorial).

> üá™üá∏ Con el siguiente loop, es posible hacer el mapeo de varias muestras
> usando el mismo √∫nico √≠ndice que proviene de un √∫nico ensamble (el
> caso de este tutorial).

    ## KEGG
    for i in ../02.CleanData/*_1_paired.fq.gz; do BASE=$(basename $i _1_paired.fq.gz); for j in ./*keggdb_seqs.fa; do bowtie2 -p 6 -x $j -1 $i -2 ../02.CleanData/${BASE}_2_paired.fq.gz -S ${BASE}_kegg.sam; done; done

    ## EGGNOG
    for i in ../02.CleanData/*_1_paired.fq.gz; do BASE=$(basename $i _1_paired.fq.gz); for j in ./*eggnog_seqs.fa; do bowtie2 -p 6 -x $j -1 $i -2 ../02.CleanData/${BASE}_2_paired.fq.gz -S ${BASE}_eggnog.sam; done; done

Ent√£o ao finalizar o processo, ter√° dois arquivos `.sam` por cada
amostra (Kegg, EggNOG).

**3. `.sam` para `.bam`**

Para facilidade na manipula√ß√£o, os arquivos `.sam` devem ser
transformados a `.bam` usando a ferramenta **SamTools**.

> üá™üá∏ Para facilidad en la manipulaci√≥n, los archivos `.sam` deben ser
> transforamdos a `.bam` usando la herramienta **SamTools**.

    samtools view -b -S -o Sample1_kegg.bam Sample1_kegg.sam

**SINTAXE** `samtools view -b -S -o output input`

-   `-b`: transformar a \`.bam¬¥
-   `-S`: desde `.sam`
-   `-o`: `output.bam`
-   `input`: `input.sam`

**Loop** Com o loop, consegue transformar todos os arquivos `.sam` para
`.bam`.

    for i in ./*.sam
    do
    BASE=$(basename $i .sam)
    samtools view -b -S -o ${BASE}.bam $i
    done

Agora voc√™ tem os mapeamentos tanto em formato `.sam` como `.bam`.

**4. Ordenando e Indexando**

O seguinte paso √© ordenar os arquivos `.bam`

    # Ordenando
    samtools sort Sample1_kegg.bam -o Sample1_kegg_sorted.bam

    # Indexando
    samtools index Sample1_kegg_sorted.bam

**SINTAXE** `samtools sort input -o output_sorted.bam`

`samtools index output_sorted.bam`

**Loop**

    # Ordenando
    for i in ./*.bam
    do
    BASE=$(basename $i .bam)
    samtools sort $i -o ${BASE}_sorted.bam
    done

    # Indexando
    for i in ./*_sorted.bam
    do
    BASE=$(basename $i _sorted.bam)
    samtools index $i
    done

**5. Criando as estat√≠sticas da anota√ß√£o funcional**

Neste paso ser√£o geradas as estat√≠sticas da anota√ß√£o funcional.

    # Estat√≠sticas
    samtools idxstats Sample1_kegg_sorted.bam > ../08.FunctionalAnnotation/Sample1_kegg_annotation_stats.txt

    cd ../08.FunctionalAnnotation/

    # Ordenando
    sort -k1,1 Sample1_kegg_annotation_stats.txt > Sample1_kegg_annotation_stats_sorted.txt

**SINTAXE** `samtools idxstats sorted.bam > table.txt`

`sort -k table.txt > table_sorted.txt`: Classifique (ordene) o arquivo
`table.txt` com base na coluna 1 e salve no arquivo `table_sorted.txt`

**Loop**

    # Estat√≠sticas
    for i in ./*_sorted.bam
    do
    BASE=$(basename $i _sorted.bam)
    samtools idxstats $i > ../08.FunctionalAnnotation/${BASE}_stats.txt
    done

    cd ../08.FunctionalAnnotation/

    # Ordenando
    for i in ./*_stats.txt
    do
    BASE=$(basename $i _stats.txt)
    sort -k1,1 $i > ${BASE}_sorted.txt
    done

Use o comando `head` para dar uma olhada em algum dos arquivos (i.e
`head Sample1_kegg_sorted.txt`).

Em cada um desses arquivos ter√° 4 colunas:

1.  nome da sequ√™ncia
2.  tamanho da sequ√™ncia
3.  quantidade de reads mapeados
4.  quantidade de reads n√£o mapeados

## 7. Constru√ß√£o Tabela Final

Finalmente √© necess√°rio construir uma tabela final com todas as
anota√ß√µes (taxon√¥mica e funcional) com todas as bases de dados e de
todas as amostras

> üá™üá∏ Finalmente es necesario construir una tabla final con todas las
> anotaciones (taxon√≥mica y funcional) con todas las bases de dados y de
> todas las muestras.

### 7.1. An√°lise Rand√¥mico do KEGG

Este processo consiste em completar a informa√ß√£o da anota√ß√£o do Kegg. Na
anota√ß√£o funcional foram obtidos os n√∫meros KEGG ou KO (*Kegg
Orthologies*) das sequ√™ncias anotadas. No entanto esse KO n√£o nos d√°
informa√ß√£o do nome do gene, ou do metabolismo ao que pertence. Para
isto, nesta fase ser√£o adicionadas essas informa√ß√µes.

Fa√ßa [download](https://figshare.com/ndownloader/files/33953774) da
tabela `kegg.tsv` que cont√©m todas as informa√ß√µes da base de dados KEGG

> üá™üá∏ Este proceso consiste en cpmpletar la informaci√≥n de la anotaci√≥n
> de Kegg. En la anotaci√≥n funcional fueron obtenidos los n√∫meros KEGG o
> KO (*Kegg Otrhologies*) de las secuencias anotadas. Sin embargo ese KO
> no nos da informaci√≥n del nombre del gen, o del metabolismo al que
> pertenece. Para esto, en esta fase ser√°n adicionadas esas
> informaciones
>
> Haga el [download](https://figshare.com/ndownloader/files/33953774) de
> la tabla `kegg.tsv` que contiene todas las informaciones de la base de
> datos KEGG.

    # Voltando na pasta base
    cd ../

    # Download
    curl -L https://figshare.com/ndownloader/files/33953774 -o kegg.tsv

A continua√ß√£o ordene o arquivo pelo c√≥digo KO:

    sort -k1,1 kegg.tsv > kegg_sorted.tsv

Agora √© momento de trabalhar na formata√ß√£o das tabelas de anota√ß√£o de
cada amostra:

1.  **Separando a terceira coluna**

A terceira coluna da tabela original da anota√ß√£o taxon√¥mica, traz o
c√≥digo de acesso do NCBI e separado por \| o n√∫mero KO, por exemplo:
WP000000.0 \| K00001. O seguinte comando separa essas informa√ß√µes em
dois colunas usando um script de *Perl*:

> üá™üá∏ La tercera columna de la tabla original de la anotaci√≥n taxon√≥mica,
> tiene el c√≥digo de acceso del NCBI y separada por \|, el n√∫mero KO,
> por exemplo: WP000000.0 \| K00001. El siguiente comando separa esas
> informaciones en dos columnas usando un script de *Perl*:

    mkdir 11.RandomicAnalyses

    for i in 08.FunctionalAnnotation/*_keggdb.txt
    do
    BASE=$(basename $i _keggdb)
    perl -pe 's/\|?(?:\s+gi|ref)?\|\s*/\t/g' $i > 11.RandomicAnalyses/${BASE}.txt
    done

2.  **Extraindo os n√∫meros KO**

<!-- -->

    for i in 11.RandomicAnalyses/*_keggdb.txt; do BASE=$(basename $i _keggdb.txt); cut -f1,4 $i > 11.RandomicAnalyses/${BASE}_kegg_ids.txt; done

O comando anterior usa o comando `cut` para cortar as colunas 1 (IDs das
sequ√™ncias) e 4 (Kegg IDs/ N√∫meros KO).

3.  **Ordenando pelos n√∫meros KO**

A continua√ß√£o, usando o comando `sort`, ser√° organizada a tabela pelos
n√∫meros KO:

    for i in 11.RandomicAnalyses/*_ids.txt
    do
    BASE=$(basename $i _ids.txt)
    sort -k2,2 $i > 11.RandomicAnalyses/${BASE}_sorted.txt
    done

4.  **Adicionando a informa√ß√£o completa do KEGG**

A seguinte etapa, compreende o uso da tabela ordenada das informa√ß√µes do
KEGG `kegg_sorted.tsv`, que contem todas as categorias metab√≥licas para
cada n√∫mero KO para adicionar estas informa√ß√µes na tabela sa√≠da do paso
anterior (`GeneNucl_sorted.txt`). A linha de comando a continua√ß√£o
cont√©m v√°rios comando anidados separados por \|. Isto indica para o
sistema que a sa√≠da de um comando √© a entrada do seguinte comando.

    for i in 11.RandomicAnalyses/*_kegg_sorted.txt; do BASE=$(basename $i _kegg_sorted.txt); cat $i | while read line ; do echo "$line" | join -1 2 -2 1 -e"NA" -o1.1,0,2.2,2.3,2.4,2.5 -t $'\t' - kegg_sorted.tsv | shuf -n1 >> 11.RandomicAnalyses/${BASE}_keggs_randomic_analysis.tsv; done; done

5.  **Ordenando a tabela pelos IDs das sequ√™ncias**

Agora √© momento de ordenar a tabela pela primeira coluna, IDs das
sequ√™ncias.

    for i in 11.RandomicAnalyses/*_randomic_analysis.tsv
    do
    BASE=$(basename $i _keggs_randomic_analysis.tsv)
    sort -k1,1 $i > 11.RandomicAnalyses/${BASE}_keggs_randomic_analysis_sorted.tsv

------------------------------------------------------------------------

## Em constru√ß√£o‚Ä¶
