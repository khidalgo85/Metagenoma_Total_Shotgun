---
output:
    github_document:
    pandoc_args: --webtex
always_allow_html: true
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "imgs/",
  out.width = "100%"
)

knitr::opts_chunk$set(echo = TRUE)
```


<!-- badges: start -->
![forthebadge](https://img.shields.io/badge/GEMM-Building-orange)
![forthebadge](https://forthebadge.com/images/badges/built-with-science.svg)

<!-- badges: end -->

# AnÃ¡lise do Metagenoma total - Shotgun <img src="imgs/1.png" align="right" width = "120px"/>
**Autor: MsC. Kelly Hidalgo**

ğŸ‡§ğŸ‡· Pipeline para a montagem e anotaÃ§Ã£o funcional de metagenomas totais. Este pipeline contempla todas as etapas do processamento, desde a avaliaÃ§Ã£o da qualidade das sequÃªncias, trimagem, montagem, cÃ¡lculo da cobertura, prediÃ§Ã£o e anotaÃ§Ã£o funcional e taxonÃ´mica dos genes. 

> ğŸ‡ªğŸ‡¸Pipeline para montaje y anotaciÃ³n funcional de metagenomas totales. Este pipeline contempla todas las etapas del procesamiento, desde la evaluaciÃ³n de la calidad de las secuencias, *trimming*, montaje, cÃ¡lculo de la cobertura, predicciÃ³n y anotaciÃ³n funcional y taxonÃ³mica de genes. 

## Ferramientas bioinformÃ¡ticas

### InstalaÃ§Ã£o Anaconda

ğŸ‡§ğŸ‡· Ã‰ recomendÃ¡vel instalar Anaconda, pois Ã© a forma mais fÃ¡cil para instalar as ferramentas bioinformÃ¡ticas necessÃ¡rias pro desenvolvimento deste pipeline. Anaconda Ã© uma distribuiÃ§Ã£o livre e aberta das linguagens *Python* e *R*, utilizada na ciÃªncia de dados e bioinformÃ¡tica. As diferente versÃµes dos programas se administram mediante um sinstema de gestÃ£o chamado *conda*, o qual faz bastante simples instalar, rodar e atualizar programas. [Aqui](https://conda.io/projects/conda/en/latest/user-guide/install/index.html) se encontram as instruÃ§Ãµes para a instalaÃ§Ã£o de Anaconda. 

Depois de instalado, *Anaconda* e o gestor *Conda*, podram ser criados *ambientes virtuais* par a instalaÃ§Ã£o das diferentes ferramentas bioinformÃ¡tica que serÃ£o usadas. 

> ğŸ‡ªğŸ‡¸ Es recomendable instalar Anaconda, pues es la forma mÃ¡s fÃ¡cil para instalar las herramientas bioinformÃ¡ticas necesarias para el desarrollo de este pipeline. Anaconda es una distribuciÃ³n libre y abierta de los lenguajes *Python* y *R*, utilizada en ciencia de datos y bioinformÃ¡tica. Las diferentes versiones de los programas se administran mediante un sistema de gestiÃ³n llamado *conda*, el cual hace bastante sencillo instalar, correr y actualizar programas.  [Aqui](https://conda.io/projects/conda/en/latest/user-guide/install/index.html) se encuentran las instrucciones para la instalaciÃ³n de Anaconda.
>
> DespuÃ©s de instalado *Anaconda* y su gestor *Conda*, podran ser creados *ambientes virtuales* para la instalaciÃ³n de las diferentes herramientas bioinformÃ¡ticas que serÃ¡n usadas.

---
# I. MetagenÃ´mica

## 0. Organizando os dados

### 0.1. SequÃªncias

ğŸ‡§ğŸ‡· Em este tutorial serÃ£o usadas seis metagenomas exemplo para rodar todo o *pipeline*. Descarregue os [*datasets*]() usando o comando `wget`.

> ğŸ‡ªğŸ‡¸ En este tutorial serÃ¡n usados seis metagenomas ejemplo para correr todo el *pipeline*. Descargue los [*datasets*]() usando el comando `wget`.

**Arquivos**

* `sample1_1.fq.gz` e `sample1_2.fq.gz`: Amostra 1
* `sample2_1.fq.gz` e `sample2_2.fq.gz`: Amostra 2
* `sample3_1.fq.gz` e `sample3_2.fq.gz`: Amostra 3
* `sample4_1.fq.gz` e `sample4_2.fq.gz`: Amostra 4
* `sample5_1.fq.gz` e `sample5_2.fq.gz`: Amostra 5
* `sample6_1.fq.gz` e `sample6_2.fq.gz`: Amostra 6

A continuaÃ§Ã£o encontrarÃ¡ uma sÃ©rie de comandos para organizar adequadamente os diretÃ³rios com as amostras.

```
## Crie um diretÃ³rio raiz para todo o processo
mkdir metagenomica

## Entre al nuevo directorio
cd metagenomica/

## Crie um novo diretÃ³rio para colocar os dados brutos
mkdir 00.RawData

## Entre em 00.RawData
cd 00.RawData/
```
Use o comando `mv` para mover os arquivos atÃ© o diretÃ³rio `00.RawData/`. 

No final do processo de organizaÃ§Ã£o deve ver seus diretÃ³rios assim:
`ls 00.RawData/`

```
sample1_1.fq.gz   sample1_2.fq.gz   sample2_1.fq.gz   sample2_2.fq.gz   sample3_1.fq.gz   sample3_2.fq.gz   sample4_1.fq.gz   sample4_2.fq.gz   sample5_1.fq.gz   sample5_2.fq.gz   sample6_1.fq.gz   sample6_2.fq.gz
```

Ã‰ fortmente recomendado rodar os comandos desde o diretÃ³rio base, que neste caso Ã©: `metagenomica/`

## 1. Controle da Qualidade

## 1.1. AvaliaÃ§Ã£o da qualidade

ğŸ‡§ğŸ‡· Para a avaliaÃ§Ã£o da qualidade serÃ¡ usado o programa [FastQC](http://www.bioinformatics.babraham.ac.uk/projects/fastqc/) que Ã© uma ferramenta que permite observar graficamente a qualidade das sequencias de Illumina. 

> ğŸ‡ªğŸ‡¸ Para la evaluaciÃ³n de la calidad serÃ¡ usado el programa [FastQC](http://www.bioinformatics.babraham.ac.uk/projects/fastqc/) que es una herramienta que  permite observar graficamente la calidad de las secuencias de Illumina. 

### 1.1.1. InstalaÃ§Ã£o 

Las instruÃ§Ãµes para a instalaÃ§Ã£o usando conda se encontram [aqui](https://anaconda.org/bioconda/fastqc). No entanto neste tutorial tambÃ©m serÃ£o apresentados.

Como jÃ¡ foi explicado anteriormente, com conda Ã© possÃ­vel criar ambientes virtuais para instalar as ferramentas bioinformÃ¡ticas. O primeiro ambiente que serÃ¡ criado se chamarÃ¡ **QualityControl**, onde se instalaram os programas relacionados com esse processo.

> ğŸ‡ªğŸ‡¸ [FastQC](http://www.bioinformatics.babraham.ac.uk/projects/fastqc/) es una herramienta para evaluar graficamente la calidad de las secuencias de Illumina. 
>
> Las instrucciones para instalaciÃ³n usando conda se encuentran [aqui](https://anaconda.org/bioconda/fastqc). Sin embargo aqui en este tutorial tambiÃ©n serÃ¡n presentadas
>
> Como ya fue explicado anteriorimente, con conda es posible crear ambientes virutuales para instalar las herramientas bioinformÃ¡ticas. El primer ambiente que serÃ¡ creado se llamarÃ¡ **QualityControl**, donde se instalaran los programas relacionados con este proceso.

```
conda create -n QualityControl
```
ğŸ‡§ğŸ‡· Durante o processo, o sistema perguntarÃ¡ se deseja proceder com a creaÃ§Ã£o do ambiente, com as opÃ§Ãµes y/n (sim ou nÃ£o). Escreva `y` e depois disso o ambiente virutal estarÃ¡ criado.

Para instalar as ferramentas dentro do ambiente anteriormente criado, Ã© necessÃ¡rio ativÃ¡-lo.

> ğŸ‡ªğŸ‡¸ Durante el proceso, el sistema preguntarÃ¡ sÃ­ desea proceder con la creaciÃ³n del ambiente, con las opciones y/n (si o no). Escriba `y` y despuÃ©s de eso el ambiente virtual estarÃ¡ creado.
>
> Para instalar las herramientas dentro del ambiente anteriormente creado, es necesario activarlo

```
conda activate QualityControl
```
ğŸ‡§ğŸ‡· O ambiente estarÃ¡ ativo quando o nome se encontre ao comeÃ§o da linha do comando, asssim: `(QualityControl) user@server:~/$`.
Posteriormente se procede Ã  instalaÃ§Ã£o do programa:

> ğŸ‡ªğŸ‡¸ El ambiente estarÃ¡ activo cuando el nombre de Ã©ste se encuentra en el comienzo de la linea de comando, asÃ­: `(QualityControl) user@server:~/$`.
> 
> Posteriormente se procede a la instalaciÃ³n del programa:

```
conda install -c bioconda fastqc
```

### 1.1.2. Uso

ğŸ‡§ğŸ‡· A primeira etapa do processo Ã© a avaliaÃ§Ã£o da qualidade das sequÃªncias cortas (Illumina paired end) usando *FastQC*, com o objetivo de determianr se Ã© necessÃ¡rio trimar ou filtrar as sequÃªncias da baixa qualidade para nos prÃ³ximos pasos. 

Esta etapa Ã© para identificar principalmente as sequÃªncias *outlier* com baixa qualidade ($Q<20$)

Ative o ambiente `QualityControl`:

> ğŸ‡ªğŸ‡¸ La primera etapa del proceso es la evaluaciÃ³n de la calidad de las secuencias cortas (Illumina paired end) usando *FastQC*, con el objetivo de determinar sÃ­ es necesario trimar o filtrar las secuencias de baja calidad en los prÃ³ximos pasos. 
>
> Ã‰sta etapa es para identificar principalmente las secuencias *outlier* con baja calidad ($Q<20$).
>
> Active el ambiente `QualityControl`:

```
conda activate QualityControl

## Onde vc estÃ¡?
pwd
```

ğŸ‡§ğŸ‡· Deve estar em `~/metagenomica/`.. Se esse nÃ£o Ã© o resultado del comando `pwd`, use o comando `cd` para chegar no diretÃ³rio desejado.

> ğŸ‡ªğŸ‡¸ Debe estar em `~/metagenomica/`. Si ese no es el resultado del comando `pwd`, use el comando `cd` para llegar en el directorio base.

Execute  **FastQC**:
```
## Crie um directÃ³rio para salvar o output do FastQC
mkdir 01.FastqcReports
## Run usando 10 threads
fastqc -t 10 00.RawData/* -o 01.FastqcReports/
```

**Sintaxe**
`fastqc [opÃ§Ãµes] input -o output`

ğŸ‡§ğŸ‡· O comando `fastqc` tem vÃ¡rias opÃ§Ãµes ou parÃ¢metros, entre eles, escolher o nÃºmero de nÃºcleos da mÃ¡quina para rodar a anÃ¡lise, para este exemplo `-t 10`. O input Ã© o diretÃ³rio que contem as sequÃªncias `00.RawData/*`, o `*` indica ao sistema que pode analisar todos os arquivos que estÃ£o dentro desse diretÃ³rio. O output, indicado pelo parÃ¢mtero `-o`, Ã© o diretÃ³rio onde se deseja que sejam guardados os resultados da anÃ¡lise. A continuaÃ§Ã£o se encontram uma explicaÃ§Ã£o detalhada de cada output gerado.

> ğŸ‡ªğŸ‡¸ El comando `fastqc` tiene varias opciones o parametros, entre ellas, escoger el nÃºmero de nÃºcleos de la mÃ¡quina para correr el anÃ¡lisis, para este caso `-t 10`. El input es el directorio que contiene las secuencias `00.RawData/*`, el `*` indica al sistema que puede analizar todos los archivos que estÃ¡n dentro de ese directorio. El output, indicado por el parametro `-o`, es el directorio donde se desea que sean guardados los resultados del anÃ¡lisis. A continuaciÃ³n se encuentra una explicaciÃ³n detallada de cada output generado.

**Outputs**

ğŸ‡§ğŸ‡· 

* Reportes html `.html`: Aqui Ã© possÃ­vel ver toda informaÃ§Ã£o de qualidade graficamente. 

* Zip files `.zip`: Aqui se encontram cada um dos grÃ¡ficos de maneira separada. **IGNORE**

Descarregue os arquivos `html` e explore no seu *web browser*. 

Observe as estatÃ­sticas bÃ¡sicas que se encontram na primeira tabela. AlÃ­, vocÃª pode saber quantas sequÃªncias tem, o tamanho e o %GC. O grÃ¡fico mais importante para saber a quealidade das leituras, Ã© o primeiro, *Per base sequence quality*. Este grÃ¡fico Ã© um boxplot com a distribuiÃ§Ã£o dos valores de qualidade *Phred Score* (eje y) em cada um dos nucleotÃ­deos das leituras (eje x). Se consideram sequÃªncias de excelente qualidade quando o *Phred Score > 30*. Ã‰ norla que o pair 2 apresente uma qualidade um pouco inferior ao pair 1.

> ğŸ‡ªğŸ‡¸ Observe las estadÃ­sticas bÃ¡sicas que se encuentran en la primera tabla. AllÃ­, ud puede saber cuantas secuencias tiene, el tamaÃ±o y el %GC. El grÃ¡fico mÃ¡s importante para saber la calidad de las lecturas es el primero, *Per base sequence quality*. Este grÃ¡fico es un boxblot con la distribuciÃ³n de los valores de calidad *Phred Score* (eje y) en cada uno de los nucleÃ³tidos de las lecturas (eje x). Se consideran secuencias de excelente calidad cuando el *Phred Score > 30*. Es normal que el pair 2 presente una calidad un poco inferior al pair 1. 


### 1.2. Trimagem

> ğŸ‡ªğŸ‡¸ 1.2 DepuraciÃ³n


ğŸ‡§ğŸ‡· [Trimmomatic](http://www.usadellab.org/cms/?page=trimmomatic) Ã© um programa pra filtrar (remover) leituras ou *reads* curtas de baixa qualidade.

Trimmomatic tem vÃ¡rios parÃ¢metros que podem ser considerados para filtrar leituras com baixa qualidade. No presente tutorial usaremos alguns deles. Se quiser saber que otros parÃ¢metros e como funciona cada um deles, consulte o [manual](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf).


> ğŸ‡ªğŸ‡¸ [Trimmomatic](http://www.usadellab.org/cms/?page=trimmomatic) es un programa para filtrar (remover) lecturas o *reads* cortas de baja calidad.
>
> Trimmomatic tiene vÃ¡rios parametros que pueden ser considerados para filtrar lecturas con baja calidad. Aqui usaremos algunos. Si quiere saber que otros parametros y como funciona cada uno de ellos, consulte el [manual](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf).

### 1.2.1. InstalaÃ§Ã£o 

ğŸ‡§ğŸ‡· Como se trata de uma ferramenta que participa dentro do processo de control de qualidade, serÃ¡ instalada dentro do ambiente virtual **QualityControl**.

> Como se trata de una herramienta que participa dentro del proceso de control de calidad, serÃ¡ instalada dentro del ambiente virtual **QualityControl**

```
# Si no estÃ¡ activado el ambiente
conda activate QualityControl

# Instale Trimmomatic
conda install -c bioconda trimmomatic
```

### 1.2.2. Uso

ğŸ‡§ğŸ‡· Segundo foi avaliado no controle de qualidade, pode ser necessÃ¡rio filtrar algumas leituras com qualidade baixa.

O programa Trimmomatic tem vÃ¡rios parÃ¢metros que podem ser considerados para filtrar reads com baixa qualidade. Aqui usaremos alguns. Se quer saber que outros parÃ¢metros e como funciona cada um deles, consulte o [manual](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf).

Para os dados aqui analizados se usara a seguinte linha de comando:

> ğŸ‡ªğŸ‡¸ SegÃºn fue evaluado en el control de calidad, puede ser necesario filtrar algunas lecturas con calidad baja.
>
> El programa Trimmomatic tiene vÃ¡rios parametros que pueden ser considerados para filtrar lecturas con baja calidad. Aqui usaremos algunos. Si quiere saber que otros parametros y como funciona cada uno de ellos, consulte el [manual](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf).
> 
> Para los datos aqui analizados se usarÃ¡ la siguiente linea de comando:

```
# Activa o ambiente QualityControl
conda activate QualityControl

# Crie uma pasta para salvar as reads limpas
mkdir 02.CleanData

# Crie uma pasta para salvar as reads nÃ£o pareadas
mkdir unpaired

# Corra Trimmomatic
trimmomatic PE -threads 10 00.RawData/sample1_1.fastq.gz 00.RawData/sample1_2.fastq.gz 02.CleanData/sample1_1_paired.fastq.gz unpaired/sample1_1_unpaired.fastq.gz 02.CleanData/sample1_2_paired.fastq.gz unpaired/sample1_2_unpaired.fastq.gz LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:150
```

ğŸ‡§ğŸ‡· Com o comando anterior vocÃª tem que rodar a linha de comando para cada amostra. Se quiser rodar todas as amostras de maneira automÃ¢tica Ã© possÃ­vel usar um *loop* `for` para executar esta tarefa.

> ğŸ‡ªğŸ‡¸ Con el comnado anterior ud tiene que correr esa lÃ­nea de comando para cada muestra. Si quiere correr todas las muestras de manera automÃ¡tica es posible usar un *loop* `for` para ejecutrar esta tarea. 

```
# loop
for i in 00.RawData/*1.fastq.gz 
do
BASE=$(basename $i 1.fastq.gz)
trimmomatic PE -threads 10 $i  00.RawData/${BASE}2.fastq.gz 02.CleanData/${BASE}1_paired.fq.gz unpaired/${BASE}1_unpaired.fq.gz 02.CleanData/${BASE}2_paired.fq.gz unpaired/${BASE}2_unpaired.fq.gz LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:100
done
```

**Sintaxe**
`trimmomatic PE -threads input_forward input_reverse output_forward_paired output_forward_unpaired output_reverse_paired output_reverse_unpaired [opÃ§Ãµes]`

ğŸ‡§ğŸ‡· O comando anterior tem muitas partes. Primeiro, o nome do comando Ã© `trimmomatic`, a continuaÃ§Ã£o a opÃ§Ã£o `PE` indica para o programa que as sequÃªncias que irÃ£o ser analisadas sÃ£o de tipo *paired end*. Depois se encontram os inputs, forward (pair1) e reverse (pair2). Depois estÃ£o os outputs, sendo o primeiro, as sequÃªncias forward pareadas (limpas) e nÃ£o pareadas ("descartadas") e depois igual para as sequÃªncias reverse. Por Ãºltimo se encontram os parÃ¢metros de filtragem. Para este caso usamos os parÃ¢metros `SLIDINGWINDOW`, `LEADING` e `TRAILING`. O primeiro de eles, gera uma janela deslizante, que em este caso vai de 4 em 4 bases, cÃ¡lcula a mÃ©dia do *Phred Score* e se estiver por baixo de 15 essas bases serÃ£o cortadas. `LEADING` corta bases do comeÃ§o da leitura que estejam por debaixo do *threshold* de qualidade, igualmente faz o `TRAILING` mas no final das leituras. `MINLEN` elimina todas as reads com tamanho menor ao informado. Trimmomatic tem muitos mais parÃ¢metros para customizar, veja no [manual](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf).

Depois de rodar Trimmomatic Ã© necessÃ¡rio avaliar a qualidade das sequÃªncias limpas usando novamente FastQC.

> ğŸ‡ªğŸ‡¸ El comando anterior tiene muchas partes. Primero, el nombre del comando es `trimmomatic`, a continuaciÃ³n la opciÃ³n `PE` indica para el programa que las secuencias que irÃ¡n a ser analizadas son de tipo *paired end*. DespuÃ©s se encuentran los inputs, forward (pair1) y reverse (pair2). DespuÃ©s son los outputs, siendo primero las secuencias forward pareadas (limpias) y no pareadas ("descartadas") y despuÃ©s las secuencias reverse. Por Ãºltimo se encuentran los parametros de filtrado. Para este caso usamos los parametros `SLIDINGWINDOW`, `LEADING` y `TRAILING`. El primero de ellos, genera una ventana deslizante, que en este caso va de 4 en 4 bases, cÃ¡lcula el promedio del *Phred Score* y si estÃ¡ por debajo de 15 esas bases son cortadas. `LEADING` corta bases del comienzo de la lectura si estÃ¡n por debajo de *threshold* de calidad, lo mismo hace `TRAILING` pero al final de las lecturas. `MINLEN` elimina todas las lecturas con tamaÃ±o menor al informado. Trimmomatic tiene muchos mÃ¡s parÃ¡metros customizables, revise en el  [manual](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf).
>
> DespuÃ©s de correr Trimmomatic es necesario evaluar la calidad de las secuencias generadas ("limpias") usando nuevamente FastQC.

```
fastqc -t 10 02.CleanData/* -o 01.FastqcReports/
```

Descargue los reportes `.html` de las secuencias pareadas (i.e. `01.FastqcReports/sample1_1_paired_fastqc.html` y `01.FastqcReports/sample1_2_paired_fastqc.html`).

FaÃ§a uma tabela com o nÃºmero de sequÃªncias antes e depois da trimagem para calcular a porcentagem de *reads* que sobreveviveram ao processo.

> ğŸ‡ªğŸ‡¸ Haga una tabla con el nÃºmero de secuencias antes y despuÃ©s de la depuraciÃ³n para calcular el porcentaje de *reads* que sobrevivieron al proceso.


### 1.3 Cobertura dos Metagenoma

ğŸ‡§ğŸ‡· AlÃ©m de limpar e trimar as sequÃªncias com baixa qualidade, Ã© necessÃ¡rio calcular a cobertura dos metagenomas.Este programa usa a redundÃ¢ncia de reads nos metagenomas para estimar a cobertura mÃ©dia e prediz a quantidade de sequÃªncias que sÃ£o requeridas para atingir o *"nearly complete coverage"*, definida como $â‰¥95$%  ou $â‰¥99$% de cobertura mÃ©dia. A ferramenta [**NonPareil v3.3.3**](https://nonpareil.readthedocs.io/en/latest/) serÃ¡ usada nesta etapa.

> ğŸ‡ªğŸ‡¸ AdemÃ¡s de limpiar y *trimar* las secuencias con baja calidad, es necesario calcular la cobertura de los metagenomas. Este programa usa la redundancia de las *reads* en los metagenomas para estimar la cobertura promedio y predice la cantidade de secuencias que son requeridas para conseguir el *"nearly complete coverage"*, definida como $â‰¥95$%  o $â‰¥99$% de la cobertura promedio. La herramienta [**NonPareil v3.3.3**](https://nonpareil.readthedocs.io/en/latest/) serÃ¡ usada en esta etapa.

### 1.3.1. InstalaÃ§Ã£o

ğŸ‡§ğŸ‡· [NonPareil v3.3.3](https://nonpareil.readthedocs.io/en/latest/) Ã© uma ferramenta que serÃ¡ usada para o cÃ¡lculo da cobertura dos metagenomas. Devido a incompatibilidades com a versÃ£o do Python usado para escrever esta ferramenta, ela serÃ¡ instalada em um ambiente diferente ao de controle de qualidade, chamado **NonPareil**.

> ğŸ‡ªğŸ‡¸ [NonPareil](https://nonpareil.readthedocs.io/en/latest/) es una herramienta que serÃ¡ usada para el cÃ¡lculo de la cobertura de los metagenomas. Debido a incompatibilidades con la versiÃ³n de Python usado para escribir esta herramienta, serÃ¡ instalada en un ambiente diferente al de control de calidad, llamado **NonPareil**.

```
# Crie o ambiente
conda create -n NonPareil

# Instale NonPareil
conda install -c bioconda nonpareil
```

### 1.3.2. Uso

Como *input* para esta anÃ¡lise sÃ³ Ã© necessÃ¡rio um pair de cada amostra, e deve estar sem compressÃ£o.


```
# Crie o diretÃ³rio pra o output
mkdir 03.NonPareil

# entre no directorio
cd 03.NonPareil

# Copie os pair 1 da pasta 02.CleanData

cp ../02.CleanData/*_1* ./

# Descomprimir 
gunzip -d *

```

ğŸ‡§ğŸ‡· Agora estÃ¡ tudo pronto para rodar a anÃ¡lise, mas antes disso tome-se o tempo para entender o comando que vai usar. Para conhecer que Ã© cada um dos argumentos, explore o menÃº de ajuda da ferramenta. 

> ğŸ‡ªğŸ‡¸ Ahora estÃ¡ todo listo para correr el anÃ¡lisis, pero antes de eso tÃ³mese el tiempo para entender el comando que va a usar. Para conocer que es cada uno de los argumentos, explore el menÃº de ayuda de la herramienta.

```
# Ative o ambiente NonPareil
conda activate NonPareil

# Explore o menÃº da ferramenta
nonpareil --help

# Comando do NonPareil para cada amostra
nohup nonpareil -s sample1.fq -T kmer -f fastq -b sample1 -t 6 &

```
No caso, se tiver vÃ¡rias amostras pode usar o seguinte loop para facilitar o processo.

```
for i in ./*.fq
do
BASE=$(basename $i .fq)
nonpareil -s $i -T kmer -f fastq -b $i -t 6
done
```

**Sintaxe**

* `-s`: caminho para o *input* 
* `-T`: algorÃ­tmo a ser usado. `kmer` Ã© recomendado para arquivos `.fastq` e `alignment` Ã© recomendado para arquivos `.fasta`.
* `-f`: indique aqui o formato do input (p.e. `fastq` ou `fasta`)
* `-b`: prefixo para os *outputs*
* `-t`: nÃºmero de threads

ğŸ‡§ğŸ‡· Ao terminar esse processo, o programa terÃ¡ criado varios [*outputs*](https://nonpareil.readthedocs.io/en/latest/redundancy.html#output) por cada amostra. Descarregue os arquivos `.npo`. Os quais sÃ£o tabelas delimitadas por tabulaÃ§Ãµes com seis colunas. A primeira coluna indica o esforÃ§o de sequenciamento (em nÃºmero de reads), as demais colunas tÃªm informaÃ§Ã£o sobre a distribuiÃ§Ã£o da redundÃ¢ncia a determinado esforÃ§o de sequenciamento. Usando os arquivos `.npo` e o R, pode grÃ¡ficar as curvas de saturaÃ§Ã£o. A continuaÃ§Ã£o se encontra o script.
AlÃ©m dos arquivos `.npo` Ã© necessÃ¡rio criar um arquivo chamado `samples.txt`, o qual deve ter trÃªs colunas (separadas por tabulaÃ§Ãµes), a primeira terÃ¡ o nome de cada arquivo `.npo`, a segunda o nome da amostra, e a terceira a cor em formato JSON que vai ser usada para a curva. A continuaÃ§Ã£o se encontram uma sÃ©rie de comandos no bash para gerar o arquivo, no entanto este arquivo pode ser construido em um bloco de notas, ou incluso no excel.

> ğŸ‡ªğŸ‡¸ Al terminar este proceso, el programa habrÃ¡ creado varios [*outputs*](https://nonpareil.readthedocs.io/en/latest/redundancy.html#output) por cada muestra. Descargue los archivos `.npo`. Los cuales son tablas delimitadas por tabulaciones con seis columnas. La primera columna indica el esfuerzo de secuenciaciÃ³n (en nÃºmero de *reads*), las demÃ¡s columnas tienen informaciÃ³n sobre la distribuciÃ³n de la redundancia a determinao esfuerzo de secuenciaciÃ³n. Usando los archivos `.npo` e R, puede grÃ¡ficar las curvas de saturaciÃ³n. A continuaciÃ³n se encuentra el script.
>
> AdemÃ¡s de los archivos `.npo` es necesario crear un archivo llamado `samples.txt`, el cual debe tener tres columnas (separadas por tabulaciones), la primera tendrÃ¡ el nombre de cada archivo `.npo`, la segunda el nombre de la muestra, y la tercera el color en formato JSON que va a ser usado para la curva. A continuaciÃ³n se encontran una serie de comandos en bash para generar el archivo, sin embargo, este archivo puede ser construido en un bloc de notas, o incluso en excel.

```
# Cria um arquivo com os nomes dos arquivos
ls *.npo > files.txt

# Cria um arquivo com os nomes das amostras

ls *.npo | sed 's/_1_paired.fq.npo//g' > prefix.txt
```

Agora precisa criar uma lista de cores para diferenciar suas amostras no grÃ¡fico. Use o site [IWantHue](http://medialab.github.io/iwanthue/) para criar uma paleta com o nÃºmero de cores igual ao nÃºmerop de amostras. Copie os cÃ³digos **HEX json** das cores e coloque dentro de um arquivo (elimine as vÃ­rgulas):

> ğŸ‡ªğŸ‡¸ Ahora necesita crear una lista de colores para diferencias sus muestras en el grÃ¡fico. Use el sitio de internet [IWantHue](http://medialab.github.io/iwanthue/) para crear una paleta con el nÃºmero de colores igual al nÃºmero de muestras. Copie los cÃ³digos **HEX json** de los colores e coloque dentro de un archivo (elimine las comas):

```
# Crie o arquivo
nano colors.txt

# Copie e cole os cÃ³digos
"#c151b6"
"#5eb04d"
"#7d65ce"
"#b5b246"
"#688ccd"
"#4bb092"
```
Cree o arquivo final com os tÃ­tulos de las columnas e una los trÃªs arquivos gerados anteriormente:

```
echo -e 'File\tName\tCol' > samples.txt

# Unindo os arquivos dentro de samples.txt
paste -d'\t' files.txt prefix.txt colors.txt >> samples.txt
```

Use `less` para explorar o arquivo, ele deve se ver assim:

```
File    Name    Col
Sample1.npo   Sample1   "#c151b6"
Sample2.npo   Sample2   "#5eb04d"
Sample3.npo   Sample3   "#7d65ce"
Sample4.pno   Sample4   "#b5b246"
Sample5.npo   Sample5   "#688ccd"
Sample6.npo   Sample6   "#4bb092"
```
Descarregue os arquivos `.npo` e o arquivo `samples.txt`. Usando o seguinte script do R, grafique as curvas de saturaÃ§Ã£o. *Nota: todos os arquivos descarregados devem estar dentro de uma pasta sÃ³, p.e. `03.NonPareil`.

```{r, eval=FALSE}
install.packages("Nonpareil") #para instalar o pacote
library(Nonpareil) # ativa o pacote
setwd("~/03.NonPareil") # determina seu diretÃ³rio de trabalho (coloque o seu, onde colocou os arquivos .npo e o arquivo samples.txt)

samples <- read.table('samples.txt', sep='\t', header=TRUE, as.is=TRUE); #lÃª o arquivo samples.txt com a informaÃ§Ã£o das amostras

attach(samples);
nps <- Nonpareil.set(File, col=Col, labels=Name, 
                     plot.opts=list(plot.observed=FALSE, 
                                    ylim = c(0, 1.05),
                                    legend.opts = FALSE)) #grafica as curvas

Nonpeil.legeng(nps, x.intersp=0.5, y.intersp=0.7, pt.cex=0.5, cex=0.5) #coloca e personaliza a legenda
  
detach(samples);
summary(nps) #mostra o resumo em forma de tabela
```

Vai obter um grÃ¡fico com as curvas de saturaÃ§Ã£o de cada amostra, como este:

<img src="imgs/nonpareil.webp" align='center' width="80%">

ğŸ‡§ğŸ‡· As linhas tracejadas <font color='red'> vermelha </font> e <font color='gray'> cinza </font> representam os *threshold* de 95% e 99% da cobertura mÃ©dia, respeitivamente. O circulo em cada curva representa a cobertura atual das amostras, o ideal Ã© que esteja por cima do primeiro *threshold*. As curvas tambÃ©m apresentam a estimaÃ§Ã£o de quanto esforÃ§o de sequenciamento Ã© necessÃ¡rio (zetas no eixo x).

> ğŸ‡ªğŸ‡¸ Las lÃ­neas punteadas <font color='red'> roja </font> y <font color='gray'> gris </font> representam los *threshold* de 95% y 99% de cobertura promedio, respectivamente. El cÃ­rculo en cada curva representa la cobertura actual de las muestras, lo ideal es que estÃ©n por encima del primer *threshold*. Las curvas tambiÃ©n presentan la estimaciÃ³n de cuanto esfuerzo de secuenciaciÃ³n es necesario (flechas en el eje x). 


### 1.4. AnÃ¡lise de DistÃ¢ncias MinHash

ğŸ‡§ğŸ‡· ApÃ³s obter as sequÃªncias limpas, de boa qualidade, e determinar a cobertura dos metagenomas, Ã© possÃ­vel fazer a montagem. No entanto, pode ser incluÃ­do um passo extra antes da montagem e Ã© verificar a similaridade dos datasets para determinar se pode ser usada a abordagem de *co-assembly*, onde sÃ£o misturadas as *reads* de vÃ¡rios metagenomas para gerar os contigs. O programa [**Mash v2.3**](https://mash.readthedocs.io/en/latest/) usa uma tÃ©cnica chamada reduÃ§Ã£o de dimensionalidad *MinHash* que avalia as distÃ¢ncias um a um entre os datasets. 

> ğŸ‡ªğŸ‡¸ DespuÃ©s de obtener las secuencias limpias, de buena calidad, y determinar la cobertura de los metagenomas, es posible hacer el montaje. Sin embargo, puede ser incluÃ­do un paso extra antes del montaje y es verificar la similaridade de los datasets para determinar si puede ser usado el abordaje de *co-assembly*, donde son mezcladas las *reads* de varios metagenomas para generar los contigs. El programa [**Mash v2.3**](https://mash.readthedocs.io/en/latest/) usa una tÃ©cnica llamada reducciÃ³n de dimensionalidad *MinHash* que evalua las distancias un a un entre los datasets.

### 1.4.1. InstalaÃ§Ã£o

ğŸ‡§ğŸ‡· [Mash v2.3](https://mash.readthedocs.io/en/latest/) Ã© uma ferramenta que usa a tÃ©cnica de reduÃ§Ã£o da dimensionalidade *MinHash* para calcular as distÃ¢ncias um a um entre os datasets, assim, Ã© possÃ­vel determinar se os metagenomas sÃ£o similares ou nÃ£o para serem montados usando *co-assembly*. 

ğŸ‡§ğŸ‡· Por ser considerada uma ferramenta que participa no processo de assembly, serÃ¡ instalada dentro de um ambiente virtual chamado **Assembly**.

> ğŸ‡ªğŸ‡¸ [Mash](https://mash.readthedocs.io/en/latest/) es una herramienta que usa la tÃ©cnica de reducciÃ³n de dimensionalidad *MinHash* para calcular las distancias un a un entre los datasets, asÃ­, es posible determinar si los metagenomas son similares o no para ser ensamblados usando *co-assembly*.
>
> ğŸ‡ªğŸ‡¸ Por ser considera una herramienta que participa en el proceso de ensamble, serÃ¡ instalada dentro de un ambiente virtual llamado **Assebly**.

```
# Crie o ambiente virtual
conda create -n Assembly

# Instale Mash
conda install -c bioconda mash
```

### 1.4.2. Uso

```
## Crie uma pasta para o output
mkdir 04.MinHash
```

ğŸ‡§ğŸ‡·  O primeiro paso Ã© concatenar os reads 1 e 2, e armazenar eles na nova pasta criada `04.MinHash/`.

**Nota:** Se vocÃª trimou suas sequÃªncias, deve usar os arquivos gerados pelo **Trimmomatic** na pasta `02.CleanData`, se pelo contrÃ¡rio suas sequÃªncias estavam de boa qualidade e nÃ£o foi necessÃ¡rio trimar, use os arquivos originais, que estÃ£o dentro da pasta `00.RawData/`.

> ğŸ‡ªğŸ‡¸ 
>
>**Nota:** Si usted filtrÃ³ sus secuencias, debe usar los archivos generados por **Trimmomatic** en el directorio `02.CleanData`, si por el contrario sus secuencias estaban de buena calidade y no fue necesario filtrar, use los archivos originales, que estÃ¡n dentro de la carpeta `00.RawData`.

```
for i in 02.CleanData/*_1_paired.fq.gz
do
BASE=$(basename $i _1_paired.fq.gz)
cat $i 02.CleanData/${BASE}_2_paired.fastq.gz > 04.MinHash/${BASE}.fq
done
```

ğŸ‡§ğŸ‡· Depois serÃ¡ criado um *sketch* para combinar todas as amostras. Usando `mash info` pode verificar o conteÃºdo e, em seguida, estimar as distÃ¢ncias par a par:

> ğŸ‡ªğŸ‡¸ 
>
> DespuÃ©s serÃ¡ creado un *sketch* para combinar todas las muestras. Usando `mash info` puede verificar el contenido y, en seguida, estimar las distancias par a par:

```
mash sketch -o 04.MinHash/reference 04.MinHash/sample1.fq 04.MinHash/sample2.fq 04.MinHash/sample3.fq 04.MinHash/sample4.fq 04.MinHash/sample5.fq 04.MinHash/sample6.fq

#verifiyng
mash info 04.MinHash/reference.msh
```

**Sintaxe**

`mash sketch -o reference [inputs]`

`mash info reference.msh`

* `sketch`: Comando para criar um *sketch*, combinando todas as amostras, recomendado quando tÃªm mais de trÃªs amostras.
* `-o`: caminho pro *output*, criarÃ¡ um *sketch* `.msh`.
* `inputs`: liste os inputs (sequencias concatenadas dos pair1 e pair2)
* `info`: pode verificar o conteÃºdo do `sketch`
* `reference.msh`: *sketch* criado

Por Ãºltimo, calcule as distÃ¢ncias entre cada par de metagenomas usando `mash dist` e salve o resultado no arquivo `distancesOutput.tsv`.

```
mash dist 04.MinHash/reference.msh 04.MinHash/reference.msh -p 6 > 04.MinHash/distancesOutputFinal.tsv
```

**Sintaxe**
`mash dist [reference] [query] [options]`

* `dist`: comando para calcular as distÃ¢ncias entre cada par de mategenomas, baseado na distÃ¢ncia *MinHash*.
* `reference`: aqui pode colocar o *sketch* criado, ou arquivos `.fq`, `fasta`.
* `query`: Ã­dem
* `-p`: nÃºmero de threads

Descarregue o output (`04.MinHash/distancesOutputFinal.tsv`) e use o seguinte script do R para plotar um heatmap com as distÃ¢ncias.

```{r, eval=FALSE}
setwd("~/04.MinHash/")

 data <- read.table("distancesOutputFinal.tsv")

 #install.packages("vegan")
 library(vegan)
 set.seed(2)
 
 dst = as.matrix(data)
 
 #install.packages("gplots")
 library(gplots)
 set.seed(2)
 x <- matrix(rnorm(100), nrow = 5)
 dist.fn <- function(x) as.dist(1-cor(t(x)))
 hclust.com <- function(x) hclust(x, method="complete")
 
 dev.off()
 h.ori <- heatmap.2(dst, trace="none", distfun=dist.fn, 
                    hclustfun=hclust.com,dendrogram = "row",main = "MinHash Clusterization",
                    cexRow=0.8, # Tamanho do texto no eixo y
                    cexCol=0.8,adjCol = c(0.5,0.2),
                    adjRow = c(0.05,0.),
                    srtCol=90,offsetRow=0, offsetCol=0, keysize = 1.5)
```

Vai obter um heatmap com clusterizaÃ§Ã£o similar a este:

<img src="imgs/distances.png" align='center' width="80%">

FaÃ§a *co-assembly* para *datasets* com distÃ¢ncias menores de 0.1, entre ellas.


## 2. Montagem dos Metagenomas

ğŸ‡§ğŸ‡· A montagem dos metagenomas Ã© a etapa mais importante do processo, porque os demais passos para adelante dependen de uma boa montagem. No caso dos metagenomas, se trata de um proceso que nÃ£o Ã© para nada trivial, requer um grande esforÃ§o computacional. Por este motivo, serÃ£o testados vÃ¡rios parÃ¢metros, para comparar cada montagem e decidir qual Ã© o melhor para Ã¡s anÃ¡lises *downstream*. Neste processo serÃ¡ usado o montador [Spades v3.15.3](https://github.com/ablab/spades).

> ğŸ‡ªğŸ‡¸ El montaje de los metagenomas es la etapa mÃ¡s importante del proceso, porque los demÃ¡s pasos para adelante dependen de un buen ensamble. En el caso de los metagenomas, se trata de un proceso que no es para nada trivial, requiere un gran esfuerzo computacional. Por este motivo serÃ¡n testados varios parÃ¡metros, para comparar cada ensamble y decidir cual es el mejor para los anÃ¡lisis *downstream*. En este proceso serÃ¡ usado el montado [Spades v3.15.3](https://github.com/ablab/spades).

### 2.1. InstalaÃ§Ã£o

ğŸ‡§ğŸ‡· [Spades v3.15.3](https://github.com/ablab/spades) Ã© um dos montadores de genomas e metagenomas, mais conhecido e com melhores resultados, pode ser usado tanto para leituras curtas como longas. Leia atentamente o [manual](http://cab.spbu.ru/files/release3.15.2/manual.html), jÃ¡ que este programa tem muitas opÃ§Ãµes diferentes. Spades usa o algorÃ­tmo do *Grafo de Bruijn* para a montagem das secuÃªncias.

Siga as seguintes instruÃ§Ãµes para a instalaÃ§Ã£o do **Spades** dentro do ambiente virtual *Assembly*.

> ğŸ‡ªğŸ‡¸ [Spades v3.15.3](https://github.com/ablab/spades) es uno de los ensambladores de genomas y metagenomas, mÃ¡s conocido y con mejores resultados, puede ser usado tanto para lecturas cortas como largas. Lea atentamente el [manual](http://cab.spbu.ru/files/release3.15.2/manual.html), ya que este programa tiene muchas opciones diferentes. Spades usa el algorÃ­tmo del *Grafo de Bruijn* para el montaje de las secuencias. 
>
> Siga las siguientes instrucciones para la instalaciÃ³n de **Spades** dentro del ambiente virtual *Assembly*.

```
# Active el ambiente virtual
conda activate Assembly

# Instale Spades
conda install -c bioconda spades
```

### 2.2. Uso

ğŸ‡§ğŸ‡· Agora Ã© momento de fazer as montagens. Use o resultado da anÃ¡lisis de distÃ¢ncias *MinHash* para decidir como serÃ£o feitos as montagens. Amostras muito prÃ³ximas pode fazer *co-assembly*, para amostras distantes Ã© recomendado montar individualmente. Opcionalmente podem ser usadas as sequÃªncias no pareadas (sequÃªncias "descartadas" pelo Trimmomatic). O montador usado neste mÃ©todo serÃ¡ [Spades](https://github.com/ablab/spades). 

A continuaÃ§Ã£o se encontram os comandos se sua montagem for individual:

> ğŸ‡ªğŸ‡¸ Ahora es el momento de hacer los ensamblajes. Use el resultado del anÃ¡lisis de distancias *MinHash* para decidir como serÃ¡n hechos los montajes. Muestras muy prÃ³xima puede hacer *co-assembly*, para muestras distantes es recomendado montar individualmente. Opcionalmente pueden ser las secuencias no pareadas (secuencias "descartadas" por Trimmomatic). El montador usado en este mÃ©todo serÃ¡ [Spades](https://github.com/ablab/spades). 

1. Criar um diretÃ³rio para todas as montagens

```
mkdir 05.Assembly
```

2. Se vocÃª quiser usar as *reads* no pareadas (saÃ­da do **Trimmomatic**), deve primeiro concatenarlas em um arquivo sÃ³

```
cat unpaired/sample1_1_unpaired.fq.gz unpaired/sample1_2_unpaired.fq.gz > unpaired/sample1_12_unpaired.fq.gz
```

3. Montagem com MetaSpades

```
metaspades.py -o 05.Assembly/sample1/ -1 02.CleanData/sample1_1_paired.fq.gz -2 02.CleanData/sample1_2_paired.fq.gz -s unpaired/sample1_12_unpaired.fq.gz -t 6 -m 100 -k 21,29,39,59,79,99,119
```

**Sintaxe**

* `metaspades.py`: script para montar metagenomas
* `-o`: caminho para diretÃ³rio de saÃ­da
* `-1`: caminho para diretÃ³rio do pair1
* `-2`: caminho para diretÃ³rio do pair2
* `-s`: caminho para diretÃ³rio das *reads* no pareadas
* `-t`: nÃºmero de threads
* `-m`: MemÃ³ria em gigas (mÃ¡ximo)
* `-k`: lista de *k-mers*

ğŸ‡§ğŸ‡· Se sua montagem for no modo *co-assembly* deve fazer uma etapa anterior, onde vai concatenar todos os pair1 das amostras que serÃ£o montadas e todos os pair2 das mesmas. 

> ğŸ‡ªğŸ‡¸  Si su ensamblaje es en el modo *co-assembly* debe hacer una etapa anterior, donde va a concatenar todos los pair1 de las muestras que serÃ¡n montadas y todos los pair2 de las mismas. 

1. Concatene os pair 1

```
cat 02.CleanData/sample4_1.fq.gz 02.CleanData/sample5_1.fq.gz > 02.CleanData/sample45_1.fq.gz
```

2. Concatene os pair 2

```
cat 02.CleanData/sample4_2.fq.gz 02.CleanData/sample5_2.fq.gz > 02.CleanData/sample45_2.fq.gz
```

3. Se vocÃª quiser usar as *reads* no pareadas (saÃ­da do **Trimmomatic**), deve primeiro concatenarlas em um arquivo sÃ³

```
cat unpaired/sample4_1_unpaired.fq.gz unpaired/sample4_2_unpaired.fq.gz unpaired/sample5_1_unpaired.fq.gz unpaired/sample5_2_unpaired.fq.gz > unpaired/sample45_12_unpaired.fq.gz
```

4. Montagem com MetaSpades

```
metaspades.py -o 05.Assembly/sample45/ -1 02.CleanData/sample45_1_paired.fq.gz -2 02.CleanData/sample45_2_paired.fq.gz -s unpaired/sample45_12_unpaired.fq.gz -t 6 -m 100 -k 21,29,39,59,79,99,119
```

**Outputs**

Para conhecer os demais parÃ¢metros do comando que nÃ£o foram modificados (usados por *default*), consulte o [manual](http://cab.spbu.ru/files/release3.15.2/manual.html).

* `corrected/`: contÃ©m as reads corregidas por **BayesHammer** em `.fastq.gz`

* `scaffolds.fasta`: contÃ©m os scaffolds obtidos

* `contigs.fasta`: contÃ©m os contigis obtidos

* `assembly_graph_with_scaffolds.gfa`: contÃ©m o grafo da montagem en formato GFA 1.0.

* `assembly_graph.fastg`: contÃ©m o grafo da montagem em formato FASTG


## 3. Controle de Qualidade das montagens

ğŸ‡§ğŸ‡· Para avaliar a qualidade das montagens serÃ¡ usada a ferramenta [**Quast v5.0.2**](http://quast.sourceforge.net/docs/manual.html) (*QUality ASsesment Tool*), especificamente o *script* `metaquast.py`, com o qual Ã© possÃ­vel determinar as principais estatÃ­sticas da montagem (i.e. N50, nÃºmero de contigs, tamanho total da montagem, tamanho dos contigs, etc). **Metaquast** gera uma sÃ©rie de arquivos e reportes onde Ã© possÃ­vel observar essas estatÃ­sticas bÃ¡sicas da montagem. Ã‰ uma ferramente muito Ãºtil para comparar montagens e escolher a melhor pro mesmo conjunto de dados. 

> ğŸ‡ªğŸ‡¸ Para evaluar la calidad de los montajes serÃ¡ usada la herramienta [**Quast v5.0.2**](http://quast.sourceforge.net/docs/manual.html) (*QUality ASsesment Tool*), especificamente el *script* `metaquast.py`, con el cual es posible determinar las principales estadÃ­sticas del montaje (i.e. N50, nÃºmero de contigs, tamaÃ±o total del montaje, tamaÃ±o de los contigs, etc). **Metaquast** genera una serie de archivos y reportes donde es posible observar esas estadÃ­sticas bÃ¡sicas del montaje. Es una herramienta muy Ãºtil para comparar monatajes y escoger el mejor del mismo conjunto de datos. 

### 3.1. InstalaÃ§Ã£o

Crie um novo ambiente virtual, chamado bioinfo, onde se instalarÃ¡ **Quast**.

```
# Crie o ambiente
conda create -n bioinfo

# Ative o ambiente bioinfo
conda activate bioinfo

# Instale Quast
conda install -c bioconda quast
```

### 3.2. Uso

ğŸ‡§ğŸ‡· Antes de rodar `metaquast.py`, Ã© necessÃ¡rio trocar os nomes dos assemblies, jÃ¡ que eles tem todos o mesmo nome, `contigs.fasta` ou `scaffolds.fasta`. Use o comando `mv` para trocar os nomes. Siga o seguinte exemplo:

> ğŸ‡ªğŸ‡¸ Antes de correr `metaquast.py` es necesario cambiar los nombres de los montajes, ya que todos tienen el mismo nombre, `contigs.fasta` ou `scaffolds.fasta`. Use el comando `mv` para cambiar los nombres. Siga el siguiente ejemplo:

```
mv 05.Assembly/sample1/scaffolds.fasta 05.Assembly/sample1/sample1.fasta

mv 05.Assembly/sample45/scaffolds.fasta 05.Assembly/sample45/sample45.fasta
```

```
# Crie um diretÃ³rio pro output
mkdir 06.AssemblyQuality

# Rode Quast
metaquast.py 05.Assembly/sample1/sample1.fasta 05.Assembly/sample45/sample45.fasta -o 06.AssemblyQuality/ --threads 6
```

**Sintaxis**
`metaquast.py path/to/assembly/contigs.fasta -o path/to/output/`

**InterpretaÃ§Ã£o dos resultados**

ğŸ‡§ğŸ‡· A ideia de usar **Metaquast**, a parte de avaliar as estatÃ­sticas bÃ¡sicas das montagens, Ã© comparar varias montagens para escolher a melhor. Por exemplo: entre menor seja o nÃºmero de contigs Ã© melhor, porque significa que a montagem estÃ¡ menos fragmentada. E isso serÃ¡ refletido no tamanho dos contigs que serÃ£o maiores. O valor de N50, Ã© melhor entre maior seja. AlÃ©m, tambÃ©m Ã© ideal um menor nÃºmero de gaps e Ns. No entanto, estas estatÃ­sticas funcionam melhor para genomas que para metagenomas, por se tratar de um conjunto de microrganismos.

> ğŸ‡ªğŸ‡¸ La idea de usar **Metaquast**, aparte de evaluar las estidÃ­sticas bÃ¡sicas de los montajes, es comparar varios montajes para escoger el mejor. Por ejemplo: entre menor sea el nÃºmero de contigs es mejor, porque significa que el montaje estÃ¡ menos fragementado. Y eso se reflejarÃ¡ en el tamaÃ±o de los contigs que serÃ¡n mÃ¡s grandes. El valor de N50, es mejor entre mayor sea. AsÃ­ mismo, es ideal menor nÃºmero de gaps y Ns. Sin embargo, Ã©stas estadÃ­sticas funcionan mejor para genomas que para metagenomas, por tratarse de un grupo de microorganismos. 

**Outputs**

Explore o diretÃ³rio do output usando o comando `ls`.

* `06.AssemblyQuality/report.html`: Este relatÃ³rio pode ser aberto em um *web browser* e contem as informaÃ§Ãµes mais relevantes. Como nÃºmero de contigs, tamanho del maior contig, tamanho total da montagem, N50, etc.

> ğŸ‡ªğŸ‡¸ `06.AssemblyQuality/report.html`: reporte puede ser abierto en un *web browser* y contiene las informaciones mÃ¡s relevantes. Como nÃºmero de contigs, tamaÃ±o del mayor contig, tamaÃ±o total del montaje, N50, etc.

<img src="imgs/report_quast1.png" align="center" width = "100%"/>


* `06.AssemblyQuality/report.tex`, `06.AssemblyQuality/report.txt`, `06.AssemblyQuality/report.tsv`, `06.AssemblyQuality/report.pdf`: Ã© o mesmo relatÃ³rio porÃ©m em diferentes formatos. 

* `06.AssemblyQuality/transposed_report.tsv`, `06.AssemblyQuality/transposed_report.tex`, `06.AssemblyQuality/transposed_report.tex`: TambÃ©m Ã© o relatÃ³rio porÃ©m em formato tabular. 

* `06.AssemblyQuality/icarus_viewers/contig_size_viewer.html`: Visualizador das contigs

* `06.AssemblyQuality/basis_stats/`: Dentro desta pasta se encontram vÃ¡rios grÃ¡ficos em formato `.pdf`. 

## 4. PrediÃ§Ã£o das ORFs (*Open Reading Frame*)

ğŸ‡§ğŸ‡· O objetivo desta etapa Ã© procurar os marcos abertos de leitura ou ORFs (em inglÃªs) dentro dos contig/scaffols. Ou seja, predizer onde iniciam e terminam os genes. Basicamente o programa procura por codons de inicio, principalmente **ATG**, porÃ©m, tambÃ©m sÃ£o cÃ³dons de iniciaÃ§Ã£o **GTG** e **TTG**. Depois, procura os cÃ³dons de parada, como **TAA**, **TAG** e **TGA**. 

O programa a usar para a prediÃ§Ã£o das ORFs em procariotos Ã© [Prodigal v2.6.3 (*Prokaryotic Dynamic Programming Genefinding Algorithm*)](https://github.com/hyattpd/prodigal/wiki).

> ğŸ‡ªğŸ‡¸ El objetivo de esta etapa es buscar los marcos abiertos de lectura o ORF (en inglÃ©s) dentro de los contigs/scaffolds. O sea, predecir donde incian y terminan los genes. Basicamente el programa busca por cÃ³dones de inicio, principalmente **ATG**, sin embargo tambiÃ©n son cÃ³dones de inico **GTG** e **TTG**. DespuÃ©s, busca los cÃ³dones de parada, como **TAA**, **TAG** y **TGA**.
>
> El programa a usar para la predicciÃ³n de ORFs en procariotos es [Prodigal v2.6.3 (*Prokaryotic Dynamic Programming Genefinding Algorithm*)](https://github.com/hyattpd/prodigal/wiki).


### 4.1. InstalaÃ§Ã£o

Crie um novo ambiente para instalaÃ§Ã£o das ferramentas relacionadas com a anotaÃ§Ã£o de genes, chamada `Annotation`. 

```
# Crie o ambiente
conda create -n Annotation

# Ative o ambiente
conda activate Annotation

# Instale Prodigal
conda install -c bioconda prodigal
```

### 4.2. Uso

Para facilitar o processo, passe todos os scaffolds de cada montagem para uma pasta sÃ³. Siga o exemplo:

```
mkdir 05.Assembly/scaffolds

mv 05.Assembly/sample1/sample1.fasta 05.Assembly/scaffolds/

mv 05.Assembly/sample45/sample45.fasta 05.Assembly/scaffolds

# Confira
ll 05.Assembly/scaffolds
```

Crie uma pasta chamada `07.GenePrediction` para colocar a saÃ­da do **Prodigal**.

`mkdir 07.GenePrediction`

A continuaÃ§Ã£o encontrarÃ¡ o comando **individual** 

```
prodigal -i 05.Assembly/scaffolds/sample1.fasta -f gff -o 07.GenePrediction/sample1.gff -a 07.GenePrediction/sample1.faa -d 07.GenePrediction/sample1.fa -p meta
```
Se tiver vÃ¡rias amostras, pode usar o seguinte loop para automatizar o processo com todas as amostras:

```
for i in 05.Assembly/scaffolds/*.fasta
do
BASE=$(basename $i .fasta)
prodigal -i $i -f gff -o 07.GenePrediction/${BASE}.gff -a 07.GenePrediction/${BASE}.faa -d 07.GenePrediction/${BASE}.fa -p meta
done
```
**Sintaxe**
```
prodigal -i assembly.fasta -f <gbk, gff, sqn, sco> -o coord -a proteins.faa -d nucleotides.fa
```

* `-i`: caminho para a montagem em formato `.fasta`, `.fa` ou `.fna`
* `-f`: formato de saÃ­da pro arquivo de coordenadas, default `.gbk` (*Genbank-like format*), `.gff` (*Gene Feature Format*), `.sqn` (*Sequin feature table format*) ou `.sco` (*Simple coordinate input*)
* `-o`: arquivo output com as coordenadas das ORFs
* `-a`: sequÃªncias das ORFs em proteÃ­na
* `-d`: sequÃªncias das ORFs em nucleotÃ­deos


**Formato `.gff` (Gene Feature Format)**

ğŸ‡§ğŸ‡· Este formato guarda as informaÃ§Ãµes dos genes preditos pelo Prodigal. Explore-o (`less sample1.gff`).

Cada sequÃªncia comenÃ§a com um *header* com as infromaÃ§Ãµes da sequÃªncia analizada, seguido de uma tabela separada por tabulaÃ§Ãµes com informaÃ§Ãµes dos genes encontrados em dita sequÃªncia.

O *header* contÃ©m os seguentes campos:

> ğŸ‡ªğŸ‡¸ Este formato guarda las informaciones de los genes predichos por Prodigal. Explorelo (`less sample1.gff`).
> 
> Cada secuencia comienza con un *header* con las informaciones de la secuencia analizada, seguido de una tabla separada por tabulaciones con informaciones de los genes encontrados en dicha secuencia.
> 
> El *header* contiene los siguientes campos:

* **seqnum**: O nÃºmero da sequÃªncia, comeÃ§ando pelo nÃºmero 1.
* **seqlen**: tamanho em bases da sequÃªncia
* **seqhdr**: tÃ­tulo completo da sequÃªncia extraÃ­do do arquivo `.fasta`.
* **version**: versÃ£o do Prodigal usado
* **run_type**: modo de corrida, p.e. m*metagenomic*
* **model**: informaÃ§Ã£o sob o arquivo de treinamento usado para a prediÃ§Ã£o.
* **gc_cont**: % de GC na sequÃªncia
* **transl_table**: Tabela do cÃ³digo genÃ©tico usada para analizar a sequÃªncia. Para bactÃ©rias e archaeas Ã© usada a [tabela 11](https://www.ncbi.nlm.nih.gov/Taxonomy/Utils/wprintgc.cgi#SG11).
* **uses_sd**: 1 se o Prodigal usa o *[RBS](https://parts.igem.org/Ribosome_Binding_Sites) finder*, ou 0 se usa outros *motifs*. 

DespuÃ©s do *header* se encuentra una tabla con las informaciones de los genes encontrados:

* **seqname**: nome da sequÃªncia, neste caso nome do scaffold/contig.
* **source**: nome do programa que gerou a prediÃ§Ã£o
* **feature**: tipo de *feature*, p.e. CDS (*Coding DNA Sequence*)
* **start**: primeira posiÃ§Ã£o da *feature*
* **end**: Ãºtlima posiÃ§Ã£o da *feature*
* **score**: Valor numerico que geralmente indica a confianÃ§a do programa na prediÃ§Ã£o da ORF.
* **strand**: fita do DNA que foi encontrado a *feature*. A fita *forward* Ã© definida como '+', e a *reverse* como '-'.
* **frame**: 0 indica que a primeira base da *feature* Ã© a primeira base do cÃ³don de inicio, 1, que a segunda base da *feature* Ã© a primeira base do cÃ³don de inicio.
* **atribute**: informaciÃ³n adicional sobre la *feature*, parada por ponto e vÃ¬rgula ";".  

  * **ID**: identificador Ãºnico de cada gene, consistindo em um nÃºmero ordinal ID da sequÃªncia e um nÃºmero ordinal ID do nÃºmero do gene separados por "_". Por exemplo "1_688" siginifa que Ã© o gene nÃºmero 688 da sequÃªncia 1.
  * **partial**: indica se o gene estÃ¡ completo ou nÃ£o. "0" indica que no gene foi encontrado o cÃ³don de inicio ou de parada, "01" indica que no gene sÃ³ foi encontrado o cÃ³ndon de inicio, "11" indica que nÃ£o foram encontrados nenhum dos dois cÃ³dons e "00" indica que foram encontrados ambos cÃ³dons.
  * **start_type**: sequÃªncia do cÃ³don de inicio.
  * **stop_type**: sequÃªncias do cÃ³don de parada
  * **rbs_motif**: *RBS motif* encontrado pelo Prodigal
  * **rbs_spacer**: nÃºmero de bases entre o cÃ³don de inicio e o *motif* observado.
  * **gc_cont**: ConetÃºdo de GC no gene
  * **conf**: nota de confianÃ§a pra o gene, representa a probabilidade que esse gene seja real. 
  * **score**: *score* total pro gene
  * **cscore**: fraÃ§Ã£o hexamero do *score*, o quanto este gene se parece com uma proteÃ­na verdadeira.
  * **sscore**: *score* para o sitio de inicio da traduÃ§Ã£o do gene. Ã© a soma dos trÃªs seguintes *scores*.
  * **rscore**: *score* pro *RBS motif*
  * **uscores**: *score* pra sequÃªncia em torno do cÃ³don de inÃ­cio.
  * **tscore**: *score* para o tipo de cÃ³don de inicio
  * **mscore**: *score* pros sinais restantes (tipo de cÃ³don de parada e informaÃ§Ãµes da fita principal / reversa).
  
  
## 5. AnotaÃ§Ã£o de genes

ğŸ‡§ğŸ‡· A anotaÃ§Ã£o dos genes Ã© feita alinhando as ORFs preditas contra bases de dados. No caso da anotaÃ§Ã£o funcional, serÃ¡ usado o alinhador [**Diamond**](https://github.com/bbuchfink/diamond) e as bases de dados serÃ£o [**EggNOG**](http://eggnog5.embl.de/#/app/home) e [**KEGG**](https://www.kegg.jp/kegg/). No caso da anotaÃ§Ã£o taxonÃ´mica, podem ser usados dois programas, o [**Kaiju**](https://github.com/bioinformatics-centre/kaiju) ou o [**Kraken2**](https://github.com/DerrickWood/kraken2/wiki).


> ğŸ‡ªğŸ‡¸La anotaciÃ³n de los genes es realizada alineando las ORFs predichas contra bases de dados. En el caso de la anotaciÃ³n funcional serÃ¡ usado el programa para alineamiento [**Diamond**](https://github.com/bbuchfink/diamond) y las bases de datos [**EggNOG**](http://eggnog5.embl.de/#/app/home) y [**KEGG**](https://www.kegg.jp/kegg/). Ya en el caso de la anotaciÃ³n taxonÃ³mica, pueden ser usados dos programas, [**Kaiju**](https://github.com/bioinformatics-centre/kaiju) o [**Kraken2**](https://github.com/DerrickWood/kraken2/wiki).


### 5.1. InstalaÃ§Ã£o

#### 5.1.1 ObtenÃ§Ã£o das Bases de Dados

ğŸ‡§ğŸ‡·Para a obtenÃ§Ã£o das bases de dados, pode ir nos sites e descarregar diretamente. No entanto, tenha em conta que a base de dados **KEGG** Ã© paga. Se vocÃª descarregar direto da fonte, deverÃ¡ formatar as DBs para o seu uso com Diamond (anotaÃ§Ã£o funcional). Isto Ã© feito com o comando `makedb --in reference.fasta -d reference`.

Para facilitar, no seguinte link, vocÃª encontrarÃ¡ as bases de dados **KEGG**, **EggNOG**, previamente formatadas para o uso em Diamond e **Kraken2**.   

Use o programa `gdown` para descarregar as dbs que se encontram em um GoogleDrive. Se nÃ£o tiver o `gdown` instalado, siga o seguintes passos: 

> ğŸ‡ªğŸ‡¸ Para la obtenciÃ³n de las bases de datos, puede ir directamente en las pÃ¡ginas web de cada una. Sin embargo, tenga en cuenta que la base de datos **KEGG** es paga. Si ud decide descargar directamente de la fuente, deberÃ¡ hacer una formataciÃ³n de las DBs para el uso con Diamond (anotaciÃ³n funcional). Este processo es realizado usando el comando `makedb --in reference.fasta -d reference`. 
>
> Para facilitar, en el siguiente link, encontrarÃ¡ las bases de datos**KEGG**, **EggNOG**, previamente formatadas para su uso en Diamond e **Kraken2**. 


* [**Dbs**](https://drive.google.com/drive/folders/1GLP6vA4Gs0cce-nnBXCmZSgmONWybOSF?usp=sharing)


```
## Se nÃ£o tiver instalado pip
sudo apt update
sudo apt install python3-pip
pip3 --verision

## Instale gdown
pip install gdown
```

ğŸ‡§ğŸ‡· Crie uma pasta, chamada `dbs/`, e use o programa `gdown` para descarregar as dbs. 

```
# Crie o diretÃ³rio
mkdir dbs/

# Descarregue as DBs
gdown https://drive.google.com/drive/folders/1GLP6vA4Gs0cce-nnBXCmZSgmONWybOSF?usp=sharing
```

SerÃ£o descarregados os seguintes arquivos:

* `eggnog.dmnd`: Base de dados EggNOG formatada para Diammond
* `kegg.dmnd`: Base de dados KEGG formatada para Diammond

ğŸ‡§ğŸ‡· **Nota** Ã‰ recomendÃ¡vel procurar os links originais para descarga das bases de dados para assim obter a versÃ£o mais atualizada (p.e. [Kraken2](https://ccb.jhu.edu/software/kraken2/index.shtml?t=downloads))

> ğŸ‡ªğŸ‡¸ **Nota** es recomendable buscar los links originales para descargar las bases de datos en sus versiones mÃ¡s actualizadas (p.e. [Kraken2](https://ccb.jhu.edu/software/kraken2/index.shtml?t=downloads))


```
## Kraken2
mkdir Kraken2
cd Kraken2

## Descarregando desde o servidor dos desenvolvedores
wget  ftp://ftp.ccb.jhu.edu/pub/data/kraken2_dbs/old/minikraken2_v1_8GB_201904.tgz

tar zxvf minikraken2_v1_8GB_201904.tgz

## Troque o nome da pasta de saÃ­da
mv minikraken2_v1_8GB/ mainDB

## Elimine o arquivo original
rm minikraken2_v1_8GB_201904.tgz
```

#### 5.1.2 InstalaÃ§Ã£o Diammond

O [**Diamond**](https://github.com/bbuchfink/diamond) serÃ¡ usado para a anotaÃ§Ã£o funcional. Instale atravÃ©s do conda, no ambiente `Annotation`

```
# Active o ambiente
conda activate Annotation

# InstalaÃ§ao
conda install -c bioconda diammond=2.0.9
```

#### 5.1.3. InstalaÃ§Ã£o Kraken2

[**Kraken2**](https://github.com/DerrickWood/kraken2/wiki) serÃ¡ usado para a anotaÃ§Ã£o taxonÃ´mica. Instale o programa no ambiente `Annotation`.

```
# Se nÃ£o estiver ativado
conda activate Annotation

# Instale 
conda install -c bioconda kraken2
```

ApÃ³s instalado, deve configurar a base de dados, isto Ã© indicar pro programa o caminho (*PATH*) onde se encontram a base de dados. DÃ­gite o seguinte comando: (*coloque o caminho que corresponda a onde vocÃª descarregou suas bases de dados*)

> ğŸ‡ªğŸ‡¸ DespuÃ©s de instalado, debe ser configurada la base de datos, esto es, indicar para el programa el camino (*PATH*) donde se encuentra la base de datos. DÃ­gite el siguiente comando: (*coloque el camino que corresponda a donde ud descargÃ³ sus bases de datos)

```
export KRAKEN2_DB_PATH="/home/metagenomica/dbs/Kraken2/"
```

### 5.2. AnotaÃ§Ã£o Funcional

ğŸ‡§ğŸ‡· Uma vez instaladas todas as ferramentas e descarregadas as bases de dados, pode proceder Ã  anotaÃ§Ã£o. Neste caso serÃ¡ feita primeiro Ã  funcional, usando Diammond e as bases de dados **KEGG** e **EggNOG**.
A continuaÃ§Ã£o se encontra o comando ndividual (*uma montagem e uma base de dados por vez*)

> ğŸ‡ªğŸ‡¸ Una vez instaladas todas las herramientas y descargadas las bases de datos, puede proceder a la anotaciÃ³n. En este caso serÃ¡ hecha primero la anotaciÃ³n funcional, usando Diammond e las bases de datos **KEGG** e **EggNOG**

```
## Crie uma pasta pra saÃ­da
mkdir 08.FunctionalAnnotation

## Diammond
diamond blastx --more-sensitive --threads 6 -k 1 -f 6 qseqid qlen sseqid sallseqid slen qstart qend sstart send evalue bitscore score length pident qcovhsp --id 60 --query-cover 60 -d dbs/kegg.dmnd --query 07.GenePrediction/sample1.fa -o 08.FunctionalAnnotation/sample1_kegg.txt --tmpdir /dev/shm
```

**SINTAXE**

```
diamond blastx --more-sensitive --threads -k -f --id --query-cover -d dbs/db.dmnd --query orfs_nucleotides.fa -o annotation.txt --tmpdir /dev/shm
```

* `blastx`: Alinha sequÃªncias de DNA contra uma base de dados de proteÃ­nas
* `--more-sensitive`: este modo permite hits com >40% de identidade. Existem outros modos `--fast --min-sensitive --very-sensitive --ultra-sensitive`. Clique [aqui](https://github.com/bbuchfink/diamond/wiki/3.-Command-line-options) para mais detalhes
* `--threads`: nÃºmero de nÃºcleos
* `-k/--max-target-seqs`: NÃºmero mÃ¡ximo de sequÃªncias *target* por *query* para reportar alinheamentos.
* `-f/--outfmt`: Formato de saÃ­da. SÃ£o aceptos os seguintes valores:
  * `0` Formato BLAST *pairwise*
  * `5` fomato BLAST XML
  * `6` Formato do BLAST tabular (default), pode customizar as colunas com uma lista separada por espaÃ§os, das seguintes opÃ§Ãµes:
    * `qseqid` id da sequÃªncia *query*
    * `qlen` tamanho da sequÃªncia *query* 
    * `sseqid` id da sequÃªncia da base de dados
    * `sallseqid` todas os id das sequÃªncias das bases de dados
    * `slen` tamanho da sequÃªncia da base de dados
    * `qstart` inicio do alinhamento no *query*
    * `qend` fim do alinhamento no *query*
    * `sstart` inicio do alinhamento na sequÃªncia da base de dados
    * `send` fim do alinhamento na sequÃªncia da base de dados
    * `evalue` 
    * `bitscore` 
    * `score` 
    * `length` tamanho do alinhamento
    * `pident` porcentagem de matches identicos
    
Com o comando anterior foi feita a anotaÃ§Ã£o da montagem da amostra `sample1` com a base de dados `kegg.dmnd` e os dados foram guardados no arquivo `kegg_annotation.txt`.

> ğŸ‡ªğŸ‡¸ Con el comando anterior fue realizada la anotaciÃ³n de la muestra `sample1` con la base de datos `kegg.dmnd` y los datos fueron guardadas en el archivo `kegg_annotation.txt`. 


Se vocÃª quiser rodar todas suas montagens e as duas bases de dados ao mesmo tempo, pode usar o seguinte loop `for`:

```
for i in 07.GenePrediction/*.fa
do
BASE=$(basename $i .fa)
  for j in dbs/*dmnd
  do
  db=$(basename $j .dmnd)
diamond blastx --more-sensitive --threads 6 -k 1 -f 6 qseqid qlen sseqid sallseqid slen qstart qend sstart send evalue bitscore score length pident qcovhsp --id 60 --query-cover 60 -d $j --query $i -o 08.FunctionalAnnotation/${BASE}_${db}.txt --tmpdir /dev/shm
```

Com o comando anterior, Ã© feita a anotaÃ§Ã£o em todas as ORFs preditas na pasta `07.GenePrediction/` com todas as bases de dados para diammond dentro da pasta `dbs/`. Veja que no loop foram declaradas duas variavÃ©is, `i` que corresponde a cada um dos arquivos das ORFs (nucleotÃ­deos) preditas com Prodigal e a variÃ¡vel `j` que corresponde a cada um dos arquivos terminados em `.dmnd` dentro da pasta `dbs/`, ou seja as bases de dados `kegg.dmnd` e `eggnog.dmnd`. Os arquivos de saÃ­da sÃ£o duas tabelas por cada montagem, uma da anotaÃ§Ã£o com *eggnog* e outra com *kegg*. 

> ğŸ‡ªğŸ‡¸ Con el comado anterior, es realizada la anotaciÃ³n de todas las ORF predichas en el directorio `07.GenePrediction/` con todas las bases de datos para Diammond dentro de la carpeta `dbs/`. Vea que en el loop fueron declaradas dos variables, `i` que corresponde a cada uno de los archivos de las ORFs (nucleÃ³tidos) predichos con Prodigal e la variable `j` que corresponde a cada uno de los archivos terminados en `.dmnd` dentro de la carpeta `dbs/`, o sea las bases de datos `kegg.dmnd` y `eggnog.dmnd`. Los archivos de salida son dos tablas por cada ensamble, una con la anotaciÃ³n con *eggnog* e otra con *kegg*. 


### 5.3 AnotaÃ§Ã£o TaxonÃ´mica

Para a anotaÃ§Ã£o taxonÃ´mica serÃ¡ usada a ferramenta Kraken2. Depois de instalada a ferramenta, descarregada e configurada a base de dados, Ã© possÃ¬vel rodar o comando para anotaÃ§Ã£o. Lembrando que este procedimento deve ser feito para cada uma das prediÃ§Ãµes de ORFs de cada montagem.

> ğŸ‡ªğŸ‡¸ Para la anotaciÃ³n taxonÃ³mica serÃ¡ usado la herramienta Kraken2. Despues de instalada la herramienta, descargada y configurada la base de dados, es posible correr el comando para anotaciÃ³n. Recordando que este procedimiento debe ser hecho apra cada una de las predicciones de ORFs de cada ensamble.

```
kraken2 --db mainDB 07.GenePrediction/sample1.fa 
```
**SINTAXE**

`kraken2 --db db orfs_nucleotides.fa`

* `--db`: nome da pasta onde se encontra a base de dados e que foi configurada no PATH.
* `orfs_nucleotides.fa`: Arquivo de saÃ­da da prediÃ§Ã£o de ORFs, en formato `.fa` (nucleotÃ­deos)

Para rodar num comando sÃ³ todas as montagens, pode ser usado o seguinte loop: 

> ğŸ‡ªğŸ‡¸ Para correr en un solo comando todas los ensambles, puede ser usado el siguiente loop:

```
for i in 07.GenePrediction/*.fa
do
BASE=$(basename $i .fa)
kraken2 --db mainDB $i
done
```

O arquivo de saÃ­da Ã© uma tabela `.tsv` por cada montagem. As colunas estÃ£o organizadas da seguinte forma:

1. "C"/"U": Para indicar se a sequÃªncia foi classificada ou nÃ£o classificada (*Unclassified*).
2. Nome do contig
3. IdentificaÃ§Ã£o TaxonÃ´mica
4. Tamanho da sequÃªncia em bp.
5. Mapeamento LCA de cada *k*-mer.


## 6. ConstruÃ§Ã£o Tabela Final

Finalmente Ã© necessÃ¡rio construir uma tabela final com todas as anotaÃ§Ãµes (taxonÃ´mica e funcional) de todas as montagens. 

1. **Formatando as tabelas de anotaÃ§Ã£o funcional**: Usando linha de comando, serÃ£o escolhidas as colunas mais importantes.

```
for i in 08.FunctionalAnnotation/*.txt
do
BASE=$(basename $i .txt)
cut -f1,3,15 $i > 08.FunctionalAnnotation/${BASE}_formated.txt
done
```

2. **Adicionando uma coluna com o nome da montagem**

```
cd 08.FunctionalAnnotation/

for i in *; do nawk '{print FILENAME"\t"$0}' $i > $i.bk; mv $i.bk $i; done
```

3. **Formatando as tabelas da anotaÃ§Ã£o taxonÃ´mica**

```
cd ../09.TaxonomicAnnotation/

for i in *.tsv
do
BASE=$(basename $i .tsv)
cut -f2,3 $i > ${BASE}_formated.tsv
done
```



















