---
output:
    github_document:
    pandoc_args: --webtex
always_allow_html: true
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "imgs/",
  out.width = "100%"
)

knitr::opts_chunk$set(echo = TRUE)
```


<!-- badges: start -->
![forthebadge](https://img.shields.io/badge/GEMM-Building-orange)
![forthebadge](https://forthebadge.com/images/badges/built-with-science.svg)

<!-- badges: end -->

# An√°lise do Metagenoma total - Shotgun <img src="imgs/1.png" align="right" width = "120px"/>
**Autor: MsC. Kelly Hidalgo**

üáßüá∑ Pipeline para a montagem e anota√ß√£o funcional de metagenomas totais. Este pipeline contempla todas as etapas do processamento, desde a avalia√ß√£o da qualidade das sequ√™ncias, trimagem, montagem, c√°lculo da cobertura, predi√ß√£o e anota√ß√£o funcional e taxon√¥mica dos genes. 

> üá™üá∏Pipeline para montaje y anotaci√≥n funcional de metagenomas totales. Este pipeline contempla todas las etapas del procesamiento, desde la evaluaci√≥n de la calidad de las secuencias, *trimming*, montaje, c√°lculo de la cobertura, predicci√≥n y anotaci√≥n funcional y taxon√≥mica de genes. 

## Ferramientas bioinform√°ticas

### Instala√ß√£o Anaconda

üáßüá∑ √â recomend√°vel instalar Anaconda, pois √© a forma mais f√°cil para instalar as ferramentas bioinform√°ticas necess√°rias pro desenvolvimento deste pipeline. Anaconda √© uma distribui√ß√£o livre e aberta das linguagens *Python* e *R*, utilizada na ci√™ncia de dados e bioinform√°tica. As diferente vers√µes dos programas se administram mediante um sinstema de gest√£o chamado *conda*, o qual faz bastante simples instalar, rodar e atualizar programas. [Aqui](https://conda.io/projects/conda/en/latest/user-guide/install/index.html) se encontram as instru√ß√µes para a instala√ß√£o de Anaconda. 

Depois de instalado, *Anaconda* e o gestor *Conda*, podram ser criados *ambientes virtuais* par a instala√ß√£o das diferentes ferramentas bioinform√°tica que ser√£o usadas. 

> üá™üá∏ Es recomendable instalar Anaconda, pues es la forma m√°s f√°cil para instalar las herramientas bioinform√°ticas necesarias para el desarrollo de este pipeline. Anaconda es una distribuci√≥n libre y abierta de los lenguajes *Python* y *R*, utilizada en ciencia de datos y bioinform√°tica. Las diferentes versiones de los programas se administran mediante un sistema de gesti√≥n llamado *conda*, el cual hace bastante sencillo instalar, correr y actualizar programas.  [Aqui](https://conda.io/projects/conda/en/latest/user-guide/install/index.html) se encuentran las instrucciones para la instalaci√≥n de Anaconda.
>
> Despu√©s de instalado *Anaconda* y su gestor *Conda*, podran ser creados *ambientes virtuales* para la instalaci√≥n de las diferentes herramientas bioinform√°ticas que ser√°n usadas.

---
# I. Metagen√¥mica

## 0. Organizando os dados

### 0.1. Sequ√™ncias

üáßüá∑ Em este tutorial ser√£o usadas seis metagenomas exemplo para rodar todo o *pipeline*. Descarregue os [*datasets*]() usando o comando `wget`.

> üá™üá∏ En este tutorial ser√°n usados seis metagenomas ejemplo para correr todo el *pipeline*. Descargue los [*datasets*]() usando el comando `wget`.

**Arquivos**

* `sample1_1.fq.gz` e `sample1_2.fq.gz`: Amostra 1
* `sample2_1.fq.gz` e `sample2_2.fq.gz`: Amostra 2
* `sample3_1.fq.gz` e `sample3_2.fq.gz`: Amostra 3
* `sample4_1.fq.gz` e `sample4_2.fq.gz`: Amostra 4
* `sample5_1.fq.gz` e `sample5_2.fq.gz`: Amostra 5
* `sample6_1.fq.gz` e `sample6_2.fq.gz`: Amostra 6

A continua√ß√£o encontrar√° uma s√©rie de comandos para organizar adequadamente os diret√≥rios com as amostras.

```
## Crie um diret√≥rio raiz para todo o processo
mkdir metagenomica

## Entre al nuevo directorio
cd metagenomica/

## Crie um novo diret√≥rio para colocar os dados brutos
mkdir 00.RawData

## Entre em 00.RawData
cd 00.RawData/
```
Use o comando `mv` para mover os arquivos at√© o diret√≥rio `00.RawData/`. 

No final do processo de organiza√ß√£o deve ver seus diret√≥rios assim:
`ls 00.RawData/`

```
sample1_1.fq.gz   sample1_2.fq.gz   sample2_1.fq.gz   sample2_2.fq.gz   sample3_1.fq.gz   sample3_2.fq.gz   sample4_1.fq.gz   sample4_2.fq.gz   sample5_1.fq.gz   sample5_2.fq.gz   sample6_1.fq.gz   sample6_2.fq.gz
```

√â fortmente recomendado rodar os comandos desde o diret√≥rio base, que neste caso √©: `metagenomica/`

## 1. Controle da Qualidade

## 1.1. Avalia√ß√£o da qualidade

üáßüá∑ Para a avalia√ß√£o da qualidade ser√° usado o programa [FastQC](http://www.bioinformatics.babraham.ac.uk/projects/fastqc/) que √© uma ferramenta que permite observar graficamente a qualidade das sequencias de Illumina. 

> üá™üá∏ Para la evaluaci√≥n de la calidad ser√° usado el programa [FastQC](http://www.bioinformatics.babraham.ac.uk/projects/fastqc/) que es una herramienta que  permite observar graficamente la calidad de las secuencias de Illumina. 

### 1.1.1. Instala√ß√£o 

Las instru√ß√µes para a instala√ß√£o usando conda se encontram [aqui](https://anaconda.org/bioconda/fastqc). No entanto neste tutorial tamb√©m ser√£o apresentados.

Como j√° foi explicado anteriormente, com conda √© poss√≠vel criar ambientes virtuais para instalar as ferramentas bioinform√°ticas. O primeiro ambiente que ser√° criado se chamar√° **QualityControl**, onde se instalaram os programas relacionados com esse processo.

> üá™üá∏ [FastQC](http://www.bioinformatics.babraham.ac.uk/projects/fastqc/) es una herramienta para evaluar graficamente la calidad de las secuencias de Illumina. 
>
> Las instrucciones para instalaci√≥n usando conda se encuentran [aqui](https://anaconda.org/bioconda/fastqc). Sin embargo aqui en este tutorial tambi√©n ser√°n presentadas
>
> Como ya fue explicado anteriorimente, con conda es posible crear ambientes virutuales para instalar las herramientas bioinform√°ticas. El primer ambiente que ser√° creado se llamar√° **QualityControl**, donde se instalaran los programas relacionados con este proceso.

```
conda create -n QualityControl
```
üáßüá∑ Durante o processo, o sistema perguntar√° se deseja proceder com a crea√ß√£o do ambiente, com as op√ß√µes y/n (sim ou n√£o). Escreva `y` e depois disso o ambiente virutal estar√° criado.

Para instalar as ferramentas dentro do ambiente anteriormente criado, √© necess√°rio ativ√°-lo.

> üá™üá∏ Durante el proceso, el sistema preguntar√° s√≠ desea proceder con la creaci√≥n del ambiente, con las opciones y/n (si o no). Escriba `y` y despu√©s de eso el ambiente virtual estar√° creado.
>
> Para instalar las herramientas dentro del ambiente anteriormente creado, es necesario activarlo

```
conda activate QualityControl
```
üáßüá∑ O ambiente estar√° ativo quando o nome se encontre ao come√ßo da linha do comando, asssim: `(QualityControl) user@server:~/$`.
Posteriormente se procede √† instala√ß√£o do programa:

> üá™üá∏ El ambiente estar√° activo cuando el nombre de √©ste se encuentra en el comienzo de la linea de comando, as√≠: `(QualityControl) user@server:~/$`.
> 
> Posteriormente se procede a la instalaci√≥n del programa:

```
conda install -c bioconda fastqc
```

### 1.1.2. Uso

üáßüá∑ A primeira etapa do processo √© a avalia√ß√£o da qualidade das sequ√™ncias cortas (Illumina paired end) usando *FastQC*, com o objetivo de determianr se √© necess√°rio trimar ou filtrar as sequ√™ncias da baixa qualidade para nos pr√≥ximos pasos. 

Esta etapa √© para identificar principalmente as sequ√™ncias *outlier* com baixa qualidade ($Q<20$)

Ative o ambiente `QualityControl`:

> üá™üá∏ La primera etapa del proceso es la evaluaci√≥n de la calidad de las secuencias cortas (Illumina paired end) usando *FastQC*, con el objetivo de determinar s√≠ es necesario trimar o filtrar las secuencias de baja calidad en los pr√≥ximos pasos. 
>
> √âsta etapa es para identificar principalmente las secuencias *outlier* con baja calidad ($Q<20$).
>
> Active el ambiente `QualityControl`:

```
conda activate QualityControl

## Onde vc est√°?
pwd
```

üáßüá∑ Deve estar em `~/metagenomica/`.. Se esse n√£o √© o resultado del comando `pwd`, use o comando `cd` para chegar no diret√≥rio desejado.

> üá™üá∏ Debe estar em `~/metagenomica/`. Si ese no es el resultado del comando `pwd`, use el comando `cd` para llegar en el directorio base.

Execute  **FastQC**:
```
## Crie um direct√≥rio para salvar o output do FastQC
mkdir 01.FastqcReports
## Run usando 10 threads
fastqc -t 10 00.RawData/* -o 01.FastqcReports/
```

**Sintaxe**
`fastqc [op√ß√µes] input -o output`

üáßüá∑ O comando `fastqc` tem v√°rias op√ß√µes ou par√¢metros, entre eles, escolher o n√∫mero de n√∫cleos da m√°quina para rodar a an√°lise, para este exemplo `-t 10`. O input √© o diret√≥rio que contem as sequ√™ncias `00.RawData/*`, o `*` indica ao sistema que pode analisar todos os arquivos que est√£o dentro desse diret√≥rio. O output, indicado pelo par√¢mtero `-o`, √© o diret√≥rio onde se deseja que sejam guardados os resultados da an√°lise. A continua√ß√£o se encontram uma explica√ß√£o detalhada de cada output gerado.

> üá™üá∏ El comando `fastqc` tiene varias opciones o parametros, entre ellas, escoger el n√∫mero de n√∫cleos de la m√°quina para correr el an√°lisis, para este caso `-t 10`. El input es el directorio que contiene las secuencias `00.RawData/*`, el `*` indica al sistema que puede analizar todos los archivos que est√°n dentro de ese directorio. El output, indicado por el parametro `-o`, es el directorio donde se desea que sean guardados los resultados del an√°lisis. A continuaci√≥n se encuentra una explicaci√≥n detallada de cada output generado.

**Outputs**

üáßüá∑ 

* Reportes html `.html`: Aqui √© poss√≠vel ver toda informa√ß√£o de qualidade graficamente. 

* Zip files `.zip`: Aqui se encontram cada um dos gr√°ficos de maneira separada. **IGNORE**

Descarregue os arquivos `html` e explore no seu *web browser*. 

Observe as estat√≠sticas b√°sicas que se encontram na primeira tabela. Al√≠, voc√™ pode saber quantas sequ√™ncias tem, o tamanho e o %GC. O gr√°fico mais importante para saber a quealidade das leituras, √© o primeiro, *Per base sequence quality*. Este gr√°fico √© um boxplot com a distribui√ß√£o dos valores de qualidade *Phred Score* (eje y) em cada um dos nucleot√≠deos das leituras (eje x). Se consideram sequ√™ncias de excelente qualidade quando o *Phred Score > 30*. √â norla que o pair 2 apresente uma qualidade um pouco inferior ao pair 1.

> üá™üá∏ Observe las estad√≠sticas b√°sicas que se encuentran en la primera tabla. All√≠, ud puede saber cuantas secuencias tiene, el tama√±o y el %GC. El gr√°fico m√°s importante para saber la calidad de las lecturas es el primero, *Per base sequence quality*. Este gr√°fico es un boxblot con la distribuci√≥n de los valores de calidad *Phred Score* (eje y) en cada uno de los nucle√≥tidos de las lecturas (eje x). Se consideran secuencias de excelente calidad cuando el *Phred Score > 30*. Es normal que el pair 2 presente una calidad un poco inferior al pair 1. 


### 1.2. Trimagem

> üá™üá∏ 1.2 Depuraci√≥n


üáßüá∑ [Trimmomatic](http://www.usadellab.org/cms/?page=trimmomatic) √© um programa pra filtrar (remover) leituras ou *reads* curtas de baixa qualidade.

Trimmomatic tem v√°rios par√¢metros que podem ser considerados para filtrar leituras com baixa qualidade. No presente tutorial usaremos alguns deles. Se quiser saber que otros par√¢metros e como funciona cada um deles, consulte o [manual](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf).


> üá™üá∏ [Trimmomatic](http://www.usadellab.org/cms/?page=trimmomatic) es un programa para filtrar (remover) lecturas o *reads* cortas de baja calidad.
>
> Trimmomatic tiene v√°rios parametros que pueden ser considerados para filtrar lecturas con baja calidad. Aqui usaremos algunos. Si quiere saber que otros parametros y como funciona cada uno de ellos, consulte el [manual](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf).

### 1.2.1. Instala√ß√£o 

üáßüá∑ Como se trata de uma ferramenta que participa dentro do processo de control de qualidade, ser√° instalada dentro do ambiente virtual **QualityControl**.

> Como se trata de una herramienta que participa dentro del proceso de control de calidad, ser√° instalada dentro del ambiente virtual **QualityControl**

```
# Si no est√° activado el ambiente
conda activate QualityControl

# Instale Trimmomatic
conda install -c bioconda trimmomatic
```

### 1.2.2. Uso

üáßüá∑ Segundo foi avaliado no controle de qualidade, pode ser necess√°rio filtrar algumas leituras com qualidade baixa.

O programa Trimmomatic tem v√°rios par√¢metros que podem ser considerados para filtrar reads com baixa qualidade. Aqui usaremos alguns. Se quer saber que outros par√¢metros e como funciona cada um deles, consulte o [manual](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf).

Para os dados aqui analizados se usara a seguinte linha de comando:

> üá™üá∏ Seg√∫n fue evaluado en el control de calidad, puede ser necesario filtrar algunas lecturas con calidad baja.
>
> El programa Trimmomatic tiene v√°rios parametros que pueden ser considerados para filtrar lecturas con baja calidad. Aqui usaremos algunos. Si quiere saber que otros parametros y como funciona cada uno de ellos, consulte el [manual](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf).
> 
> Para los datos aqui analizados se usar√° la siguiente linea de comando:

```
# Activa o ambiente QualityControl
conda activate QualityControl

# Crie uma pasta para salvar as reads limpas
mkdir 02.CleanData

# Crie uma pasta para salvar as reads n√£o pareadas
mkdir unpaired

# Corra Trimmomatic
trimmomatic PE -threads 10 00.RawData/sample1_1.fastq.gz 00.RawData/sample1_2.fastq.gz 02.CleanData/sample1_1_paired.fastq.gz unpaired/sample1_1_unpaired.fastq.gz 02.CleanData/sample1_2_paired.fastq.gz unpaired/sample1_2_unpaired.fastq.gz LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:150
```

üáßüá∑ Com o comando anterior voc√™ tem que rodar a linha de comando para cada amostra. Se quiser rodar todas as amostras de maneira autom√¢tica √© poss√≠vel usar um *loop* `for` para executar esta tarefa.

> üá™üá∏ Con el comnado anterior ud tiene que correr esa l√≠nea de comando para cada muestra. Si quiere correr todas las muestras de manera autom√°tica es posible usar un *loop* `for` para ejecutrar esta tarea. 

```
# loop
for i in 00.RawData/*1.fastq.gz 
do
BASE=$(basename $i 1.fastq.gz)
trimmomatic PE -threads 10 $i  00.RawData/${BASE}2.fastq.gz 02.CleanData/${BASE}1_paired.fq.gz unpaired/${BASE}1_unpaired.fq.gz 02.CleanData/${BASE}2_paired.fq.gz unpaired/${BASE}2_unpaired.fq.gz LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:100
done
```

**Sintaxe**
`trimmomatic PE -threads input_forward input_reverse output_forward_paired output_forward_unpaired output_reverse_paired output_reverse_unpaired [op√ß√µes]`

üáßüá∑ O comando anterior tem muitas partes. Primeiro, o nome do comando √© `trimmomatic`, a continua√ß√£o a op√ß√£o `PE` indica para o programa que as sequ√™ncias que ir√£o ser analisadas s√£o de tipo *paired end*. Depois se encontram os inputs, forward (pair1) e reverse (pair2). Depois est√£o os outputs, sendo o primeiro, as sequ√™ncias forward pareadas (limpas) e n√£o pareadas ("descartadas") e depois igual para as sequ√™ncias reverse. Por √∫ltimo se encontram os par√¢metros de filtragem. Para este caso usamos os par√¢metros `SLIDINGWINDOW`, `LEADING` e `TRAILING`. O primeiro de eles, gera uma janela deslizante, que em este caso vai de 4 em 4 bases, c√°lcula a m√©dia do *Phred Score* e se estiver por baixo de 15 essas bases ser√£o cortadas. `LEADING` corta bases do come√ßo da leitura que estejam por debaixo do *threshold* de qualidade, igualmente faz o `TRAILING` mas no final das leituras. `MINLEN` elimina todas as reads com tamanho menor ao informado. Trimmomatic tem muitos mais par√¢metros para customizar, veja no [manual](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf).

Depois de rodar Trimmomatic √© necess√°rio avaliar a qualidade das sequ√™ncias limpas usando novamente FastQC.

> üá™üá∏ El comando anterior tiene muchas partes. Primero, el nombre del comando es `trimmomatic`, a continuaci√≥n la opci√≥n `PE` indica para el programa que las secuencias que ir√°n a ser analizadas son de tipo *paired end*. Despu√©s se encuentran los inputs, forward (pair1) y reverse (pair2). Despu√©s son los outputs, siendo primero las secuencias forward pareadas (limpias) y no pareadas ("descartadas") y despu√©s las secuencias reverse. Por √∫ltimo se encuentran los parametros de filtrado. Para este caso usamos los parametros `SLIDINGWINDOW`, `LEADING` y `TRAILING`. El primero de ellos, genera una ventana deslizante, que en este caso va de 4 en 4 bases, c√°lcula el promedio del *Phred Score* y si est√° por debajo de 15 esas bases son cortadas. `LEADING` corta bases del comienzo de la lectura si est√°n por debajo de *threshold* de calidad, lo mismo hace `TRAILING` pero al final de las lecturas. `MINLEN` elimina todas las lecturas con tama√±o menor al informado. Trimmomatic tiene muchos m√°s par√°metros customizables, revise en el  [manual](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf).
>
> Despu√©s de correr Trimmomatic es necesario evaluar la calidad de las secuencias generadas ("limpias") usando nuevamente FastQC.

```
fastqc -t 10 02.CleanData/* -o 01.FastqcReports/
```

Descargue los reportes `.html` de las secuencias pareadas (i.e. `01.FastqcReports/sample1_1_paired_fastqc.html` y `01.FastqcReports/sample1_2_paired_fastqc.html`).

Fa√ßa uma tabela com o n√∫mero de sequ√™ncias antes e depois da trimagem para calcular a porcentagem de *reads* que sobreveviveram ao processo.

> üá™üá∏ Haga una tabla con el n√∫mero de secuencias antes y despu√©s de la depuraci√≥n para calcular el porcentaje de *reads* que sobrevivieron al proceso.


### 1.3 Cobertura dos Metagenoma

üáßüá∑ Al√©m de limpar e trimar as sequ√™ncias com baixa qualidade, √© necess√°rio calcular a cobertura dos metagenomas.Este programa usa a redund√¢ncia de reads nos metagenomas para estimar a cobertura m√©dia e prediz a quantidade de sequ√™ncias que s√£o requeridas para atingir o *"nearly complete coverage"*, definida como $‚â•95$%  ou $‚â•99$% de cobertura m√©dia. A ferramenta [**NonPareil v3.3.3**](https://nonpareil.readthedocs.io/en/latest/) ser√° usada nesta etapa.

> üá™üá∏ Adem√°s de limpiar y *trimar* las secuencias con baja calidad, es necesario calcular la cobertura de los metagenomas. Este programa usa la redundancia de las *reads* en los metagenomas para estimar la cobertura promedio y predice la cantidade de secuencias que son requeridas para conseguir el *"nearly complete coverage"*, definida como $‚â•95$%  o $‚â•99$% de la cobertura promedio. La herramienta [**NonPareil v3.3.3**](https://nonpareil.readthedocs.io/en/latest/) ser√° usada en esta etapa.

### 1.3.1. Instala√ß√£o

üáßüá∑ [NonPareil v3.3.3](https://nonpareil.readthedocs.io/en/latest/) √© uma ferramenta que ser√° usada para o c√°lculo da cobertura dos metagenomas. Devido a incompatibilidades com a vers√£o do Python usado para escrever esta ferramenta, ela ser√° instalada em um ambiente diferente ao de controle de qualidade, chamado **NonPareil**.

> üá™üá∏ [NonPareil](https://nonpareil.readthedocs.io/en/latest/) es una herramienta que ser√° usada para el c√°lculo de la cobertura de los metagenomas. Debido a incompatibilidades con la versi√≥n de Python usado para escribir esta herramienta, ser√° instalada en un ambiente diferente al de control de calidad, llamado **NonPareil**.

```
# Crie o ambiente
conda create -n NonPareil

# Instale NonPareil
conda install -c bioconda nonpareil
```

### 1.3.2. Uso

Como *input* para esta an√°lise s√≥ √© necess√°rio um pair de cada amostra, e deve estar sem compress√£o.


```
# Crie o diret√≥rio pra o output
mkdir 03.NonPareil

# entre no directorio
cd 03.NonPareil

# Copie os pair 1 da pasta 02.CleanData

cp ../02.CleanData/*_1* ./

# Descomprimir 
gunzip -d *

```

üáßüá∑ Agora est√° tudo pronto para rodar a an√°lise, mas antes disso tome-se o tempo para entender o comando que vai usar. Para conhecer que √© cada um dos argumentos, explore o men√∫ de ajuda da ferramenta. 

> üá™üá∏ Ahora est√° todo listo para correr el an√°lisis, pero antes de eso t√≥mese el tiempo para entender el comando que va a usar. Para conocer que es cada uno de los argumentos, explore el men√∫ de ayuda de la herramienta.

```
# Ative o ambiente NonPareil
conda activate NonPareil

# Explore o men√∫ da ferramenta
nonpareil --help

# Comando do NonPareil para cada amostra
nohup nonpareil -s sample1.fq -T kmer -f fastq -b sample1 -t 6 &

```
No caso, se tiver v√°rias amostras pode usar o seguinte loop para facilitar o processo.

```
for i in ./*.fq
do
BASE=$(basename $i .fq)
nonpareil -s $i -T kmer -f fastq -b $i -t 6
done
```

**Sintaxe**

* `-s`: caminho para o *input* 
* `-T`: algor√≠tmo a ser usado. `kmer` √© recomendado para arquivos `.fastq` e `alignment` √© recomendado para arquivos `.fasta`.
* `-f`: indique aqui o formato do input (p.e. `fastq` ou `fasta`)
* `-b`: prefixo para os *outputs*
* `-t`: n√∫mero de threads

üáßüá∑ Ao terminar esse processo, o programa ter√° criado varios [*outputs*](https://nonpareil.readthedocs.io/en/latest/redundancy.html#output) por cada amostra. Descarregue os arquivos `.npo`. Os quais s√£o tabelas delimitadas por tabula√ß√µes com seis colunas. A primeira coluna indica o esfor√ßo de sequenciamento (em n√∫mero de reads), as demais colunas t√™m informa√ß√£o sobre a distribui√ß√£o da redund√¢ncia a determinado esfor√ßo de sequenciamento. Usando os arquivos `.npo` e o R, pode gr√°ficar as curvas de satura√ß√£o. A continua√ß√£o se encontra o script.
Al√©m dos arquivos `.npo` √© necess√°rio criar um arquivo chamado `samples.txt`, o qual deve ter tr√™s colunas (separadas por tabula√ß√µes), a primeira ter√° o nome de cada arquivo `.npo`, a segunda o nome da amostra, e a terceira a cor em formato JSON que vai ser usada para a curva. A continua√ß√£o se encontram uma s√©rie de comandos no bash para gerar o arquivo, no entanto este arquivo pode ser construido em um bloco de notas, ou incluso no excel.

> üá™üá∏ Al terminar este proceso, el programa habr√° creado varios [*outputs*](https://nonpareil.readthedocs.io/en/latest/redundancy.html#output) por cada muestra. Descargue los archivos `.npo`. Los cuales son tablas delimitadas por tabulaciones con seis columnas. La primera columna indica el esfuerzo de secuenciaci√≥n (en n√∫mero de *reads*), las dem√°s columnas tienen informaci√≥n sobre la distribuci√≥n de la redundancia a determinao esfuerzo de secuenciaci√≥n. Usando los archivos `.npo` e R, puede gr√°ficar las curvas de saturaci√≥n. A continuaci√≥n se encuentra el script.
>
> Adem√°s de los archivos `.npo` es necesario crear un archivo llamado `samples.txt`, el cual debe tener tres columnas (separadas por tabulaciones), la primera tendr√° el nombre de cada archivo `.npo`, la segunda el nombre de la muestra, y la tercera el color en formato JSON que va a ser usado para la curva. A continuaci√≥n se encontran una serie de comandos en bash para generar el archivo, sin embargo, este archivo puede ser construido en un bloc de notas, o incluso en excel.

```
# Cria um arquivo com os nomes dos arquivos
ls *.npo > files.txt

# Cria um arquivo com os nomes das amostras

ls *.npo | sed 's/_1_paired.fq.npo//g' > prefix.txt
```

Agora precisa criar uma lista de cores para diferenciar suas amostras no gr√°fico. Use o site [IWantHue](http://medialab.github.io/iwanthue/) para criar uma paleta com o n√∫mero de cores igual ao n√∫merop de amostras. Copie os c√≥digos **HEX json** das cores e coloque dentro de um arquivo (elimine as v√≠rgulas):

> üá™üá∏ Ahora necesita crear una lista de colores para diferencias sus muestras en el gr√°fico. Use el sitio de internet [IWantHue](http://medialab.github.io/iwanthue/) para crear una paleta con el n√∫mero de colores igual al n√∫mero de muestras. Copie los c√≥digos **HEX json** de los colores e coloque dentro de un archivo (elimine las comas):

```
# Crie o arquivo
nano colors.txt

# Copie e cole os c√≥digos
"#c151b6"
"#5eb04d"
"#7d65ce"
"#b5b246"
"#688ccd"
"#4bb092"
```
Cree o arquivo final com os t√≠tulos de las columnas e una los tr√™s arquivos gerados anteriormente:

```
echo -e 'File\tName\tCol' > samples.txt

# Unindo os arquivos dentro de samples.txt
paste -d'\t' files.txt prefix.txt colors.txt >> samples.txt
```

Use `less` para explorar o arquivo, ele deve se ver assim:

```
File    Name    Col
Sample1.npo   Sample1   "#c151b6"
Sample2.npo   Sample2   "#5eb04d"
Sample3.npo   Sample3   "#7d65ce"
Sample4.pno   Sample4   "#b5b246"
Sample5.npo   Sample5   "#688ccd"
Sample6.npo   Sample6   "#4bb092"
```
Descarregue os arquivos `.npo` e o arquivo `samples.txt`. Usando o seguinte script do R, grafique as curvas de satura√ß√£o. *Nota: todos os arquivos descarregados devem estar dentro de uma pasta s√≥, p.e. `03.NonPareil`.

```{r, eval=FALSE}
install.packages("Nonpareil") #para instalar o pacote
library(Nonpareil) # ativa o pacote
setwd("~/03.NonPareil") # determina seu diret√≥rio de trabalho (coloque o seu, onde colocou os arquivos .npo e o arquivo samples.txt)

samples <- read.table('samples.txt', sep='\t', header=TRUE, as.is=TRUE); #l√™ o arquivo samples.txt com a informa√ß√£o das amostras

attach(samples);
nps <- Nonpareil.set(File, col=Col, labels=Name, 
                     plot.opts=list(plot.observed=FALSE, 
                                    ylim = c(0, 1.05),
                                    legend.opts = FALSE)) #grafica as curvas

Nonpeil.legeng(nps, x.intersp=0.5, y.intersp=0.7, pt.cex=0.5, cex=0.5) #coloca e personaliza a legenda
  
detach(samples);
summary(nps) #mostra o resumo em forma de tabela
```

Vai obter um gr√°fico com as curvas de satura√ß√£o de cada amostra, como este:

<img src="imgs/nonpareil.webp" align='center' width="80%">

üáßüá∑ As linhas tracejadas <font color='red'> vermelha </font> e <font color='gray'> cinza </font> representam os *threshold* de 95% e 99% da cobertura m√©dia, respeitivamente. O circulo em cada curva representa a cobertura atual das amostras, o ideal √© que esteja por cima do primeiro *threshold*. As curvas tamb√©m apresentam a estima√ß√£o de quanto esfor√ßo de sequenciamento √© necess√°rio (zetas no eixo x).

> üá™üá∏ Las l√≠neas punteadas <font color='red'> roja </font> y <font color='gray'> gris </font> representam los *threshold* de 95% y 99% de cobertura promedio, respectivamente. El c√≠rculo en cada curva representa la cobertura actual de las muestras, lo ideal es que est√©n por encima del primer *threshold*. Las curvas tambi√©n presentan la estimaci√≥n de cuanto esfuerzo de secuenciaci√≥n es necesario (flechas en el eje x). 


### 1.4. An√°lise de Dist√¢ncias MinHash

üáßüá∑ Ap√≥s obter as sequ√™ncias limpas, de boa qualidade, e determinar a cobertura dos metagenomas, √© poss√≠vel fazer a montagem. No entanto, pode ser inclu√≠do um passo extra antes da montagem e √© verificar a similaridade dos datasets para determinar se pode ser usada a abordagem de *co-assembly*, onde s√£o misturadas as *reads* de v√°rios metagenomas para gerar os contigs. O programa [**Mash v2.3**](https://mash.readthedocs.io/en/latest/) usa uma t√©cnica chamada redu√ß√£o de dimensionalidad *MinHash* que avalia as dist√¢ncias um a um entre os datasets. 

> üá™üá∏ Despu√©s de obtener las secuencias limpias, de buena calidad, y determinar la cobertura de los metagenomas, es posible hacer el montaje. Sin embargo, puede ser inclu√≠do un paso extra antes del montaje y es verificar la similaridade de los datasets para determinar si puede ser usado el abordaje de *co-assembly*, donde son mezcladas las *reads* de varios metagenomas para generar los contigs. El programa [**Mash v2.3**](https://mash.readthedocs.io/en/latest/) usa una t√©cnica llamada reducci√≥n de dimensionalidad *MinHash* que evalua las distancias un a un entre los datasets.

### 1.4.1. Instala√ß√£o

üáßüá∑ [Mash v2.3](https://mash.readthedocs.io/en/latest/) √© uma ferramenta que usa a t√©cnica de redu√ß√£o da dimensionalidade *MinHash* para calcular as dist√¢ncias um a um entre os datasets, assim, √© poss√≠vel determinar se os metagenomas s√£o similares ou n√£o para serem montados usando *co-assembly*. 

üáßüá∑ Por ser considerada uma ferramenta que participa no processo de assembly, ser√° instalada dentro de um ambiente virtual chamado **Assembly**.

> üá™üá∏ [Mash](https://mash.readthedocs.io/en/latest/) es una herramienta que usa la t√©cnica de reducci√≥n de dimensionalidad *MinHash* para calcular las distancias un a un entre los datasets, as√≠, es posible determinar si los metagenomas son similares o no para ser ensamblados usando *co-assembly*.
>
> üá™üá∏ Por ser considera una herramienta que participa en el proceso de ensamble, ser√° instalada dentro de un ambiente virtual llamado **Assebly**.

```
# Crie o ambiente virtual
conda create -n Assembly

# Instale Mash
conda install -c bioconda mash
```

### 1.4.2. Uso

```
## Crie uma pasta para o output
mkdir 04.MinHash
```

üáßüá∑  O primeiro paso √© concatenar os reads 1 e 2, e armazenar eles na nova pasta criada `04.MinHash/`.

**Nota:** Se voc√™ trimou suas sequ√™ncias, deve usar os arquivos gerados pelo **Trimmomatic** na pasta `02.CleanData`, se pelo contr√°rio suas sequ√™ncias estavam de boa qualidade e n√£o foi necess√°rio trimar, use os arquivos originais, que est√£o dentro da pasta `00.RawData/`.

> üá™üá∏ 
>
>**Nota:** Si usted filtr√≥ sus secuencias, debe usar los archivos generados por **Trimmomatic** en el directorio `02.CleanData`, si por el contrario sus secuencias estaban de buena calidade y no fue necesario filtrar, use los archivos originales, que est√°n dentro de la carpeta `00.RawData`.

```
for i in 02.CleanData/*_1_paired.fq.gz
do
BASE=$(basename $i _1_paired.fq.gz)
cat $i 02.CleanData/${BASE}_2_paired.fastq.gz > 04.MinHash/${BASE}.fq
done
```

üáßüá∑ Depois ser√° criado um *sketch* para combinar todas as amostras. Usando `mash info` pode verificar o conte√∫do e, em seguida, estimar as dist√¢ncias par a par:

> üá™üá∏ 
>
> Despu√©s ser√° creado un *sketch* para combinar todas las muestras. Usando `mash info` puede verificar el contenido y, en seguida, estimar las distancias par a par:

```
mash sketch -o 04.MinHash/reference 04.MinHash/sample1.fq 04.MinHash/sample2.fq 04.MinHash/sample3.fq 04.MinHash/sample4.fq 04.MinHash/sample5.fq 04.MinHash/sample6.fq

#verifiyng
mash info 04.MinHash/reference.msh
```

**Sintaxe**

`mash sketch -o reference [inputs]`

`mash info reference.msh`

* `sketch`: Comando para criar um *sketch*, combinando todas as amostras, recomendado quando t√™m mais de tr√™s amostras.
* `-o`: caminho pro *output*, criar√° um *sketch* `.msh`.
* `inputs`: liste os inputs (sequencias concatenadas dos pair1 e pair2)
* `info`: pode verificar o conte√∫do do `sketch`
* `reference.msh`: *sketch* criado

Por √∫ltimo, calcule as dist√¢ncias entre cada par de metagenomas usando `mash dist` e salve o resultado no arquivo `distancesOutput.tsv`.

```
mash dist 04.MinHash/reference.msh 04.MinHash/reference.msh -p 6 > 04.MinHash/distancesOutputFinal.tsv
```

**Sintaxe**
`mash dist [reference] [query] [options]`

* `dist`: comando para calcular as dist√¢ncias entre cada par de mategenomas, baseado na dist√¢ncia *MinHash*.
* `reference`: aqui pode colocar o *sketch* criado, ou arquivos `.fq`, `fasta`.
* `query`: √≠dem
* `-p`: n√∫mero de threads

Descarregue o output (`04.MinHash/distancesOutputFinal.tsv`) e use o seguinte script do R para plotar um heatmap com as dist√¢ncias.

```{r, eval=FALSE}
setwd("~/04.MinHash/")

 data <- read.table("distancesOutputFinal.tsv")

 #install.packages("vegan")
 library(vegan)
 set.seed(2)
 
 dst = as.matrix(data)
 
 #install.packages("gplots")
 library(gplots)
 set.seed(2)
 x <- matrix(rnorm(100), nrow = 5)
 dist.fn <- function(x) as.dist(1-cor(t(x)))
 hclust.com <- function(x) hclust(x, method="complete")
 
 dev.off()
 h.ori <- heatmap.2(dst, trace="none", distfun=dist.fn, 
                    hclustfun=hclust.com,dendrogram = "row",main = "MinHash Clusterization",
                    cexRow=0.8, # Tamanho do texto no eixo y
                    cexCol=0.8,adjCol = c(0.5,0.2),
                    adjRow = c(0.05,0.),
                    srtCol=90,offsetRow=0, offsetCol=0, keysize = 1.5)
```

Vai obter um heatmap com clusteriza√ß√£o similar a este:

<img src="imgs/distances.png" align='center' width="80%">

Fa√ßa *co-assembly* para *datasets* com dist√¢ncias menores de 0.1, entre ellas.


## 2. Montagem dos Metagenomas

üáßüá∑ A montagem dos metagenomas √© a etapa mais importante do processo, porque os demais passos para adelante dependen de uma boa montagem. No caso dos metagenomas, se trata de um proceso que n√£o √© para nada trivial, requer um grande esfor√ßo computacional. Por este motivo, ser√£o testados v√°rios par√¢metros, para comparar cada montagem e decidir qual √© o melhor para √°s an√°lises *downstream*. Neste processo ser√° usado o montador [Spades v3.15.3](https://github.com/ablab/spades).

> üá™üá∏ El montaje de los metagenomas es la etapa m√°s importante del proceso, porque los dem√°s pasos para adelante dependen de un buen ensamble. En el caso de los metagenomas, se trata de un proceso que no es para nada trivial, requiere un gran esfuerzo computacional. Por este motivo ser√°n testados varios par√°metros, para comparar cada ensamble y decidir cual es el mejor para los an√°lisis *downstream*. En este proceso ser√° usado el montado [Spades v3.15.3](https://github.com/ablab/spades).

### 2.1. Instala√ß√£o

üáßüá∑ [Spades v3.15.3](https://github.com/ablab/spades) √© um dos montadores de genomas e metagenomas, mais conhecido e com melhores resultados, pode ser usado tanto para leituras curtas como longas. Leia atentamente o [manual](http://cab.spbu.ru/files/release3.15.2/manual.html), j√° que este programa tem muitas op√ß√µes diferentes. Spades usa o algor√≠tmo do *Grafo de Bruijn* para a montagem das secu√™ncias.

Siga as seguintes instru√ß√µes para a instala√ß√£o do **Spades** dentro do ambiente virtual *Assembly*.

> üá™üá∏ [Spades v3.15.3](https://github.com/ablab/spades) es uno de los ensambladores de genomas y metagenomas, m√°s conocido y con mejores resultados, puede ser usado tanto para lecturas cortas como largas. Lea atentamente el [manual](http://cab.spbu.ru/files/release3.15.2/manual.html), ya que este programa tiene muchas opciones diferentes. Spades usa el algor√≠tmo del *Grafo de Bruijn* para el montaje de las secuencias. 
>
> Siga las siguientes instrucciones para la instalaci√≥n de **Spades** dentro del ambiente virtual *Assembly*.

```
# Active el ambiente virtual
conda activate Assembly

# Instale Spades
conda install -c bioconda spades
```

### 2.2. Uso

üáßüá∑ Agora √© momento de fazer as montagens. Use o resultado da an√°lisis de dist√¢ncias *MinHash* para decidir como ser√£o feitos as montagens. Amostras muito pr√≥ximas pode fazer *co-assembly*, para amostras distantes √© recomendado montar individualmente. Opcionalmente podem ser usadas as sequ√™ncias no pareadas (sequ√™ncias "descartadas" pelo Trimmomatic). O montador usado neste m√©todo ser√° [Spades](https://github.com/ablab/spades). 

A continua√ß√£o se encontram os comandos se sua montagem for individual:

> üá™üá∏ Ahora es el momento de hacer los ensamblajes. Use el resultado del an√°lisis de distancias *MinHash* para decidir como ser√°n hechos los montajes. Muestras muy pr√≥xima puede hacer *co-assembly*, para muestras distantes es recomendado montar individualmente. Opcionalmente pueden ser las secuencias no pareadas (secuencias "descartadas" por Trimmomatic). El montador usado en este m√©todo ser√° [Spades](https://github.com/ablab/spades). 

1. Criar um diret√≥rio para todas as montagens

```
mkdir 05.Assembly
```

2. Se voc√™ quiser usar as *reads* no pareadas (sa√≠da do **Trimmomatic**), deve primeiro concatenarlas em um arquivo s√≥

```
cat unpaired/sample1_1_unpaired.fq.gz unpaired/sample1_2_unpaired.fq.gz > unpaired/sample1_12_unpaired.fq.gz
```

3. Montagem com MetaSpades

```
metaspades.py -o 05.Assembly/sample1/ -1 02.CleanData/sample1_1_paired.fq.gz -2 02.CleanData/sample1_2_paired.fq.gz -s unpaired/sample1_12_unpaired.fq.gz -t 6 -m 100 -k 21,29,39,59,79,99,119
```

**Sintaxe**

* `metaspades.py`: script para montar metagenomas
* `-o`: caminho para diret√≥rio de sa√≠da
* `-1`: caminho para diret√≥rio do pair1
* `-2`: caminho para diret√≥rio do pair2
* `-s`: caminho para diret√≥rio das *reads* no pareadas
* `-t`: n√∫mero de threads
* `-m`: Mem√≥ria em gigas (m√°ximo)
* `-k`: lista de *k-mers*

üáßüá∑ Se sua montagem for no modo *co-assembly* deve fazer uma etapa anterior, onde vai concatenar todos os pair1 das amostras que ser√£o montadas e todos os pair2 das mesmas. 

> üá™üá∏  Si su ensamblaje es en el modo *co-assembly* debe hacer una etapa anterior, donde va a concatenar todos los pair1 de las muestras que ser√°n montadas y todos los pair2 de las mismas. 

1. Concatene os pair 1

```
cat 02.CleanData/sample4_1.fq.gz 02.CleanData/sample5_1.fq.gz > 02.CleanData/sample45_1.fq.gz
```

2. Concatene os pair 2

```
cat 02.CleanData/sample4_2.fq.gz 02.CleanData/sample5_2.fq.gz > 02.CleanData/sample45_2.fq.gz
```

3. Se voc√™ quiser usar as *reads* no pareadas (sa√≠da do **Trimmomatic**), deve primeiro concatenarlas em um arquivo s√≥

```
cat unpaired/sample4_1_unpaired.fq.gz unpaired/sample4_2_unpaired.fq.gz unpaired/sample5_1_unpaired.fq.gz unpaired/sample5_2_unpaired.fq.gz > unpaired/sample45_12_unpaired.fq.gz
```

4. Montagem com MetaSpades

```
metaspades.py -o 05.Assembly/sample45/ -1 02.CleanData/sample45_1_paired.fq.gz -2 02.CleanData/sample45_2_paired.fq.gz -s unpaired/sample45_12_unpaired.fq.gz -t 6 -m 100 -k 21,29,39,59,79,99,119
```

**Outputs**

Para conhecer os demais par√¢metros do comando que n√£o foram modificados (usados por *default*), consulte o [manual](http://cab.spbu.ru/files/release3.15.2/manual.html).

* `corrected/`: cont√©m as reads corregidas por **BayesHammer** em `.fastq.gz`

* `scaffolds.fasta`: cont√©m os scaffolds obtidos

* `contigs.fasta`: cont√©m os contigis obtidos

* `assembly_graph_with_scaffolds.gfa`: cont√©m o grafo da montagem en formato GFA 1.0.

* `assembly_graph.fastg`: cont√©m o grafo da montagem em formato FASTG


## 3. Controle de Qualidade das montagens

üáßüá∑ Para avaliar a qualidade das montagens ser√° usada a ferramenta [**Quast v5.0.2**](http://quast.sourceforge.net/docs/manual.html) (*QUality ASsesment Tool*), especificamente o *script* `metaquast.py`, com o qual √© poss√≠vel determinar as principais estat√≠sticas da montagem (i.e. N50, n√∫mero de contigs, tamanho total da montagem, tamanho dos contigs, etc). **Metaquast** gera uma s√©rie de arquivos e reportes onde √© poss√≠vel observar essas estat√≠sticas b√°sicas da montagem. √â uma ferramente muito √∫til para comparar montagens e escolher a melhor pro mesmo conjunto de dados. 

> üá™üá∏ Para evaluar la calidad de los montajes ser√° usada la herramienta [**Quast v5.0.2**](http://quast.sourceforge.net/docs/manual.html) (*QUality ASsesment Tool*), especificamente el *script* `metaquast.py`, con el cual es posible determinar las principales estad√≠sticas del montaje (i.e. N50, n√∫mero de contigs, tama√±o total del montaje, tama√±o de los contigs, etc). **Metaquast** genera una serie de archivos y reportes donde es posible observar esas estad√≠sticas b√°sicas del montaje. Es una herramienta muy √∫til para comparar monatajes y escoger el mejor del mismo conjunto de datos. 

### 3.1. Instala√ß√£o

Crie um novo ambiente virtual, chamado bioinfo, onde se instalar√° **Quast**.

```
# Crie o ambiente
conda create -n bioinfo

# Ative o ambiente bioinfo
conda activate bioinfo

# Instale Quast
conda install -c bioconda quast
```

### 3.2. Uso

üáßüá∑ Antes de rodar `metaquast.py`, √© necess√°rio trocar os nomes dos assemblies, j√° que eles tem todos o mesmo nome, `contigs.fasta` ou `scaffolds.fasta`. Use o comando `mv` para trocar os nomes. Siga o seguinte exemplo:

> üá™üá∏ Antes de correr `metaquast.py` es necesario cambiar los nombres de los montajes, ya que todos tienen el mismo nombre, `contigs.fasta` ou `scaffolds.fasta`. Use el comando `mv` para cambiar los nombres. Siga el siguiente ejemplo:

```
mv 05.Assembly/sample1/scaffolds.fasta 05.Assembly/sample1/sample1.fasta

mv 05.Assembly/sample45/scaffolds.fasta 05.Assembly/sample45/sample45.fasta
```

```
# Crie um diret√≥rio pro output
mkdir 06.AssemblyQuality

# Rode Quast
metaquast.py 05.Assembly/sample1/sample1.fasta 05.Assembly/sample45/sample45.fasta -o 06.AssemblyQuality/ --threads 6
```

**Sintaxis**
`metaquast.py path/to/assembly/contigs.fasta -o path/to/output/`

**Interpreta√ß√£o dos resultados**

üáßüá∑ A ideia de usar **Metaquast**, a parte de avaliar as estat√≠sticas b√°sicas das montagens, √© comparar varias montagens para escolher a melhor. Por exemplo: entre menor seja o n√∫mero de contigs √© melhor, porque significa que a montagem est√° menos fragmentada. E isso ser√° refletido no tamanho dos contigs que ser√£o maiores. O valor de N50, √© melhor entre maior seja. Al√©m, tamb√©m √© ideal um menor n√∫mero de gaps e Ns. No entanto, estas estat√≠sticas funcionam melhor para genomas que para metagenomas, por se tratar de um conjunto de microrganismos.

> üá™üá∏ La idea de usar **Metaquast**, aparte de evaluar las estid√≠sticas b√°sicas de los montajes, es comparar varios montajes para escoger el mejor. Por ejemplo: entre menor sea el n√∫mero de contigs es mejor, porque significa que el montaje est√° menos fragementado. Y eso se reflejar√° en el tama√±o de los contigs que ser√°n m√°s grandes. El valor de N50, es mejor entre mayor sea. As√≠ mismo, es ideal menor n√∫mero de gaps y Ns. Sin embargo, √©stas estad√≠sticas funcionan mejor para genomas que para metagenomas, por tratarse de un grupo de microorganismos. 

**Outputs**

Explore o diret√≥rio do output usando o comando `ls`.

* `06.AssemblyQuality/report.html`: Este relat√≥rio pode ser aberto em um *web browser* e contem as informa√ß√µes mais relevantes. Como n√∫mero de contigs, tamanho del maior contig, tamanho total da montagem, N50, etc.

> üá™üá∏ `06.AssemblyQuality/report.html`: reporte puede ser abierto en un *web browser* y contiene las informaciones m√°s relevantes. Como n√∫mero de contigs, tama√±o del mayor contig, tama√±o total del montaje, N50, etc.

<img src="imgs/report_quast1.png" align="center" width = "100%"/>


* `06.AssemblyQuality/report.tex`, `06.AssemblyQuality/report.txt`, `06.AssemblyQuality/report.tsv`, `06.AssemblyQuality/report.pdf`: √© o mesmo relat√≥rio por√©m em diferentes formatos. 

* `06.AssemblyQuality/transposed_report.tsv`, `06.AssemblyQuality/transposed_report.tex`, `06.AssemblyQuality/transposed_report.tex`: Tamb√©m √© o relat√≥rio por√©m em formato tabular. 

* `06.AssemblyQuality/icarus_viewers/contig_size_viewer.html`: Visualizador das contigs

* `06.AssemblyQuality/basis_stats/`: Dentro desta pasta se encontram v√°rios gr√°ficos em formato `.pdf`. 

## 4. Predi√ß√£o das ORFs (*Open Reading Frame*)

üáßüá∑ O objetivo desta etapa √© procurar os marcos abertos de leitura ou ORFs (em ingl√™s) dentro dos contig/scaffols. Ou seja, predizer onde iniciam e terminam os genes. Basicamente o programa procura por codons de inicio, principalmente **ATG**, por√©m, tamb√©m s√£o c√≥dons de inicia√ß√£o **GTG** e **TTG**. Depois, procura os c√≥dons de parada, como **TAA**, **TAG** e **TGA**. 

O programa a usar para a predi√ß√£o das ORFs em procariotos √© [Prodigal v2.6.3 (*Prokaryotic Dynamic Programming Genefinding Algorithm*)](https://github.com/hyattpd/prodigal/wiki).

> üá™üá∏ El objetivo de esta etapa es buscar los marcos abiertos de lectura o ORF (en ingl√©s) dentro de los contigs/scaffolds. O sea, predecir donde incian y terminan los genes. Basicamente el programa busca por c√≥dones de inicio, principalmente **ATG**, sin embargo tambi√©n son c√≥dones de inico **GTG** e **TTG**. Despu√©s, busca los c√≥dones de parada, como **TAA**, **TAG** y **TGA**.
>
> El programa a usar para la predicci√≥n de ORFs en procariotos es [Prodigal v2.6.3 (*Prokaryotic Dynamic Programming Genefinding Algorithm*)](https://github.com/hyattpd/prodigal/wiki).


### 4.1. Instala√ß√£o

Crie um novo ambiente para instala√ß√£o das ferramentas relacionadas com a anota√ß√£o de genes, chamada `Annotation`. 

```
# Crie o ambiente
conda create -n Annotation

# Ative o ambiente
conda activate Annotation

# Instale Prodigal
conda install -c bioconda prodigal
```

### 4.2. Uso

Para facilitar o processo, passe todos os scaffolds de cada montagem para uma pasta s√≥. Siga o exemplo:

```
mkdir 05.Assembly/scaffolds

mv 05.Assembly/sample1/sample1.fasta 05.Assembly/scaffolds/

mv 05.Assembly/sample45/sample45.fasta 05.Assembly/scaffolds

# Confira
ll 05.Assembly/scaffolds
```

Crie uma pasta chamada `07.GenePrediction` para colocar a sa√≠da do **Prodigal**.

`mkdir 07.GenePrediction`

A continua√ß√£o encontrar√° o comando **individual** 

```
prodigal -i 05.Assembly/scaffolds/sample1.fasta -f gff -o 07.GenePrediction/sample1.gff -a 07.GenePrediction/sample1.faa -d 07.GenePrediction/sample1.fa -p meta
```
Se tiver v√°rias amostras, pode usar o seguinte loop para automatizar o processo com todas as amostras:

```
for i in 05.Assembly/scaffolds/*.fasta
do
BASE=$(basename $i .fasta)
prodigal -i $i -f gff -o 07.GenePrediction/${BASE}.gff -a 07.GenePrediction/${BASE}.faa -d 07.GenePrediction/${BASE}.fa -p meta
done
```
**Sintaxe**
```
prodigal -i assembly.fasta -f <gbk, gff, sqn, sco> -o coord -a proteins.faa -d nucleotides.fa
```

* `-i`: caminho para a montagem em formato `.fasta`, `.fa` ou `.fna`
* `-f`: formato de sa√≠da pro arquivo de coordenadas, default `.gbk` (*Genbank-like format*), `.gff` (*Gene Feature Format*), `.sqn` (*Sequin feature table format*) ou `.sco` (*Simple coordinate input*)
* `-o`: arquivo output com as coordenadas das ORFs
* `-a`: sequ√™ncias das ORFs em prote√≠na
* `-d`: sequ√™ncias das ORFs em nucleot√≠deos


**Formato `.gff` (Gene Feature Format)**

üáßüá∑ Este formato guarda as informa√ß√µes dos genes preditos pelo Prodigal. Explore-o (`less sample1.gff`).

Cada sequ√™ncia comen√ßa com um *header* com as infroma√ß√µes da sequ√™ncia analizada, seguido de uma tabela separada por tabula√ß√µes com informa√ß√µes dos genes encontrados em dita sequ√™ncia.

O *header* cont√©m os seguentes campos:

> üá™üá∏ Este formato guarda las informaciones de los genes predichos por Prodigal. Explorelo (`less sample1.gff`).
> 
> Cada secuencia comienza con un *header* con las informaciones de la secuencia analizada, seguido de una tabla separada por tabulaciones con informaciones de los genes encontrados en dicha secuencia.
> 
> El *header* contiene los siguientes campos:

* **seqnum**: O n√∫mero da sequ√™ncia, come√ßando pelo n√∫mero 1.
* **seqlen**: tamanho em bases da sequ√™ncia
* **seqhdr**: t√≠tulo completo da sequ√™ncia extra√≠do do arquivo `.fasta`.
* **version**: vers√£o do Prodigal usado
* **run_type**: modo de corrida, p.e. m*metagenomic*
* **model**: informa√ß√£o sob o arquivo de treinamento usado para a predi√ß√£o.
* **gc_cont**: % de GC na sequ√™ncia
* **transl_table**: Tabela do c√≥digo gen√©tico usada para analizar a sequ√™ncia. Para bact√©rias e archaeas √© usada a [tabela 11](https://www.ncbi.nlm.nih.gov/Taxonomy/Utils/wprintgc.cgi#SG11).
* **uses_sd**: 1 se o Prodigal usa o *[RBS](https://parts.igem.org/Ribosome_Binding_Sites) finder*, ou 0 se usa outros *motifs*. 

Despu√©s do *header* se encuentra una tabla con las informaciones de los genes encontrados:

* **seqname**: nome da sequ√™ncia, neste caso nome do scaffold/contig.
* **source**: nome do programa que gerou a predi√ß√£o
* **feature**: tipo de *feature*, p.e. CDS (*Coding DNA Sequence*)
* **start**: primeira posi√ß√£o da *feature*
* **end**: √∫tlima posi√ß√£o da *feature*
* **score**: Valor numerico que geralmente indica a confian√ßa do programa na predi√ß√£o da ORF.
* **strand**: fita do DNA que foi encontrado a *feature*. A fita *forward* √© definida como '+', e a *reverse* como '-'.
* **frame**: 0 indica que a primeira base da *feature* √© a primeira base do c√≥don de inicio, 1, que a segunda base da *feature* √© a primeira base do c√≥don de inicio.
* **atribute**: informaci√≥n adicional sobre la *feature*, parada por ponto e v√¨rgula ";".  

  * **ID**: identificador √∫nico de cada gene, consistindo em um n√∫mero ordinal ID da sequ√™ncia e um n√∫mero ordinal ID do n√∫mero do gene separados por "_". Por exemplo "1_688" siginifa que √© o gene n√∫mero 688 da sequ√™ncia 1.
  * **partial**: indica se o gene est√° completo ou n√£o. "0" indica que no gene foi encontrado o c√≥don de inicio ou de parada, "01" indica que no gene s√≥ foi encontrado o c√≥ndon de inicio, "11" indica que n√£o foram encontrados nenhum dos dois c√≥dons e "00" indica que foram encontrados ambos c√≥dons.
  * **start_type**: sequ√™ncia do c√≥don de inicio.
  * **stop_type**: sequ√™ncias do c√≥don de parada
  * **rbs_motif**: *RBS motif* encontrado pelo Prodigal
  * **rbs_spacer**: n√∫mero de bases entre o c√≥don de inicio e o *motif* observado.
  * **gc_cont**: Conet√∫do de GC no gene
  * **conf**: nota de confian√ßa pra o gene, representa a probabilidade que esse gene seja real. 
  * **score**: *score* total pro gene
  * **cscore**: fra√ß√£o hexamero do *score*, o quanto este gene se parece com uma prote√≠na verdadeira.
  * **sscore**: *score* para o sitio de inicio da tradu√ß√£o do gene. √© a soma dos tr√™s seguintes *scores*.
  * **rscore**: *score* pro *RBS motif*
  * **uscores**: *score* pra sequ√™ncia em torno do c√≥don de in√≠cio.
  * **tscore**: *score* para o tipo de c√≥don de inicio
  * **mscore**: *score* pros sinais restantes (tipo de c√≥don de parada e informa√ß√µes da fita principal / reversa).
  
  
## 5. Anota√ß√£o de genes

üáßüá∑ A anota√ß√£o dos genes √© feita alinhando as ORFs preditas contra bases de dados. No caso da anota√ß√£o funcional, ser√° usado o alinhador [**Diamond**](https://github.com/bbuchfink/diamond) e as bases de dados ser√£o [**EggNOG**](http://eggnog5.embl.de/#/app/home) e [**KEGG**](https://www.kegg.jp/kegg/). No caso da anota√ß√£o taxon√¥mica, podem ser usados dois programas, o [**Kaiju**](https://github.com/bioinformatics-centre/kaiju) ou o [**Kraken2**](https://github.com/DerrickWood/kraken2/wiki).


> üá™üá∏La anotaci√≥n de los genes es realizada alineando las ORFs predichas contra bases de dados. En el caso de la anotaci√≥n funcional ser√° usado el programa para alineamiento [**Diamond**](https://github.com/bbuchfink/diamond) y las bases de datos [**EggNOG**](http://eggnog5.embl.de/#/app/home) y [**KEGG**](https://www.kegg.jp/kegg/). Ya en el caso de la anotaci√≥n taxon√≥mica, pueden ser usados dos programas, [**Kaiju**](https://github.com/bioinformatics-centre/kaiju) o [**Kraken2**](https://github.com/DerrickWood/kraken2/wiki).


### 5.1. Instala√ß√£o

#### 5.1.1 Obten√ß√£o das Bases de Dados

üáßüá∑Para a obten√ß√£o das bases de dados, pode ir nos sites e descarregar diretamente. No entanto, tenha em conta que a base de dados **KEGG** √© paga. Se voc√™ descarregar direto da fonte, dever√° formatar as DBs para o seu uso com Diamond (anota√ß√£o funcional). Isto √© feito com o comando `makedb --in reference.fasta -d reference`.

Para facilitar, no seguinte link, voc√™ encontrar√° as bases de dados **KEGG**, **EggNOG**, previamente formatadas para o uso em Diamond e **Kraken2**.   

Use o programa `gdown` para descarregar as dbs que se encontram em um GoogleDrive. Se n√£o tiver o `gdown` instalado, siga o seguintes passos: 

> üá™üá∏ Para la obtenci√≥n de las bases de datos, puede ir directamente en las p√°ginas web de cada una. Sin embargo, tenga en cuenta que la base de datos **KEGG** es paga. Si ud decide descargar directamente de la fuente, deber√° hacer una formataci√≥n de las DBs para el uso con Diamond (anotaci√≥n funcional). Este processo es realizado usando el comando `makedb --in reference.fasta -d reference`. 
>
> Para facilitar, en el siguiente link, encontrar√° las bases de datos**KEGG**, **EggNOG**, previamente formatadas para su uso en Diamond e **Kraken2**. 


* [**Dbs**](https://drive.google.com/drive/folders/1GLP6vA4Gs0cce-nnBXCmZSgmONWybOSF?usp=sharing)


```
## Se n√£o tiver instalado pip
sudo apt update
sudo apt install python3-pip
pip3 --verision

## Instale gdown
pip install gdown
```

üáßüá∑ Crie uma pasta, chamada `dbs/`, e use o programa `gdown` para descarregar as dbs. 

```
# Crie o diret√≥rio
mkdir dbs/

# Descarregue as DBs
gdown https://drive.google.com/drive/folders/1GLP6vA4Gs0cce-nnBXCmZSgmONWybOSF?usp=sharing
```

Ser√£o descarregados os seguintes arquivos:

* `eggnog.dmnd`: Base de dados EggNOG formatada para Diammond
* `kegg.dmnd`: Base de dados KEGG formatada para Diammond

üáßüá∑ **Nota** √â recomend√°vel procurar os links originais para descarga das bases de dados para assim obter a vers√£o mais atualizada (p.e. [Kraken2](https://ccb.jhu.edu/software/kraken2/index.shtml?t=downloads))

> üá™üá∏ **Nota** es recomendable buscar los links originales para descargar las bases de datos en sus versiones m√°s actualizadas (p.e. [Kraken2](https://ccb.jhu.edu/software/kraken2/index.shtml?t=downloads))


```
## Kraken2
mkdir Kraken2
cd Kraken2

## Descarregando desde o servidor dos desenvolvedores
wget  ftp://ftp.ccb.jhu.edu/pub/data/kraken2_dbs/old/minikraken2_v1_8GB_201904.tgz

tar zxvf minikraken2_v1_8GB_201904.tgz

## Troque o nome da pasta de sa√≠da
mv minikraken2_v1_8GB/ mainDB

## Elimine o arquivo original
rm minikraken2_v1_8GB_201904.tgz
```

#### 5.1.2 Instala√ß√£o Diammond

O [**Diamond**](https://github.com/bbuchfink/diamond) ser√° usado para a anota√ß√£o funcional. Instale atrav√©s do conda, no ambiente `Annotation`

```
# Active o ambiente
conda activate Annotation

# Instala√ßao
conda install -c bioconda diammond=2.0.9
```

#### 5.1.3. Instala√ß√£o Kraken2

[**Kraken2**](https://github.com/DerrickWood/kraken2/wiki) ser√° usado para a anota√ß√£o taxon√¥mica. Instale o programa no ambiente `Annotation`.

```
# Se n√£o estiver ativado
conda activate Annotation

# Instale 
conda install -c bioconda kraken2
```

Ap√≥s instalado, deve configurar a base de dados, isto √© indicar pro programa o caminho (*PATH*) onde se encontram a base de dados. D√≠gite o seguinte comando: (*coloque o caminho que corresponda a onde voc√™ descarregou suas bases de dados*)

> üá™üá∏ Despu√©s de instalado, debe ser configurada la base de datos, esto es, indicar para el programa el camino (*PATH*) donde se encuentra la base de datos. D√≠gite el siguiente comando: (*coloque el camino que corresponda a donde ud descarg√≥ sus bases de datos)

```
export KRAKEN2_DB_PATH="/home/metagenomica/dbs/Kraken2/"
```

### 5.2. Anota√ß√£o Funcional

üáßüá∑ Uma vez instaladas todas as ferramentas e descarregadas as bases de dados, pode proceder √† anota√ß√£o. Neste caso ser√° feita primeiro √† funcional, usando Diammond e as bases de dados **KEGG** e **EggNOG**.
A continua√ß√£o se encontra o comando ndividual (*uma montagem e uma base de dados por vez*)

> üá™üá∏ Una vez instaladas todas las herramientas y descargadas las bases de datos, puede proceder a la anotaci√≥n. En este caso ser√° hecha primero la anotaci√≥n funcional, usando Diammond e las bases de datos **KEGG** e **EggNOG**

```
## Crie uma pasta pra sa√≠da
mkdir 08.FunctionalAnnotation

## Diammond
diamond blastx --more-sensitive --threads 6 -k 1 -f 6 qseqid qlen sseqid sallseqid slen qstart qend sstart send evalue bitscore score length pident qcovhsp --id 60 --query-cover 60 -d dbs/kegg.dmnd --query 07.GenePrediction/sample1.fa -o 08.FunctionalAnnotation/sample1_kegg.txt --tmpdir /dev/shm
```

**SINTAXE**

```
diamond blastx --more-sensitive --threads -k -f --id --query-cover -d dbs/db.dmnd --query orfs_nucleotides.fa -o annotation.txt --tmpdir /dev/shm
```

* `blastx`: Alinha sequ√™ncias de DNA contra uma base de dados de prote√≠nas
* `--more-sensitive`: este modo permite hits com >40% de identidade. Existem outros modos `--fast --min-sensitive --very-sensitive --ultra-sensitive`. Clique [aqui](https://github.com/bbuchfink/diamond/wiki/3.-Command-line-options) para mais detalhes
* `--threads`: n√∫mero de n√∫cleos
* `-k/--max-target-seqs`: N√∫mero m√°ximo de sequ√™ncias *target* por *query* para reportar alinheamentos.
* `-f/--outfmt`: Formato de sa√≠da. S√£o aceptos os seguintes valores:
  * `0` Formato BLAST *pairwise*
  * `5` fomato BLAST XML
  * `6` Formato do BLAST tabular (default), pode customizar as colunas com uma lista separada por espa√ßos, das seguintes op√ß√µes:
    * `qseqid` id da sequ√™ncia *query*
    * `qlen` tamanho da sequ√™ncia *query* 
    * `sseqid` id da sequ√™ncia da base de dados
    * `sallseqid` todas os id das sequ√™ncias das bases de dados
    * `slen` tamanho da sequ√™ncia da base de dados
    * `qstart` inicio do alinhamento no *query*
    * `qend` fim do alinhamento no *query*
    * `sstart` inicio do alinhamento na sequ√™ncia da base de dados
    * `send` fim do alinhamento na sequ√™ncia da base de dados
    * `evalue` 
    * `bitscore` 
    * `score` 
    * `length` tamanho do alinhamento
    * `pident` porcentagem de matches identicos
    
Com o comando anterior foi feita a anota√ß√£o da montagem da amostra `sample1` com a base de dados `kegg.dmnd` e os dados foram guardados no arquivo `kegg_annotation.txt`.

> üá™üá∏ Con el comando anterior fue realizada la anotaci√≥n de la muestra `sample1` con la base de datos `kegg.dmnd` y los datos fueron guardadas en el archivo `kegg_annotation.txt`. 


Se voc√™ quiser rodar todas suas montagens e as duas bases de dados ao mesmo tempo, pode usar o seguinte loop `for`:

```
for i in 07.GenePrediction/*.fa
do
BASE=$(basename $i .fa)
  for j in dbs/*dmnd
  do
  db=$(basename $j .dmnd)
diamond blastx --more-sensitive --threads 6 -k 1 -f 6 qseqid qlen sseqid sallseqid slen qstart qend sstart send evalue bitscore score length pident qcovhsp --id 60 --query-cover 60 -d $j --query $i -o 08.FunctionalAnnotation/${BASE}_${db}.txt --tmpdir /dev/shm
```

Com o comando anterior, √© feita a anota√ß√£o em todas as ORFs preditas na pasta `07.GenePrediction/` com todas as bases de dados para diammond dentro da pasta `dbs/`. Veja que no loop foram declaradas duas variav√©is, `i` que corresponde a cada um dos arquivos das ORFs (nucleot√≠deos) preditas com Prodigal e a vari√°vel `j` que corresponde a cada um dos arquivos terminados em `.dmnd` dentro da pasta `dbs/`, ou seja as bases de dados `kegg.dmnd` e `eggnog.dmnd`. Os arquivos de sa√≠da s√£o duas tabelas por cada montagem, uma da anota√ß√£o com *eggnog* e outra com *kegg*. 

> üá™üá∏ Con el comado anterior, es realizada la anotaci√≥n de todas las ORF predichas en el directorio `07.GenePrediction/` con todas las bases de datos para Diammond dentro de la carpeta `dbs/`. Vea que en el loop fueron declaradas dos variables, `i` que corresponde a cada uno de los archivos de las ORFs (nucle√≥tidos) predichos con Prodigal e la variable `j` que corresponde a cada uno de los archivos terminados en `.dmnd` dentro de la carpeta `dbs/`, o sea las bases de datos `kegg.dmnd` y `eggnog.dmnd`. Los archivos de salida son dos tablas por cada ensamble, una con la anotaci√≥n con *eggnog* e otra con *kegg*. 


### 5.3 Anota√ß√£o Taxon√¥mica

Para a anota√ß√£o taxon√¥mica ser√° usada a ferramenta Kraken2. Depois de instalada a ferramenta, descarregada e configurada a base de dados, √© poss√¨vel rodar o comando para anota√ß√£o. Lembrando que este procedimento deve ser feito para cada uma das predi√ß√µes de ORFs de cada montagem.

> üá™üá∏ Para la anotaci√≥n taxon√≥mica ser√° usado la herramienta Kraken2. Despues de instalada la herramienta, descargada y configurada la base de dados, es posible correr el comando para anotaci√≥n. Recordando que este procedimiento debe ser hecho apra cada una de las predicciones de ORFs de cada ensamble.

```
kraken2 --db mainDB 07.GenePrediction/sample1.fa 
```
**SINTAXE**

`kraken2 --db db orfs_nucleotides.fa`

* `--db`: nome da pasta onde se encontra a base de dados e que foi configurada no PATH.
* `orfs_nucleotides.fa`: Arquivo de sa√≠da da predi√ß√£o de ORFs, en formato `.fa` (nucleot√≠deos)

Para rodar num comando s√≥ todas as montagens, pode ser usado o seguinte loop: 

> üá™üá∏ Para correr en un solo comando todas los ensambles, puede ser usado el siguiente loop:

```
for i in 07.GenePrediction/*.fa
do
BASE=$(basename $i .fa)
kraken2 --db mainDB $i
done
```

O arquivo de sa√≠da √© uma tabela `.tsv` por cada montagem. As colunas est√£o organizadas da seguinte forma:

1. "C"/"U": Para indicar se a sequ√™ncia foi classificada ou n√£o classificada (*Unclassified*).
2. Nome do contig
3. Identifica√ß√£o Taxon√¥mica
4. Tamanho da sequ√™ncia em bp.
5. Mapeamento LCA de cada *k*-mer.


## 6. Constru√ß√£o Tabela Final

Finalmente √© necess√°rio construir uma tabela final com todas as anota√ß√µes (taxon√¥mica e funcional) de todas as montagens. 

1. **Formatando as tabelas de anota√ß√£o funcional**: Usando linha de comando, ser√£o escolhidas as colunas mais importantes.

```
for i in 08.FunctionalAnnotation/*.txt
do
BASE=$(basename $i .txt)
cut -f1,3,15 $i > 08.FunctionalAnnotation/${BASE}_formated.txt
done
```

2. **Adicionando uma coluna com o nome da montagem**

```
cd 08.FunctionalAnnotation/

for i in *; do nawk '{print FILENAME"\t"$0}' $i > $i.bk; mv $i.bk $i; done
```

3. **Formatando as tabelas da anota√ß√£o taxon√¥mica**

```
cd ../09.TaxonomicAnnotation/

for i in *.tsv
do
BASE=$(basename $i .tsv)
cut -f2,3 $i > ${BASE}_formated.tsv
done
```



















